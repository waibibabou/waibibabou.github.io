##### 6.1

1.以往的用强化学习做推荐的算法中，在不同的状态下推荐相同的物品的r相同，但这是不太合理的，应该针对不同的状态得到的r也不同，所以在ddpg框架下引入一个额外的判别器discriminator，得到额外的奖励r。或者用model-based方式也能实现不同状态相同动作奖励不同

输入到判别器中的是推荐序列，假样本序列是actor近期输出的序列，真样本是从数据集抽取的，需要预填充矩阵，但如何抽取还没有想好。

##### 6.4

lc 1631 917  codefun 68  total 985

##### 6.5

1.虚拟机集群各个组件启动顺序：hadoop zookeeper kafka hbase关闭时倒序关闭

2.python中报错inconsistent use of tabs and spaces in indentation，原因是混用了4个空格和tab

##### 6.7

1.背包问题中的存在问题可以用set代替数组，就不需要所有位置都更新了

##### 6.8

1.取模运算%mod注意结果范围是0到mod-1，如果从1开始则减1映射到从0开始

2.对于图与树如果边没有属性则用邻接表存储即可，如果边有属性则用邻接矩阵或者邻接表存储

##### 6.11

1.横轴上计算两两点的距离之和，利用前缀和计算<img src="D:\TyporaPicture\6月日报\image-20230611113622810.png" alt="image-20230611113622810" style="zoom:50%;" />

lc  1655 931 codefun 70  total 1001

##### 6.13

1.mod的运算性质

模(mod)运算规则与基本四则运算类似，但是除法例外：

$(a+b)\%p=(a\%p+b\%p)\%p$

$(a-b)\%p=(a\%-b\%p)\%p$

$(a*b)\%p=(a\%p*b\%p)\%p$

$a^b\%p=((a\%p)^b)\%p$

##### 6.14

1.bisect.insort_right()函数时间复杂度为O(n)因为其通过O(logn)找到插入位置后通过O(n)的时间进行插入

2.policy-based算法中，在分布中采样与重采样（重参数化技巧）都能够得到取某值的概率值，不同点是采样对参数不可导，重采样可导

3.ReinForce以及Actor-Critic可以用于连续动作空间，策略网络的输出为高斯分布的均值与方差，通过log_prob()函数得到动作的概率值的log值，从而可以利用这两种算法训练，但是在Pendulum-v1环境中的结果非常差，连续动作空间建议用DDPG与SAC，存储四元组时可以将log值也存储下来，在训练时就不用在过一遍policy网络了

##### 6.16

1.python运算符优先级：

1)算术运算符，先幂运算、乘除、加减  2)位运算符  3)比较运算符  4)布尔运算符 先and后or  5)赋值运算符

一定注意位运算符优先级低于算术运算符

##### 6.18

1.力扣上最多能跑$10^8$数量级

lc 1673 940 codefun 70 total 1010

##### 6.20

1.《Generative Adversarial User Model for Reinforcement Learning Based Recommendation System》其中奖励函数会根据s与a共同决定，而不是只有a，更加合理，否则在不同状态下采取相同动作会得到相同的奖励

2.在agent执行它的动作之前，他能否对下一步的状态和奖励做出预测，如果可以，则是model-based方法，如果不能，为model-free方法

3.重构别人的代码时，要有明确的Env与Agent

##### 6.22

1.写二分查找代码时候循环条件是left<=right，最后跳出循环时一定是left=right+1，两者一定是横跨在问题的分界上，最后返回left还是right看题目要求

##### 6.23

1.神经网络是通过调用$\_\_call\_\_$函数从而调用的forward函数

2.<代码>将函数参数自动赋值为实例属性的代码：

[参数自动赋值]: ../代码块/1.md

3.np.array与np.asarray都可以将数组转化为ndarray对象。区别：当参数为一般数组时，两个函数结果相同；当参数本身就是ndarray类型时，array会新建一个ndarray对象，但是asarray不会新建，而是与参数共享同一个内存

4.torch中乘法总结：

torch.dot用于计算两个一维张量的点乘，得到一个标量

torch.inner函数可执行一维张量的内积，当有高维度张量时，它会把一维内积的结果沿着张量最后的维度方向累加起来

torch.mul是两个张量对应位置相乘，两个张量形状必须一致

torch.mm与torch.matmul都用于矩阵乘法，但torch.mm只支持二维张量的乘法，不支持广播，torch.matmul支持高维张量的乘法并支持广播

torch.bmm用于批量矩阵乘法，计算两个三维张量的乘积，shape分别是(batch_size,n,m)与(batch_size,m,p)

##### 6.24

1.datetime模块基于time模块进行了封装，有以下几类：timedelta用于计算时间跨度，time处理时分秒，date处理年月日，datetime都能处理

2.torch.cat与torch.stack都是将多个tensor在指定的dim维度上进行拼接，数据形状要求相同。但是stack操作会增加一个维度，cat操作后维度不会变。

$torch.cat()=torch.concat()$

vstack与hstack也不会增加维度，用cat操作指定dim即可替代，后续不要用vstack以及hstack

$torch.cat(dim=0)=torch.vstack()$

$torch.cat(dim=1)=torch.hstack()$

3.torch.view与torch.reshape都用来重塑tensor的shape，view只适合操作满足连续性条件的tensor，而reshape还可以操作不满足连续性条件的tensor。所以后面都用torch.reshape就可以了

##### 6.25

1.记忆化搜索中参数的设定可以是根据已经走过的位置得来的，或者是对于未走过的位置要求的

2.torch.unsqueeze对输入的tensor的指定位置插入维度1。torch.squeeze如果不指定dim则去除tensor.shape中的所有1并返回，如果指定dim则只去除该维度的1，如果该维度不是1则不变

3.torch.tensor与torch.Tensor都用于生成新的张量，Tensor是一个类，而tensor只是一个函数。torch.tensor生成的张量数据类型会从输入数据中进行推断，Tensor使用全局默认类型torch.float32。当输入了几个单独数字时，Tensor创建对应维度数的张量，而tensor会报错

![image-20230625202903127](D:\TyporaPicture\6月日报\image-20230625202903127.png)

4.tensor.uniform(low,high)函数在low到high的均匀分布中抽取tensor.shape的数进行替换

5.定义nn.GRU时候的input_size表示seq中每个位置特征维度，hidden_size表示每层gru的神经元数量，num_layers表示gru层数，batch_first表示第一个维度是batchsize。gru的两个输出 1）output：shape为（batchsize，seq_len，h_out）表示每个样本的每个时间片时候的最后一层gru的隐藏状态。2）h_n：shape为（num_layers，N，h_out）表示每个样本的最后一个时间片的所有神经元的隐藏状态。

注意：隐藏状态==输出值

lc 947 2441 codefun 72 total 1019

##### 6.26

1.使用utf-8读取movies.dat文件时报错UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte。这是因为文件编码不是utf-8，使用Notepad++将该文件编码方式转为utf-8再读取即可

2.跑了一下DRGR代码，user的NDCG@5从0.03提高到0.1后报错

![image-20230626154722722](D:\TyporaPicture\6月日报\image-20230626154722722.png)

是因为p其中有0了，不能有为0的概率

3.recsys-rl代码中是将s与actor的输出送入critic计算q，DRGR文章说将分值最高的item的embedding送入critic，哪种是对的？将item的embedding送入critic那q对于actor还有梯度吗

##### 6.27

1.torch.flatten()如果没有传start_dim与end_dim参数则将输入的tensor展平成一维，有这两个参数则是将start_dim到end_dim之间的所有维度值乘起来，其他的维度保持不变。例如x是一个size为[4,5,6]的tensor, flatten(x, 0, 1)的结果是一个size为[20,6]的tensor

2.weight_decay权重衰减系数，即L2正则化项前面的$\lambda$系数，决定了在更新参数时权重的衰减比例

##### 6.29

1.对于dataframe的合并操作，主要有四种方法，concat()可沿任意轴连接 Pandas 对象，并且可在串联轴上添加一层分层索引；merge() 使用数据库样式的连接合并，连接是基于列或索引；append()与join()是前面两者的简易版，不要使用

2.<实验>DRGR代码user的NDCG@5从0.03提高到0.09后下降到0.05

[DRGR实验]: ../实验记录/1.md

原因可能是：train时候的item候选集没有设定、critic的输入不对，改成actor原本的输出试试，train时候actor输出没有加噪声

3.github解决push不上去问题：

先是报错fatal: unable to access 'https://github.com/waibibabou/DRGR.git/': Failed to connect to 127.0.0.1 port 1080 after 2029 ms: Connection refused

这是因为设置了代理的问题，修改C:\Users\27399\\.gitconfig，删除其中的proxy即代理即可

后续报错OpenSSL SSL_read: Connection was reset, errno 10054或者Failed to connect to github.com port 443 after 21148 ms: Timed out尝试换节点即可，本次最后用香港节点成功push

4.github导入别的网站项目需要其状态为public

##### 6.30

1.debug如果想进入到forward函数则在其中打个断点否则无法进入forward函数

2.tensor的复制操作总结：

tensor.clone()函数返回一个完全相同的tensor，开辟新的内存，但是仍然留在计算图中，即有梯度信息

tensor.detach()函数返回一个与旧的tensor共享内存的新的tensor，新tensor脱离计算图

![image-20230630202544171](D:\TyporaPicture\6月记录\image-20230630202544171.png)

copy_()函数完成与clone()类似的功能，但是调用copy\_()的对象是目标tensor，参数是复制操作from的tensor，最后返回目标tensor；而clone()调用对象为源tensor，返回一个新tensor:

![image-20230630202852564](D:\TyporaPicture\6月记录\image-20230630202852564.png)

3.程序在运行时修改代码不会影响运行结果，并且一个py文件可以多个进程共同执行

4.<实验>仅仅将DRGR中train时加了ou噪声：

[DRGR]: ../实验记录/2.md

全程没有train上去
