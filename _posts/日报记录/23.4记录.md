##### 4.1

下午做了hands on RL行为克隆

晚上做了GAIL

参加了双周赛

##### 4.2

中午见了高中同学

下午去国家速滑馆滑冰了

1485 836

##### 4.4

下午写完了学术英语写作大作业

晚上看完了武师兄论文

##### 4.5

上午发现服务器输入conda list显示conda命令找不到，在bashrc中配置了一下anaconda/bin路径后解决，后面在activate pytorch时候报错CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.输入source activate重新进入虚拟环境即可

下午看了Evolution-Guided Policy Gradient in Reinforcement Learning

查看进程ps aux 查看显卡占用gpustat

##### 4.6

上午找了老师讨论

GAIL源码 EA云计算讲

画图   按照两个大方向归类论文、想法  别人都做了什么我做了什么我还要做什么  找老师  分析可行性

腾讯那个计划都用什么数据集

康师姐论文，用进化算法先做offline，后面是online，用进化算法作为专家指导A-C网络

听云计算组会

##### 4.9

参加了周赛

周杰论文看完了2.2

1505 843

##### 4.10

下午看完了周杰师兄论文

##### 4.11

周赛第四题被re了，第一次被re

上午参加了云计算组会

##### 4.12

学习了最短路算法，包括典型Dijkstra以及堆优化

下午学完了hands on RL 第16章

晚上看完了hands on RL第17章

##### 4.13

上午参加了组会 协同过滤包括基于内存(邻域)以及基于模型的，重点是有邻居的概念。基于神经网络的有人把它归于基于模型的，也有人把它归于新的一类。基于内容的不属于协同过滤

晚上做完了hands on RL第18章

##### 4.14

1.奖励很稀疏是指在训练初期难以完成目标时只能得到-1的奖励，在完成任务时才能得到正数的奖励，从而使整个算法的训练速度缓慢

2.np.hstack（）将多个array水平堆叠，np.vstack（）将多个array竖直堆叠

3.神经网络由三种类型的层组成，输入层、隐藏层、输出层

输入层只保存输入数据，没有计算，所以不使用激活函数

输出层内使用激活函数的选择取决于我们要解决的问题类型

我们从不在输出层中使用tanh函数或者relu函数

必须在多分类任务中的输出层使用softmax函数，不在隐藏层中使用softmax函数

恒等激活函数仅用于回归问题的神经网络模型的输出层，不在隐藏层中使用该函数

4.下午完成了hands on RL第19章，HER适用于rewards很稀疏的情况，并且需要有一个目标

看完了hands on RL，多智能体的两章没看

##### 4.15

1.程序手动终止以后gpustat发现还在占用显存，但是显卡是0%，fuser -v /dev/nvidia*查看对应的pid，此时kill掉对应的PID后再查看发现不再占用显存。程序正常结束不会再占用显存

2.expected sequence of length 3 at dim 1 (got 2)

报这个错误是由于创建tensor时候，需要每一行的列数相同，如果第一行有3列，第二行只给了两列则会报错

gym版本：8p中为0.24.0，air为0.26.1，server为0.26.2，后续都统一成了0.26.2

在新版gym的API下，将原来env.step()返回的done，拆分成了两部分，terminated以及truncated，分别指成功或者失败而终止、到达指定了长度限制而终止两种情况，如果想把这两种情况合并，则加一行done=terminated or truncated即可。并且新API，env.reset()返回了state以及一个附加的dict组成的tuple。

3.设置了automatically upload好像有点问题，一直不自动上传，所以我选择了用ctrl+s上传，后来发现ctrl+s也不上传，不如直接手动上传

##### 4.16

1.使用堆优化的dijkstra算法，pop后不需要判断dis[i]是否等于d，好理解很多

2.矩阵每步向上下左右四个方向走，也能看作是一个图

3.下午做完了周赛以及双周赛的题目，总结了最短路问题

lc1540 864 codefun3  total 867

##### 4.17

1.acm中常说的卡常是指，程序的渐进复杂度满足要求，但是算法本身的时间常数因子较大，使得无法在规定的时限内运行结束。优化方式包括输入输出优化等等

2.使用SortedList最大的问题是，有的oj上没有安装这个包，此时只能考虑改成堆实现

3.acm代码模式下，想让程序主动结束使用sys.exit()，以免使用多个if else

4.学完了floyd算法，用于任意点到任意点的最短路，负权依然适用

##### 4.18

1.看完了transformer

transformer中抛弃了传统的CNN与RNN，提高并行能力，然后用cpu跑了一下transformer代码，太慢了

2.解释器有更改如果用的jupyter，需要重启kernel

torch从1.11.0改到1.8.0后报错，说是3070ti和pytorch版本不兼容，应该是版本太低了

报错No CUDA GPUs are available，与cuda什么的无关，可能是代码中设置了os.environ["CUDA_VISIBLE_DEVICES"]="2"，本机没有第三块显卡，改成0即可

torch与pytorch不能同时安装，这两个东西好像还不太一样，conda list之后发现这两个可以同时出现

安装torchtext需要与pytorch版本对应，并且在0.12.0后面有大更新

在卸载torch以及pytorch后执行conda install pytorch==1.10.0 torchvision==0.11.0 torchtext==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge再用gpu跑transformer成功，cuda占用99%，显存占用7.3G

##### 4.19

1.发现一些包在pycharm解释器中看到的与终端看到的版本不一样，pycharm中的版本是对的，最保准的方法是import后.version看一下

2.看了《Social Attentive Deep Q-network for Recommendation》论文，其用了DQN结构，只是在计算Q值时结合了社交网络信息。最后计算NDCG时候是每个时刻根据所有a的q值排序计算一个ndcg后所有时间下取平均得到，感觉所有的value-based的方法的NDCG都这样计算吗。在训练时居然没有用回放池以及目标网络

3.用本机与服务器的显卡跑了一下transformer，当服务器显卡只有我一个人用时发现每个epoch耗时两者几乎相同，别人也在用时，每个epoch耗时翻倍

4.想找论文的代码可以尝试在arXiv页面下的Code Data Media模块中找找

5.自回归（autoregressive）是把自己当前以及之前的所有输出作为下一次的输入，迭代产生一个输出序列

6.看论文时候记得找代码，找到后在文章名字后面(pycharm项目名字)

##### 4.20

1.离线强化学习是用固定的s,a,r对进行训练，不能与环境交互，推荐数据集相当于就是环境，那我之前将的都不能算离线强化学习，虽然说都没有放到真实网站上用

2.我之前理解的online以及offline是针对推荐系统来说的，offline是使用淘宝这种类似的离线数据集，online是真正放到网站上。而离线强化学习是针对强化学习算法说的，指的是能不能与环境交互。

3.感觉movielens那种矩阵形式的数据集也能用？和淘宝这种基于session的只是表现形式不一样

##### 4.21

1.学习了一下旅行商问题，一个状压dp问题

2.深度强化学习中，CPU用于收集经验数据，GPU用于训练神经网络

3.强化学习中的模型，可以是给定的，也可以是利用在交互过程中的采样对环境进行估计出来的

强化学习中模型的三种利用模式：Simulating the environment、Strengthening the policy、Assisting the learning algorithm

强化学习如何降低训练的采样复杂度，特别是对在线交互数据的需求：Model based RL-online learning with a learned model、imitation learning-online learning with an expert、offline RL-offline learning without extra online interaction

4.求最大公约数gcd(a,b)时间复杂度是O(logn)

5.算法中的离线的概念：不按他读入的顺序来执行操作，而是可以自由的对操作进行排序来处理

##### 4.22

1.tensor.squeeze()将张量shape中的1去除并返回，如果输入的shape为A,1,B,1,C,1,D则输出shape为A,B,C,D

tensor.unsqueeze()扩展维度，返回一个新的张量，对输入的既定位置插入维度1

2.技巧：tensor到第一个数有多少个[就有多少维

3.回溯类型：1排列型 使用一个visited数组  2子集型 选或者不选、选哪个两种写法  3组合型 可以看做长度固定的子集型回溯，加上剪枝即可

4.tensor.numel()用于迅速查看一个张量有多少元素

5.model.train()的作用是启用Batch Normalization以及Dropout，model.eval()作用是不启用Batch Normalization以及Dropout

6.pycharm中想在项目中所有位置查找使用ctrl+shift+S

7.力扣比赛提交时如果显示超出时间限制，但是下面是空白的，没有指定case或者指定hidden case，那说明不是某个case超时，而是总时间超时

##### 4.23

1.算法中离散化指的是排序去重后，将数组映射到他们的下标上，实现数据的压缩

2.两个电脑上的文件传输不需要用微信，因为需要反复登录，直接复制到onedrive文件夹中，可以实现两个电脑的同步

用onedrive实现了8p到air的组会+论文文件同步，8p中有修改时air中从onedrive复制下来就行了，不过发现onedrive文件夹同步得非常慢，手动重启onedrive也可以

3.看过的论文文件夹中日期带括号是讲过的，不带括号是没讲过的

lc1546 865 codefun22  total 887

##### 4.24

1.《RobustDQN》文章中说推荐的环境是动态的，而Atari游戏环境是静态的，该文章主要针对在线环境的动态变化，我有的都是历史数据集是不是不需要考虑环境的变化，环境在我这里类似游戏环境是静态的。其中使用了分层采样的方法训练网络，能够减小抽样的方差，根据用户的一些特征比如年龄等进行分层。这里的动态环境是指，比如在晚上用户对于结果的反馈的r可能远小于白天的r，但这并不是因为晚上推荐的结果不好，而是本身用户特性决定的。但是这是对于在线环境来说的，如果我用离线数据集训练的话应该是不需要考虑的，如果分层采样倒是第一次见。还有个创新点是在获取的r中减去了一个baseline的r，使得在动态环境中能更准确的估计奖励值。

实验是推荐淘宝的tip标签，而不是哪个物品，而且是在线做A/B测试。

2.报错...is not callable说明在...后面加了()当成一个函数用了

3.四舍五入可以将该数+0.5后int()

4.nn中的嵌入层nn.Embedding(num_embeddings,embedding_dim),num_embeddings表示词典的大小,embedding_dim表示嵌入向量的维度，生成一个权重矩阵（embedding table），embedding的过程本质就是lookup这个权重矩阵的过程，等价于一层没有bias项的全连接。一开始矩阵是随机的，在训练的时候会自动更新。除此以外，也可以导入已经训练好的embedding，设置weight.requires_grad=False即可。

nn.embedding参数中的padding_idx是填充的index，用于填充不等长的句子，对应的index那行的embedding都为0

5.《State representation modeling for deep reinforcement learning based recommendation》中的动作不是某个物品，需要与所有物品做点乘后选score最高的动作动作，之前有看到过这样的。这篇文章没什么创新，就是对比了四种不同的获取状态embedding的方式，使用AC框架。用户对于推荐动作如果是负反馈则t+1时候状态s不变。构建状态s方式：

<img src="D:\TyporaPicture\4月日报\image-20230424224101233.png" alt="image-20230424224101233" style="zoom:50%;" />

推荐的物品如果用户从来没评分过则r为0或者最好是用PMF先填充一下，该文章用的数据集是ML、Yahoo!Music、Jester，在网络训练时候可以推荐的物品是整个动作空间，而在评测计算NDCG是只能推荐某个session中的物品进行重排，但数据集并不是基于session的，那看来也能这么说

训练时候的动作空间是整个集合是比较好的，由于这篇论文数据集是有分数，其利用了PMF填充了一下空白位置，没有认为空白的r就是0。而有的数据集是略过、点击、购买的序列，这时候如果想动作空间是整个数据集需要用模拟器机制，没有模拟器就只能在训练时候推荐session中的物品。在评测阶段大部分用NDCG，不管对于分数数据集还是session数据集都是进行物品的重排。

用户以及物品的embedding是根据PMF得到的

这篇论文感觉是最base的，如果写代码可以尝试先写这种

6.状态转移矩阵P只存在于model-based强化学习算法中。

7.element-wise product==element-wise multiplication==Hadamard product两个矩阵对应位置元素进行乘积

##### 4.25

1.云计算组会：珊瑚虫进化算法、多智能体强化学习、状态空间与动作空间是一样的，DQN是下一个状态选哪个的概率，最后目的是选哪个状态最好，可能是根据策略后进行策略评估，选V最大的状态。但是策略评估得有P，这里面没有P啊

2.对于acm模式那种输入n-1条边建立的树，不管是不是二叉树，都用g存储邻居即可

3.python别的进制转十进制int(x,base)  x为字符串或者数字 base表示进制数

##### 4.26

1.对于有两个变量的题目，通常可以枚举其中一个变量，把它视为常量，从而转化为只有一个变量的题目，两数之和就是枚举第二个数，去左边找第一个数，用哈希表优化找第一个数的过程

2.torch.nn.DataParallel(model, device_ids=[0,1])过程是在每次forward时候将gpu0的参数复制到其他gpu的模型中，把一个batch分成几块分别送入不同gpu中，把每个卡的输出gather到gpu0中，只在gpu0中计算loss，把loss scatter到其他gpu上，把每个gpu各自计算的梯度汇总到gpu0中进行参数更新

3.argparse是一个用于命令行参数解析的模块，使用add_argument添加参数，default=不指定参数时的默认值，type=命令行参数应该被转换成的类型，help=参数的帮助信息，-metavar=在usage说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称

4.好像现在python的每个文件夹中不用再有一个init文件了，也可以正常调用

5.ndarray数据类型强转用astype()，tensor强转用 .指定类型

6.python函数参数中Optional[x]==Union[x,None]

7.学习了KMP算法，先是根据pattern字符串构建next数组，再扫描原字符串进行匹配，两部分比较相似

8.子数组/子串问题主要有两种思路：1）双指针，使用条件需要满足两段性，即一侧满足条件，另一侧不满足
2）前缀和+哈希表，常用于统计子数组个数，不需要有两段性

9.dfs且输入是一个数组的两大类题型：
（1）全排列类型，需要用到所有的数字，并考虑数字之间的顺序：
**使用visited数组**，每一层递归函数中，遍历所有没有visited的数字。（见全排列1）
如果输入中有重复数字并且需要去重，则将输入的数组排序，每层函数中，判断前一个数字是否与当前数字相同，如果相同并且前一个数字没有被使用则跳过当前数字（思想就是说当前要选择的数字是没有被使用的第一个从而实现去重）。（见全排列2）
注意：如果限定选出的数组的长度，则同样不需要用到所有的数字（见活字印刷）


（2）得到所有子集类型，不需要用到所有数字，不考虑数字之间的顺序：
**不使用visited数组**，在递归函数的输入中加入begin参数，在for中从begin开始遍历，到最后一个数字，选择出一个满足条件的数字加入到current中，进入下一层。（见组合总数1、子集1）
如果输入中有重复数字并且需要去重，则将输入的数组排序，在for中判断当前要选择的数字是否与前一个相同，如果相同则跳过。（见组合总数2、子集2）
注意：如果在某一层中可以什么都不选择，在for循环后单独调用begin为末尾的dfs函数

还有一种方法就是直接从前往后遍历nums，为每个数字做出类似  选择/不选择  的决定，该方法可以使用题型（2）的方法替换，写在这里只是为了提醒（见火柴拼正方形）

10.广度优先搜索最关键的就是在节点入队后必须立即标记其已经入队（通过visited数组或者改变原来数组中的值），以免产生一个节点两次入队的情况

##### 4.27

1.collections.Deque的insert(index,item)时间复杂度O(n)，remove(item)时间复杂度O(n)，支持in运算符O(n)

2.模型一般是一个epoch训完后进行一次eval

3.使用.cpu()将显存的数据移到内存  .cuda()将内存数据移到显存

4.好像不能简单使用nn.DataParallel使强化学习用多张gpu，因为获取a时候的batch_size为1，是一个序列问题，A3C等框架应该可以

##### 4.30

lc1560  873 codefun 33  total 906

