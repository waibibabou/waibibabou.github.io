---
layout: post
title: 23年下半年总结
subtitle: 2024冲冲冲
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.jpg
share-img: /assets/img/path.jpg
---


##### 7.1

1.<实验记录>将DRGR中critic的输入从s与item的embedding改为了s与actor输出

实验效果明显比改之前提高了许多

2.用GAN的两种想法：生成user model或者用于判别轨迹的真假得到额外的奖励值

3.DDPG-KNN用于解决动作空间大的问题，就是actor输出动作之后用KNN选距离最近的几个动作，用critic分别计算q之后选择q最大的动作执行。

##### 7.2

lc 1689 948 codefun 72 total 1020

##### 7.3

1.相同的代码，用本机跑比服务器快很多，服务器gpu利用率很低不到10%，并不是因为cpu太多人用的问题，通过top命令观察到该进程占用了一个核心的100%，所以原因是服务器cpu单核性能不足。但如果开两个相同的进程一起跑则会占用到70%，然而速度并没有变快，关掉其中一个则又回到不到10%，这又是为什么

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326351176-d317764a-5401-42d8-9547-efd50e61067f.png#averageHue=%239fa8a1&clientId=u38f25dad-8f88-4&from=paste&height=63&id=u727d0ef4&originHeight=100&originWidth=1310&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=52033&status=done&style=none&taskId=u33daa4d7-930e-4042-8351-4cb8b16aff0&title=&width=831.3061310234949)

如果用实验室的cpu会更慢，虽然用top命令查看到cpu占用为1600多。

目前的执行速度为：本机cpu，本机gpu，服务器gpu，服务器cpu

2.linux的watch命令，用于周期性的执行一个命令，免得一遍遍输入，命令格式为watch[参数][命令]，参数-n缺省则默认每两秒执行该命令，如果发现两次命令时间间隔与n不符是因为执行该命令需要一定时间，参数-d会高亮有变化的位置

##### 7.4

1.sarsa的行为策略与目标策略一致，都是epsilon-greedy，也就是说训练完成后也是用epsilon-greedy选择动作执行

q-learning的行为策略与目标策略不一致，行为策略是epsilon-greedy，目标策略为greedy，训练完成后用greedy选择动作执行

##### 7.5

1.on-policy的算法也不全是回合更新的，比如SARSA，就像Q-learning一样单步更新

##### 7.6

1.<组会文件>讲了组会：

##### 7.7

1.<实验记录>DRGR训4000轮，到最高点后下降

##### 7.9

lc 1689 948  codefun 72 total 1020

##### 7.11

1.云计算组会训练lstm作为离线环境

2.使用newbing，ip设置为美国，bing的区域设置为美国，使用国际版，并且bing的账号用google的账号登录

3.使用命令行执行plt.show()会阻塞，所以代码中就不show了直接plt.savefig()保存图片，使用plt.figure()画不同的图

##### 7.14

1.<代码块>优先经验回放PER

2.类方法装饰器：

[@property ](/property ) 

可以直接调用该方法获取实例对象的属性，但是不能做修改

@方法名.setter

使得可以调用该方法对指定属性值做修改

@方法名.deleter

使得可以调用该方法删除指定属性

##### 7.16

1.<代码块>将一个折线图变得平滑

2.pycharm中c为class、m为类方法method、f为普通函数function、f又可为类属性field、v普通变量variable

lc 1705 963   codefun 72  total 1035

##### 7.17

1.python中reduce函数

reduce(lambda x,y:x+y,list)，与map函数不同的是，reduce中匿名函数的参数是两个，map的匿名函数是一个，reduce的输入是之前序列的结果与当前值，从而迭代产生最终结果

2.python内置了sum函数但没有内置mean函数

3.用detach()代替tensor.data,  用tensor代替Variable()

##### 7.20

1.PER相比普通经验回放优缺点：

优点：同样的交互次数，更高的性能，适合稀疏奖励环境，或者高奖励难以获取的复杂环境

缺点：SumTree结构导致训练时间长，性能也不一定比普通的高

##### 7.24

1.Edge无法找到DNS地址，而chrome可以正常上网，在edge隐私模块中选择：使用当前服务提供商即可

##### 7.31

lc 1740 980   codefun 72  total 1052


##### 8.13

1.ValueError: Exceeds the limit (4300) for integer string conversion; use sys.set_int_max_str_digits() to increase the limit。设置sys.set_int_max_str_digits(100000)即可

2.python中pow(,,mod)实现快速幂，虽然python中不需要考虑int的溢出问题，但是过大的int值会超内存限制，在运算过程中先mod而不是到最后mod会大大提高运算速度并减小内存消耗

3.max()如果传入大于一个参数，则返回这些参数中的最大值，如果只传入一个参数，则该参数必须可迭代，返回该参数中的最大值

##### 8.16

1.python中在进行浮点运算时，可能由于精度问题导致结果不准确，如果对数据精度有要求，可以用Decimal类，实例化时传入整数或者字符串，如果传入float会导致结果不准确（float本身就存在精度问题），之后可以像基本数据类型一样进行运算

##### 8.17

1.list.copy()与copy.copy()为浅拷贝，如果有多层嵌套需要用copy.deepcopy()深拷贝

2.python中的闭包函数可以使用外面的变量，但是如果要对其做出改变（list.append之类的没有改变地址的不算），需要nonlocal关键字

3.with关键字工作原理：

(1)紧跟with后面的语句被求值后，返回对象的“__enter__()”方法被调用，这个方法的返回值将被赋值给as后面的变量，如果没有as就不赋值了

(2)当with后面的代码块全部被执行完之后，将调用前面返回对象的“__exit__()”方法。

4.python字符串、list之类的切片操作时间复杂度为O(k) 正比于切出来的数量，但是切片操作太牛了导致很多n=10**5的通过切片可以用n^2的复杂度通过

##### 8.18

1.再看了DRGR的代码，改了其中attention的部分，因为之前维度有问题，attention的softmax后权重都是1，改了之后跑一直没有跑上去

##### 8.19

1.<实验记录>DRGR在改了attention维度基础上又加了网络权重初始化操作，就能跑上去了

##### 8.22

1.在import torch时候报错：OSError: [WinError 1455] 页面文件太小,无法完成操作。是因为内存不足，清除部分内存即可

2.用户变量和系统变量的优先级：

（1）普通变量：如果在用户变量和系统变量中创建同名变量，那么用户变量会覆盖系统变量，用户环境变量优先级高于系统环境变量

（2）Path变量：对于 Path 变量，系统环境变量优先级高于用户环境变量

##### 8.24

1.<实验记录>在DRGR代码中在概率为0时主动加一个很小的数依然后续报错概率为0，分析后可能因为概率是一个非常接近0的数而不是0，导致没有加上很小的数，改后不再报错

##### 8.29

1.在单向链表中，头结点为空，尾节点正常存储值。但在lru题目中需要的双向链表中，由于需要改next节点的pre，所以将尾节点也设为空，防止next出现none情况，这样方便写代码，否则需要额外判断是否为none

##### 8.30

1.tensor的.T与.t()相同都是只能对一个二维tensor做转置，如果要对>=3维的tensor转置用.transpose(dim1,dim2)

##### 8.31

1.<代码块>ndcg指标计算：

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?DCG_p%3D%5Csum_%7Bi%3D1%7D%5Ep%5Cfrac%7Brel_i%7D%7Blog_2(i%2B1)%7D#card=math&code=DCG_p%3D%5Csum_%7Bi%3D1%7D%5Ep%5Cfrac%7Brel_i%7D%7Blog_2%28i%2B1%29%7D&id=V4URR)或者有一种更常用的公式![](https://www.yuque.com/api/services/graph/generate_redirect/latex?DCG_p%3D%5Csum%5Ep_%7Bi%3D1%7D%5Cfrac%7B2%5E%7Brel_i%7D-1%7D%7Blog_2(i%2B1)%7D#card=math&code=DCG_p%3D%5Csum%5Ep_%7Bi%3D1%7D%5Cfrac%7B2%5E%7Brel_i%7D-1%7D%7Blog_2%28i%2B1%29%7D&id=yB1HP)，其中rel为relative缩写，表示物品相关性，当相关性只有0或1时，即![](https://www.yuque.com/api/services/graph/generate_redirect/latex?rel_i%5Cin%5C%7B0%2C1%5C%7D#card=math&code=rel_i%5Cin%5C%7B0%2C1%5C%7D&id=Kkimw)上面两个公式等价。

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?IDCG_p%3D%5Csum_%7Bi%3D1%7D%5E%7B%7CREL_p%7C%7D%5Cfrac%7B2%5E%7Brel_i%7D-1%7D%7Blog_2(i%2B1)%7D#card=math&code=IDCG_p%3D%5Csum_%7Bi%3D1%7D%5E%7B%7CREL_p%7C%7D%5Cfrac%7B2%5E%7Brel_i%7D-1%7D%7Blog_2%28i%2B1%29%7D&id=EJbQg)其中![](https://www.yuque.com/api/services/graph/generate_redirect/latex?REL_p#card=math&code=REL_p&id=o0ivr)将推荐结果按照相关性从大到小排序，而不是数据库中的最好的k个物品

ndcg是召回率recall的补充，因为recall没有考虑推荐结果的顺序，而ndcg考虑顺序，所以这两个指标一般同时出现

2.<实验记录>via那份代码

该次实验结果与readme.md中结果一致

该代码将embedding进行pretrain并在后续训练中固定，在后续的额外实验中，第一种用concat代替embedding网络中的dot操作，效果变差；第二种用评分值代替学习使用的label值方法，效果变好了一点，ndcg@10从0.39提高到0.42。后续又尝试了加入per，几乎没有提升

代码中存在的问题，对于训练集测试集划分方式是对用户划分，而不是对于用户的记录划分。然而在训练embedding时候，用到了所有评分数据进行训练，测试数据提前泄露了，github中有人指出了该问题。user_movie_embedding_case4.h5不知道如何生成的。

lc 2643 1034   codefun 74  total 1108

##### 9.1

1.同一个py文件无法同时在ide中运行并且debug，但是可以在命令行运行，在ide debug

2.<实验记录>actor最后一层的tanh激活函数去掉，换成linear、 correct判断小改、 令state中全是>3的电影，将这三处都改了后via代码训练过程中，q_loss变得非常大，precision全是0

而后续让actor最后一层还是tanh，训练变得正常

3.<实验记录>via的评测阶段，如果对所有user的ndcg做平均：ndcg@10为0.413，如果对所有次的evaluation直接做平均ndcg@10为0.638

##### 9.2

1.<实验记录>再改了两个小问题使得状态中都是>3的电影后via代码的评测，对所有user做平均结果：结果反而下降了ndcg@10为0.391，后续又train了一次ndcg为0.400，还是略微下降了

不过后续用训练中途得到的网络权重以及随机权重做了eval，ndcg分别为0.36、0.052说明训练还是有效果的

##### 9.3

1.python中阶乘使用math.factorial()，计算排列组合![](https://www.yuque.com/api/services/graph/generate_redirect/latex?A_m%5En#card=math&code=A_m%5En&id=EOUlh)使用math.perm函数   计算![](https://www.yuque.com/api/services/graph/generate_redirect/latex?C_m%5En#card=math&code=C_m%5En&id=NVyx9)使用math.comb函数

2.非零值都为True，即-1也是True

##### 9.4

1.使用下划线开头的函数或类无法使用from xxx import _导入，除此之外还可以使用模块提供的_![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5C_%5C_all%5C_%5C_#card=math&code=%5C_%5C_all%5C_%5C_&id=dnU0w)_变量，该变量为一个list存储当前模块中的变量、函数、类名称，在其他模块用from xxx import _导入时只能导入all中指定的成员

2.python中@是装饰器用来标记函数或类方法，@staticmethod与@classmethod都是类方法的装饰器，@staticmethod不能访问或修改类状态或者实例状态，可以通过类名调用或者实例调用。@classmethod接受一个特殊的第一个参数cls，表示类本身，该方法可以访问修改类状态，但不能访问修改实例状态，可以通过类名调用或者实例调用。

总结：如果需要访问修改类的状态，用@classmethod，如果只执行与类和实例都无关的任务，用[@staticmethod ](/staticmethod ) 

3.nn.Parameter生成一个tensor，requires_grad默认就为true，并且自动加到模型参数列表中，后续通过optimizer.step()更新

##### 9.5

1.类中的特殊方法可以使得该类的实例与python内置函数相互作用，比如![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5C_%5C_getitem%5C_%5C_#card=math&code=%5C_%5C_getitem%5C_%5C_&id=oJ472)使得可以使用索引操作符[]获取元素，![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5C_%5C_len%5C_%5C_#card=math&code=%5C_%5C_len%5C_%5C_&id=LjTGB)使得可以使用len()函数

2.模型有可学习的参数时用nn.Module，否则可以用nn.Module也可以用nn.functional比如激活函数、池化层

dropout虽然没有参数但是需要用nn.Module因为其在train与eval阶段行为有差别，使用nn.Module能够通过model.eval()操作加以区分

3.bfs中的状态可能不仅需要记录位置，还需要记录额外信息比如上一步的执行方向

##### 9.7

1.taobao.txt这数据集每个time最后一个都是click

##### 9.10

1.python中虽然int可以存储无限大，计算到最后再mod也能得到正确结果，但是当数字过大时候运算会很耗时，并且由于数字过长可能出现超出内存限制的情况，所以最好还是计算过程中就mod

##### 9.12

1.直接与原文结果比较虽然说不是绝对公平的，可能测试集分割的不同，但是当数据量足够大的情况下，认为数据的好不好预测的程度相同，所以直接比也是可以的，但是如果有源码还是需要自己跑一遍源码看看结果

##### 9.14

1.nn.ModuleList()将list中各个层的参数自动注册到model中，如果用原生list不会自动注册。不会自动执行向前传播，需要手动一个个调用其中的layer

nn.Sequential()会自动执行前向传播，而不需要一个个调用其中的layer，但这就会缺乏灵活性，不易于进行复杂的向前传播逻辑

##### 9.15

1.线性dp中的经典问题

这种情况可以对数值域用双指针进行dp或者对给的n个taxi用二分进行dp

而这种情况不能对数值域dp，只能对n个job用二分进行dp了

2.github又clone不了了有时报错connection was reset或者time out，但是github.com可以ping通，过了一天之后又不报错了

##### 9.20

1.defaultdict(defaultdict(int))会报错，使用defaultdict(Counter)或者defaultdict(lambda :defaultdict(int))

2.dfs判断何时需要还原现场，比如将![](https://www.yuque.com/api/services/graph/generate_redirect/latex?visited%5Bi%5D%5Bj%5D%3D0#card=math&code=visited%5Bi%5D%5Bj%5D%3D0&id=uoxgA)，如果在dfs的另一条路径中也会访问到该状态，则将该状态的![](https://www.yuque.com/api/services/graph/generate_redirect/latex?visited%5Bi%5D%5Bj%5D%3D0#card=math&code=visited%5Bi%5D%5Bj%5D%3D0&id=c1K4V)，如果不需要再访问该状态了，则其一直为1即可

##### 9.21

1.torch.nn中的常用loss函数：

回归任务（Regression Tasks）

1. Mean Squared Error Loss (`nn.MSELoss`): 均方误差损失（L2 loss），适用于回归问题。
2. L1 Loss (`nn.L1Loss`): 绝对值误差（L1范数）损失，也用于回归。
3. Smooth L1 Loss (`nn.SmoothL1Loss`): 平滑L1损失，用于处理离群点（outliers）结合了L1 loss以及L2 loss。
4. Huber Loss (`nn.HuberLoss`): 类似于 Smooth L1 Loss，但提供了一个额外的`delta`参数来调整敏感度。

分类任务（Classification Tasks）

1. Cross Entropy Loss (`nn.CrossEntropyLoss`): 交叉熵损失，用于多类别分类。=log_softmax+nn.NLLLoss(负对数似然loss)
2. Binary Cross Entropy (`nn.BCELoss`): 二分类问题中的交叉熵损失。
3. BCE With Logits Loss（nn.BCEWithLogitsLoss）：结合Sigmoid激活函数和Binary Cross Entropy损失函数
4. NLL Loss (`nn.NLLLoss`): 负对数似然损失，通常在网络的最后一层使用 softmax 之后使用。

生成模型（Generative Models）

1. Kullback-Leibler Divergence (`nn.KLDivLoss`): 用于衡量两个概率分布的相似度。

其他特定任务

1. Cosine Embedding Loss (`nn.CosineEmbeddingLoss`): 用于衡量两个向量的余弦相似度。
2. Margin Ranking Loss (`nn.MarginRankingLoss`): 用于排序和推荐系统。
3. Hinge Loss (`nn.MultiMarginLoss` or `nn.HingeEmbeddingLoss`): 用于支持向量机（SVM）。

##### 9.25

1.推荐根据任务分类：

ctr预测：在广告推荐中常见，任务是预测用户是否会点击某个推荐的广告

评分预测：预测用户对尚未评分的物品可能给出的评分

topn推荐：为用户推荐一个n个物品组成的列表，常用协同过滤、矩阵分解等等

序列推荐：预测用户下一步最可能与哪个物品进行交互，这类推荐系统特别关注用户行为的时间顺序，主要有时间的概念，比如GRU4Rec、Transformer、强化学习等，要对用户在当前时刻前的历史记录做嵌入

分组推荐：为一个用户群体(而不是单一用户)推荐物品

多目标推荐：在满足多个目标（如准确性、多样性、解释性）的前提下进行推荐

实时推荐：在用户与系统实时互动过程中生成推荐，比如新闻推送

跨域推荐：使用一个领域（如电影）的数据来推荐另一个不同领域（如书籍）的物品

解释性推荐：生成可以解释或理解的推荐，以增加用户的信任和满意度

leetcode  2806   1099    codefun  78  total  1177

---


## layout: post
title: 23年10月日报
subtitle: 与老师battle的一个月
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/thumb.png
share-img: /assets/img/path.jpg

#### 10.6

1.L2正则化项通常不加到loss函数中，而是直接在定义optimizer时候指定weight_decay参数，因为两者等价，并且可以减少计算loss以及反向传播的计算量。

而L1正则化项不能通过optimizer定义，需要加到loss中

2.L2正则化==权重衰减(weight decay)

#### 10.7

1.tensorboard打开网站后没有图像，显示没有画任何数据。可能是输入tensorboard --logdir=。。。时候的路径有问题，退回到记录数据的上一级目录即可

2.我发现ddpg中计算critic的td误差时，q_now用的是经验回放中存储的action，这个action是加了噪声的，而计算q_target是将target_actor直接的输出的action没加噪声的

3.在不同的py文件往一个tensorboard上画时候报错：E1007 22:13:09.027264 25924 directory_watcher.py:254] File runs\experiment_1\events.out.tfevents.1696687683.zyk8p.2144.0 updated even though the current file is runs\experiment_1\events.out.tfevents.1696687683.zyk8p.2144.1

将画图的步骤放在一个文件中不再报错

#### 10.8

1.评测指标MAP（mean average precision）：

首先计算AP：

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?AP%3D%5Cfrac%7B1%7D%7BRel%7D%5Csum_%7Bk%3D1%7D%5EnP(k)*rel(k)%0A#card=math&code=AP%3D%5Cfrac%7B1%7D%7BRel%7D%5Csum_%7Bk%3D1%7D%5EnP%28k%29%2Arel%28k%29%0A&id=hdsAT)

P(k)是在前k个推荐结果中相关项的比例，rel(k)是个指示函数，当第k个结果是相关项时为1，否则为0。Rel是结果的相关项总数，n是推荐结果的数量

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?MAP%3D%5Cfrac%7B1%7D%7BQ%7D%5Csum_%7Bq%3D1%7D%5EQAP_q%0A#card=math&code=MAP%3D%5Cfrac%7B1%7D%7BQ%7D%5Csum_%7Bq%3D1%7D%5EQAP_q%0A&id=OLxCF)

Q是查询（或用户）的总数，AP是第q个用户的AP

2.<实验记录>

#### 10.12

1.dfs递归题目有两种类型，一个是递到树的某一位置或者到底部时候计算一个全局的指标，另一个是在归的过程中把指标一层一层return回去

2.用gpu将my项目original跑了，变快非常多，但模型训练极其不稳定

3.由于推荐里done信号也是人为加上的，尝试了下在agent计算q_target时候将done信号去掉，结果episode_reward根本跑不上去。又跑了几次，没有done信号真不行。但是好像在测试集上的指标还行啊

#### 10.17

1.chardet库可以检测出文件的编码方式

#### 10.21

1.tqdm用法：

（1）自动更新：

将迭代对象包裹在tqdm()中即可，每次迭代，进度条都会自动更新  for i in tqdm(range(100)):

（2）手动更新：

```python
#手动更新常用with关键字
with tqdm(total=10) as pbar:
    for i in range(10):
        time.sleep(0.5)
        # 更新进度
        pbar.update(1)
        # 在进度条后面添加或修改后缀信息
        pbar.set_postfix(stage=f"step {i+1}", current_value=i**2)
```

注意只有手动更新时候用set_postfix才很方便

#### 10.22

1.模型可视化主要有：

(1)直接print模型：这种如果某一层的输入输出没有对上不会报错

(2)torchsummary：可以在终端输出模型的各个层，各个层输入输出不匹配会报错，但这种对于模型如果有多个输入的情况不友好

(3)tensorboard：利用add_graph函数，可以处理多个输入的情况

2.运行一个py文件时，os.getcwd()返回当前工作目录，这是启动脚本的地方，而不是被调用的脚本文件的位置。也就是，如果一个py文件调用了另一个py文件中的函数，两者的当前路径是一样的，所以导致了找某文件的时候，ctrl+左键可以找到，但是用路径找却找不到的情况

3.在online测试的时候，对于同一个用户在liked_items不同的情况下，推荐的结果都一样，思考后感觉是因为：在训练时候得到的r相当于只与uid有关，所以会对同一user推荐相同的item因为agent认为这些item这个user会喜欢，并没有考虑user此时的状态。debug后发现同一user在不同状态下得到的state_embed不一样，但是action_embed非常相近，导致推荐的物品都一样：

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326420543-de457e14-eb71-4d61-bd8f-9f6403aa1da3.png#averageHue=%2398a59f&clientId=ubf94cabb-1f2e-4&from=paste&height=228&id=u0e3582a2&originHeight=360&originWidth=1285&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=220847&status=done&style=none&taskId=u28bc01ee-5959-4847-a830-9e3ab7b8edd&title=&width=815.4415102024359)

应该是actor网络的第一层和state_embed中的item_embed的连接的权重都被训得非常小了

#### 10.23

1.log没有下标通常为ln而不是![](https://www.yuque.com/api/services/graph/generate_redirect/latex?log_%7B10%7D#card=math&code=log_%7B10%7D&id=j9p7y)

#### 10.26

1.判断一个点是否在多边形内部：射线法，从该点发射一条水平射线，统计与多边形边界的交点数，如果交点为奇数，点在多边形内；若为偶数，点在多边形外。为确保交点有效，我们检查线段是否跨越射线并确保交点在该点的右侧

#### 10.27

1.把写好的md文件同步到个人网站步骤：

（1）复制TyporaPicture文件夹到myblog1文件夹中

（2）复制要post的md文件到对应路径

（3）把要post的md文件名前加要在什么时间post，如果是此时的时间后，则不会立即post出去  md文件前面加一些yaml信息

（4）修改md文件中的图片路径，将D：这两个字符删除即可

（5）把myblog1 push到waibibabou.github.io仓库即可

#### 10.28

1.SortedList中add() remove() pop() bisect_left() count() index()都是![](https://www.yuque.com/api/services/graph/generate_redirect/latex?O(logn)#card=math&code=O%28logn%29&id=zioWf)

2.![](https://www.yuque.com/api/services/graph/generate_redirect/latex?gcd(x%2Cy)%3Dgcd(x%E2%88%92y%2Cy)#card=math&code=gcd%28x%2Cy%29%3Dgcd%28x%E2%88%92y%2Cy%29&id=m40Af)

3.图上dfs使用visited数组(或set)避免重复访问，而不是树上dfs使用的(pos,fa)（其实这种方式也可以用visited代替，只不过加一个fa方便），因为图上有环，可能从不同的fa访问到同一个pos

#### 10.29

1.记忆化搜索中的参数的设定是根据已经走过的位置得来的，并且同时是对未走过的位置要求的

2.从维护单调性的角度上来说，单调队列和单调栈是一样的，一个弹出队尾元素，另一个弹出栈顶元素。在单调栈的基础上，单调队列只不过多了一个移除队首过期元素的操作。

#### 10.30

1.pycharm中右键运行时，可能看到Run Python test in 而不是Run 意味着pycharm识别到了文件中的一些测试代码，检查一下文件名和函数名，不要用“test”或者“test”作为前后缀

2.往github上push用香港的节点好，用gpt用台湾的节点

leetcode 2893  1124  codefun 81  total  1205

---


## layout: post
title: 23年11月日报
subtitle: 搞论文的一个月
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.jpg
share-img: /assets/img/path.jpg

#### 11.3

1.wandb总结：

wandb.init()中指定project、name、config参数

wandb.log()传入字典形式的要记录的数据

wandb.finish()将所有数据上传至服务器

#### 11.4

1.单调栈从左到右遍历还是从右到左遍历还是有区别的，比如：[2454. 下一个更大元素 IV - 力扣（LeetCode）](https://leetcode.cn/problems/next-greater-element-iv/description/)这个问题就只能从左到右遍历，因为如果从右到左遍历，右边的值已经遍历过了，不能获取第二个更大的了

#### 11.5

1.二分类问题中，输出层的激活函数用sigmoid或者softmax都可以，不过常用sigmoid

sigmoid就用BCE(二元交叉熵 Binary Cross Entropy)：

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?H(y%2C%5Chat%7By%7D)%3D-(ylog(%5Chat%7By%7D)%2B(1-y)log(1-%5Chat%7By%7D))%0A#card=math&code=H%28y%2C%5Chat%7By%7D%29%3D-%28ylog%28%5Chat%7By%7D%29%2B%281-y%29log%281-%5Chat%7By%7D%29%29%0A&id=TZKkE)

softmax就用CE(交叉熵 Cross Entropy)

而BCE With Logits Loss（nn.BCEWithLogitsLoss）就是将最后一层的sigmoid操作和BCE结合起来了

2.loss函数需要具备以下特点：可微性，loss函数需要在参数的所有可能值上都是可微的。loss函数应当形状良好，没有太多局部最小值

metric应当直观并且易于理解，与业务目标对齐，ndcg和map不可微，所以只能作为metric不能作为loss，而mae，rmse既可以作为loss又可以作为metric

3.推荐的结果都一样的问题解决：初始化线性层的bias不要用nn.init.uniform_(self.fc1.bias)，因为其是[0,1]的均匀分布，值太大，会将weight与输入的乘积抹除，就剩下bias的值了，导致无论什么样的输入，actor的输出都非常相近，导致推荐结果几乎相同。以后将线性层的bias初始化为0即可

如果用uniform方式初始化bias，训练后甚至在uid不同的情况下推荐的物品都很相似，说明收到bias影响太大了

训练发现，同一个用户浏览序列不同推荐的物品也不同了，但是推荐的结果与浏览序列中的完全不同，userid 1喜欢drama，序列是comedy，而推荐的都是action的

4.想看网络中某一层的输出，如果这一层在nn.Sequential中，想观察比较困难，可以写成一层一层的方式，在forward中打断点即可

#### 11.6

1.现在发现训练时候将embedding fixed，完全训不上去

2.如果发现训练后的部分参数几乎接近0，比如1e-42，可能是因为optimizer中设置了L2正则化，其每次更新参数都会将参数乘一个小于1的数，并且没有梯度的参数也会乘，比如embedding table中user id为0的一行，没有被look up过，但其参数训练后几乎为0

3.tensor转numpy  tensor.cpu().numpy()

numpy转tensor  tensor=torch.from_numpy(numpy_array)

4.能用贪心的条件：最优子结构、无后效性、全局最优解

能用dp的条件：最优子结构、无后效性

能用贪心解决的一定可以用dp解决，能用dp解决的不一定能用贪心

5.如果一个问题需要考虑所有可能的情况，可以用dfs或者bfs，bfs的典型应用是：寻找无权图中的最短路径（有权图用dijkstra）。如果问最短、最少、最小，可以尝试建模成一个无权图最短路问题，用bfs解决

6.global关键字用于在函数内部修改全局作用域中的变量

nonlocal关键字用于在嵌套函数中修改外层函数作用域（非全局）中的变量

7.根据对象是否可以在不改变对象自身标识（即内存地址）前提下改变内部的值，可以将对象分为可变、不可变两种。不可变包括：整数、浮点数、布尔值、字符串、元组；可变变量：列表、字典、集合

8.dp的自底向上（迭代）需要有一个明确的迭代方向，比如线性、区间、树形、状压等，但是有一些问题的迭代方向是不确定的，这时可以退而求其次用记忆化来做

9. 


![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326462476-211f4c8e-8853-4d1a-bfff-3540fa25553f.png#averageHue=%23fbfbfa&clientId=ubd689537-7fb3-4&from=paste&height=365&id=u0a5f4d42&originHeight=575&originWidth=1059&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=328398&status=done&style=none&taskId=u3c189ada-7713-474c-9bd5-6f3a1e3c67c&title=&width=672.025337980062)

10.在二维坐标轴上x表示水平方向，y表示垂直方向。在矩阵或者数组中，索引原点[0] [0]表示第一行第一列元素，位于左上角。注意这两者区别

指定axis是沿着该轴操作，使得结果中不再包含这个轴
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326470580-9bc85916-4c93-4282-af0f-68c9ec465e80.png#averageHue=%23f4f4f4&clientId=ubd689537-7fb3-4&from=paste&height=233&id=u82035f77&originHeight=367&originWidth=694&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=50159&status=done&style=none&taskId=u3654a8f7-48dd-4333-b031-82b88acad74&title=&width=440.40187399259963)

#### 11.7

1.numpy.random可以实现标准库random的所有功能，但如果只需要简单的随机数功能，用random即可

#### 11.8

1.子序列和问题解法：

如果数组长度特别大，但是数组的和不大(sum<=10^5)，使用背包问题解决

如果数组长度不大(n<=20)，但是数值特别大，使用枚举子集的方式

如果长度稍大，比如n=40，直接枚举子集会超时，需要折半枚举，将数组平均分为两部分，两部分分别枚举

2. 


![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326481529-5425dfa8-9fc9-4ff4-9b13-1f7886abd827.png#averageHue=%23cdbd87&clientId=ubd689537-7fb3-4&from=paste&height=297&id=ud828b083&originHeight=468&originWidth=779&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=34390&status=done&style=none&taskId=u3e7c4cb1-e372-4946-bc5d-3a75dc13ccc&title=&width=494.34158478420045)

chatgpt遇到问题，进入页面后无法交互，应该是openai自己的问题，如果号被封了是不能到这个页面的

3.记忆化搜索记得dfs.cache_clear()  并且记忆化搜索不会计算所有的状态，只会计算用到的状态，dp会计算所有状态

#### 11.9

1.wandb.Artifact()有三级目录，type、name、v几版本，最后的v几版本可以不指定，系统自动从v0开始指定，用来保存文件夹、代码文件、模型参数文件

#### 11.12

1.折半枚举(Meet in the Middle)

用于当直接枚举所有可能的解会导致时间复杂度过高，通过将问题分为两部分，分别枚举每部分的解，将这些解组合起来得到最终解

而折半查找和折半枚举没有关系，折半查找==二分查找

2.用clash verge时出现问题，能够访问国内网站，不能访问所有外网以及bing搜索，重启后解决，可十分钟后又出现该现象，将vpn换成walless的节点解决。但是walless节点也有问题，有时候bing搜索不出任何东西(不是不能用bing)，而clash verge没有发现该问题

3.在push到github时候遇到问题，让重新登录github账号，并且pycharm中github登录信息显示连接超时，换了个网络连接方式，从wifi改成热点就解决了
#### 11.13

1.![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326500000-2fddd0eb-9d28-4222-8dd4-74b6ea05b2cc.png#averageHue=%23f9f7f6&clientId=ubd689537-7fb3-4&from=paste&height=544&id=u4093f1cb&originHeight=857&originWidth=692&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=532404&status=done&style=none&taskId=u1e19a2c7-0605-47ee-aa7c-30a6e2c007e&title=&width=439.1327043269149)

lstm有两个状态![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=sDvND)(hidden state) 和![](https://www.yuque.com/api/services/graph/generate_redirect/latex?c_t#card=math&code=c_t&id=RVfFJ) (cell state)

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=x0LV8)是hidden state，隐藏状态，提到lstm的隐藏状态指的就是![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=kRPB0)，根据![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=pLFp7)得到输出，输出可以直接就是![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=OYg4q)，也可以是经过得到额外网络得到输出

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?c_t#card=math&code=c_t&id=lDlgP)细胞状态是lstm的另一种内部状态，通过门控机制(遗忘门、输入门、输出门)来添加和移除信息

而GRU中只有一个隐藏状态![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=lR0Hi)，没有细胞状态，也是根据![](https://www.yuque.com/api/services/graph/generate_redirect/latex?h_t#card=math&code=h_t&id=VhbcK)得到输出，要么一样，要么再过一个网络

2.![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326511524-fb0bb738-a357-44f1-b319-3c1a4f925d16.png#averageHue=%23deedd1&clientId=ubd689537-7fb3-4&from=paste&height=483&id=u76268649&originHeight=761&originWidth=1280&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=667627&status=done&style=none&taskId=uc0b7a58e-b740-4fa5-be33-1c15f78f710&title=&width=812.268586038224)

#### 11.15

1.向日葵远程控制，被控制的电脑如果有扩展屏，则鼠标完全不受控制，现在用ToDesk了，没发现该bug

2.解压包时候用windows自带的工具解压显示内存不足，用winrar解压即可

#### 11.19

1.从大到小排序的list不能直接用bisect，只能list[::-1]后再用

#### 11.21

1.套路：从前往后遍历+需要考虑相邻元素+有消除操作=栈

#### 11.25

1.程序报错段错误可能是递归深度太大了

#### 11.28

1.![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326520533-d57ea272-6368-4442-8189-21a74feed792.png#averageHue=%23f9f7f5&clientId=ubd689537-7fb3-4&from=paste&height=379&id=u44ec8ef0&originHeight=598&originWidth=1275&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=510032&status=done&style=none&taskId=u0fad0b0b-7714-41cc-ab88-aa97e7b4eb6&title=&width=809.0956618740123)

---


## layout: post
title: 23年12月日报
subtitle: 搞论文的一个月
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.jpg
share-img: /assets/img/path.jpg

#### 12.1

1.记忆化搜索与递推的状态定义以及计算方向(因为是归的时候才能计算出结果)都是一样的，从记忆化搜索翻译到递推：dfs函数改成dp数组、递归改成循环(每个参数都对应一层循环)、递归边界改为dp数组初始值、递归入口即为答案

#### 12.3

1.注意线性dp的方向，有可能是从前往后也可能是从后往前

#### 12.4

1.回溯法是DFS的一个子集或特例，DFS是回溯法的基础，但回溯法在DFS基础上增加了剪枝的策略。DFS是一种遍历算法，而回溯法是一种利用DFS思想并加入剪枝策略的解决特定问题的算法。

2.分治与动态规划：两者都是将原问题分解为子问题，然后合并子问题的解得到原问题的解，但分治分解出的子问题是不重叠的，因此分治法解决的问题不具有重复子问题，而动态规划解决的问题具有重复子问题。

3.对数似然损失函数(负对数似然损失函数)，与交叉熵损失函数等价

4.GET请求：form标签 method=get、a标签、link标签引入CSS文件、Script标签引入js文件、img引入图片、在浏览器地址栏输入地址后回车。地址栏的地址是action属性[+?+请求参数]

POST请求：form标签 method=post。地址栏的地址只有action属性

5.plt画图时每个线都设置名称：给plt.plot()中参数label=''传入名称，使用plt.legend()产生图例

#### 12.5

1.正则表达式在多种语言中都可以用，在python中使用re模块
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326544911-bb93c8ee-efee-487c-ad2a-bf5cb636f79e.png#averageHue=%23eaeaea&clientId=u2b3c7fce-a209-4&from=paste&height=524&id=ud45bda33&originHeight=825&originWidth=804&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=226765&status=done&style=none&taskId=u953c50a2-4284-4e24-b637-68dfd24d9cf&title=&width=510.2062056052595)


2.java：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326550745-3852ae0c-2e9f-470d-a904-55fc02594dc3.png#averageHue=%23c6dcc7&clientId=u2b3c7fce-a209-4&from=paste&height=262&id=u794d0d47&originHeight=413&originWidth=975&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=229039&status=done&style=none&taskId=uab69a46a-0d30-41cb-a3cf-645de251d9d&title=&width=618.7202120213035)

static关键字：

static变量：它是属于类而不是类的某个特定实例。无法通过this和super引用静态变量，因为它们是在类层面上，而不是实例层面上

static方法：是属于类的方法，可以在不创建类的实例的情况下调用，不能访问类的实例变量或实例方法，只能访问其他静态变量和静态方法

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326564782-fead7790-31b7-486e-a3a5-f1b1c833aa5e.png#averageHue=%23f0efdd&clientId=u2b3c7fce-a209-4&from=paste&height=610&id=u6942d22d&originHeight=961&originWidth=1379&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=588648&status=done&style=none&taskId=u68884637-8842-44d4-98c6-f1bec26cf18&title=&width=875.092484489618)



StringBuffer和String的相互转换：

String到StringBuffer：使用StringBuffer的构造函数、使用append()方法

StringBuffer到String：StringBuffer调用substring()获取子串、使用String的构造函数、StringBuffer调用toString()方法得到String对象

3.位运算的交换律成立，结合律与分配律不成立

#### 12.7

1.Docker：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326593552-03ac8ccb-1814-4370-8187-8f03cc8cc18b.png#averageHue=%23fbfafa&clientId=u2b3c7fce-a209-4&from=paste&height=373&id=u900333f4&originHeight=587&originWidth=1282&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=338776&status=done&style=none&taskId=udff83550-f657-46e6-9532-3861c937479&title=&width=813.5377557039088)


#### 12.8

1.Hinge loss(合页损失函数)用于二分类问题，常用于支持向量机(SVM)

![](https://www.yuque.com/api/services/graph/generate_redirect/latex?L(y%2Cf(x))%3Dmax(0%2C1-y*f(x))#card=math&code=L%28y%2Cf%28x%29%29%3Dmax%280%2C1-y%2Af%28x%29%29&id=zRRhk)，y是真实标签，通常为+1或者-1，当![](https://www.yuque.com/api/services/graph/generate_redirect/latex?y*f(x)#card=math&code=y%2Af%28x%29&id=ChJUf)小于1时，即预测不准确或错误，合页损失是正的。当![](https://www.yuque.com/api/services/graph/generate_redirect/latex?y*f(x)#card=math&code=y%2Af%28x%29&id=NnyB5)大于等于1时，预测正确，合页损失为0

2.常见的距离度量方法：

欧氏距离：![](https://www.yuque.com/api/services/graph/generate_redirect/latex?d(A%2CB)%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En(%7BA_i-B_i%7D)%5E2%7D#card=math&code=d%28A%2CB%29%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%28%7BA_i-B_i%7D%29%5E2%7D&id=HvhZ8)

曼哈顿距离：![](https://www.yuque.com/api/services/graph/generate_redirect/latex?d(A%2CB)%3D%5Csum_%7Bi%3D1%7D%5En%7CA_i-B_i%7C#card=math&code=d%28A%2CB%29%3D%5Csum_%7Bi%3D1%7D%5En%7CA_i-B_i%7C&id=ss0QL)

余弦相似度，用于衡量两个向量方向上的差异，可以转化为距离度量：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326611922-f2ee40ff-0331-4f91-b1e6-e667f7e371b5.png#averageHue=%23fefefe&clientId=u2b3c7fce-a209-4&from=paste&height=69&id=u3e692c72&originHeight=109&originWidth=840&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=29640&status=done&style=none&taskId=uec62bafb-7db5-4d68-bb81-ec7cdb087b4&title=&width=533.0512595875846)

余弦距离可以表示为![](https://www.yuque.com/api/services/graph/generate_redirect/latex?1-cosine%5C_similarity(A%2CB)#card=math&code=1-cosine%5C_similarity%28A%2CB%29&id=nVWdR)

#### 12.9

1.Dropout：训练时使用，测试时禁用

批归一化(BN)：训练和测试时都使用，但训练时使用实时批次统计，测试时使用训练期间的全局统计

BN中的可训练参数![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5Cgamma#card=math&code=%5Cgamma&id=PIY0y)和![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5Cbeta#card=math&code=%5Cbeta&id=ascX0)使得网络在进行必要的标准化同时，还能保持足够的能力来学习和表示数据的特定特征

#### 12.10

1.题目结果需要mod的，一般有两种方法：记忆化/dp这种可以有个递推式的，或者组合数/幂次这种可以得到很大的数的

2.python中生成器是一种特殊的迭代器，生成器按需生成值，而不是一次性计算出所有值存储在内存中。

迭代器与可迭代对象不同，list以及range(100)都是可迭代对象，通过iter(可迭代对象)函数获取迭代器，迭代过程通过不断next(my_iter)获取下一个元素实现

3.python中两个存储的值相同的变量的id是否相同的问题：

对于不可变对象，为了优化性能，会对小的整数（-5到256）以及短的字符串进行预先分配，在运行中重复使用这些对象，id一样，比如a、b两个变量都为1，其id一样。对于其他情况的不可变对象，id不同

对于两个值相同的可变对象，id不同

#### 12.11

1.![](https://www.yuque.com/api/services/graph/generate_redirect/latex?y#card=math&code=y&id=UUI2G)表示真实值，![](https://www.yuque.com/api/services/graph/generate_redirect/latex?%5Chat%7By%7D#card=math&code=%5Chat%7By%7D&id=isWDt)表示预测值

#### 12.13

1.在**数据预处理**中，连续值的特征需要进行一些预处理以提高模型性能和稳定性：

归一化(Normalization or Min-Max Scaling)：![](https://www.yuque.com/api/services/graph/generate_redirect/latex?x'%3D%5Cfrac%7Bx-min(x)%7D%7Bmax(x)-min(x)%7D#card=math&code=x%27%3D%5Cfrac%7Bx-min%28x%29%7D%7Bmax%28x%29-min%28x%29%7D&id=Q9PEF)通常把数据缩放到0和1之间，或者到-1到1之间但不常用

标准化(Standardization)：![](https://www.yuque.com/api/services/graph/generate_redirect/latex?x'%3D%5Cfrac%7Bx-%5Cmu%7D%7B%5Csigma%7D#card=math&code=x%27%3D%5Cfrac%7Bx-%5Cmu%7D%7B%5Csigma%7D&id=mooGB)使数据具有0的均值和1的标准差，不是会变成正态分布，标准化并不改变数据的分布。之所以会说标准化后数据变成标准正态分布，是因为原始的数据就是符合正态分布的，只不过不是标准正态分布。

不同的特征往往具有不同的量纲和量纲单位，为了消除特征之间量纲的影响，需要进行归一化、标准化处理，以解决数据之间的可比性。

而在**深度学习**中，Batch Normalization更接近上面说的标准化：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326634309-29b9eb15-146e-4e78-a1f4-cfe907ba1aec.png#averageHue=%23fefefe&clientId=u2b3c7fce-a209-4&from=paste&height=96&id=uec400bd4&originHeight=152&originWidth=661&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=26760&status=done&style=none&taskId=uf5917cab-58cf-46cf-9741-514e7ed2f05&title=&width=419.46057450880164)

2.模型的方差与偏差：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326642041-e6697672-398f-47a8-9e9e-62ac4a98d01b.png#averageHue=%23d0c4bc&clientId=u2b3c7fce-a209-4&from=paste&height=175&id=uc4456cad&originHeight=275&originWidth=1280&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=272981&status=done&style=none&taskId=u6798a47f-8b6b-4835-b587-f838ed3b38e&title=&width=812.268586038224)


靶心是测试集的真实值，靶子上的每个点都是用训练集（用不同的训练数据子集）训练一次后在测试集上的结果，不是一次训练后所有样本的结果(我这种理解更接近误差的概念)

高偏差：表示模型误解了数据的真实模式，在训练集和测试集上都表现不好，即“欠拟合”，偏差关注模型在训练集上的表现，是否能捕捉到基本的数据模式

高方差：表示模型对于训练数据过度敏感，导致预测结果波动很大，在训练集表现好，在测试集表现不好，即“过拟合”，方差关注模型对不同训练数据子集的敏感性，以及这种敏感性如何影响模型在测试集上的表现。

3.过拟合总结

过拟合的表现：

在训练集表现好，在测试集上表现不好

过拟合的原因：

模型复杂度太高、数据量太小、训练集和测试集分布不一致、数据质量差(有噪声)、过度训练

过拟合的解决方案：

模型层面：使用复杂度不高的模型、loss中加正则化、dropout、加BN层、使用残差结构

数据层面：加数据、在切分数据集时候保证分布一致性

训练层面：early-stopping

#### 12.15

1.双指针遇到的小问题：
双指针满足的二段性分两种：一种是left右侧满足题意，此时while出去后left是满足题意的第一个位置，根据left到i的距离更新ans即可
还有一种是left左侧满足题意，此时while出去后因为left此时位置是不满足题意的第一个位置，可能需要判断一下left是否为0，即left左侧存在满足题意的位置，才可以更新ans，否则不更新ans，见[LC长度最小的子数组](https://leetcode.cn/problems/minimum-size-subarray-sum/description/)。

**注意：**两种情况都在while外更新ans。并且由于left是满足或者不满足题意的第一个位置，在跳出while循环后，left可能为i+1，while的第一个条件加上left<=i防止left提前跑到i后面很多的位置

#### 12.18
1.AUC指标
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703326669293-e3ec7cc0-ac4e-40ca-a0ac-f0fd95de1cd3.png#averageHue=%23fcfbfb&clientId=u2b3c7fce-a209-4&from=paste&height=396&id=u676914f9&originHeight=792&originWidth=1259&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=451892&status=done&style=none&taskId=uffb788f1-d702-4163-993c-7dd739f104c&title=&width=630)
TPR也就是召回率，以TPR为纵轴，FPR为横轴，调整正负样本划分阈值画出ROC曲线，与横轴围成的面积就是AUC指标

#### 12.23
1.sudo命令的常用参数
-u <用户>：以指定的用户身份执行命令，如果不使用-u，默认以root用户身份执行，例如sudo -u username command
-s ：以shell执行命令，这将启动一个root shell（或者通过-u指定的其他用户的shell）
-i：模拟登录环境，sudo -i将启动一个登录shell，将会加载用户的环境变量等

#### 12.24
1.token解释：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703408610683-1852b1ab-ef5e-4264-b221-d99600685d87.png#averageHue=%23fcfaf7&clientId=u3191f929-ccb0-4&from=paste&height=228&id=ucc836ad2&originHeight=455&originWidth=1048&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=178566&status=done&style=none&taskId=u291fbe63-e1af-44c6-914a-01a88609ec9&title=&width=524)

2.Attention以及Self-Attention机制讲解：
[史上最小白之Attention详解_target attention-CSDN博客](https://blog.csdn.net/Tink1995/article/details/105012972)
Attention图示：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703409280226-b1cde84e-4fab-40c7-bb51-d1a78d6c45d5.png#averageHue=%23f9f8f8&clientId=u3191f929-ccb0-4&from=paste&height=272&id=u58acfea1&originHeight=544&originWidth=1123&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=147251&status=done&style=none&taskId=u0cf720a1-fca7-441a-adb7-2b26d2fb3ef&title=&width=562)
就是通过计算Q与所有K的相关性后Softmax，再乘对应的Value得到这个Q的Attention Value值（一个embedding），这种普通的Attention也叫Target Attention
而Self-Attention只是Attention的一种特殊情况，即Q K V都是根据同一个向量得来的，而普通的Attention中，Q与K V不是同一个向量得来的
本质思想：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703409518257-b884f1f0-4ae3-4e7d-9d01-0e0eaa11a19f.png#averageHue=%23ececec&clientId=u3191f929-ccb0-4&from=paste&height=43&id=ud3f4bc91&originHeight=86&originWidth=1123&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=50304&status=done&style=none&taskId=uef423e5c-1279-449c-9f24-76bed652e9c&title=&width=562)
其中![](https://cdn.nlark.com/yuque/__latex/2bf6dd7a40f1082e7fd63db667ddcdbb.svg#card=math&code=Similarity%28Q%2CK_i%29&id=bpjkZ)计算方式有以下几种：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40887030/1703409612188-e9871881-d42c-49e8-9f6a-eb8427e87f42.png#averageHue=%23e0dbd3&clientId=u3191f929-ccb0-4&from=paste&height=122&id=u3ca5f3b0&originHeight=243&originWidth=1123&originalType=binary&ratio=1.5758334398269653&rotation=0&showTitle=false&size=196292&status=done&style=none&taskId=u868a3024-856d-4d02-a3a6-e3000f345ea&title=&width=562)
MLP网络的方式用的比较少，Transformer中用的点积的方法

Transformer讲解：
[史上最小白之Transformer详解-CSDN博客](https://blog.csdn.net/Tink1995/article/details/105080033?spm=1001.2014.3001.5502)
Encoder中的Multi-Head Attention是**基于Self-Attention**的，Decoder中的Masked Multi-Head Attention也是基于Self-Attention的，因为两者的QKV都是用相同的东西得来的
而Decoder中第二次做Attention，也就是Multi-Head Attention**只是基于Attention**，因为Q来自Masked Multi-Head Attention的输出，而K和V来自Encoder中最后一层的输出

3.ipconfig出来的是内网ip，公网ip要从网上查
公网ip地址代表的是互联网上的一个独一无二的地址，这个地址可以分配给任何接入互联网的设备，无论是路由器、主机（比如个人电脑、服务器等）、打印机。路由器或者主机都可以应答ping，所以ping公网ip不需要找到对应主机，路由器也能回复
ping命令后的ip可以是内网ip也可以是公网ip

#### 12.25
1.linux输出重定向：
格式：command > txt     其中>是将command的标准输出覆盖文件，文件如果不存在则创建；>>将command输出添加到文件内容最后
"2>" 将标准错误重定向到文件并覆盖，"2>>"将标准错误添加到文件
"&>"将标准输出和标准错误都重定向到文件 ，"&>>"标准输出与标准错误追加到文件
linux中1表示标准输出，2表示标准错误，">"中1被省略了

输入重定向用的很少，就是把文件中内容作为命令的输入

"sh train.sh >> ./daily.log 2>&1" 把train.sh输出重定向到daily.log，并且把标准错误输出也输出到标准输出重定向的文件中。写成如下也一样："sh train.sh &>> ./daily.log"

2.linux执行命令：
首先shell会检查是否为内建命令，其次会查找$PATH环境变量指定的目录下的可执行命令。执行当前路径下的某文件，需要./wenjian.sh，而不是直接wenjian.sh，因为这会将其认定为是某个命令而不是执行该文件。或者sh wenjian.sh得了，py文件就是python wenjian.py，这样统一也好记

3.jdk安装路径不能有空格，否则后续可能报错JAVA_HOME is incorrectly set
#### 12.27
1.命令行的命令的选项与参数区别：
与-或者--开头的是命令的选项，比如ls -l，其中l是ls的选项。而有些选项可能会有值，会紧跟在该选项后面，而该命令的参数出现在所有选项之后，注意区分两者
而命令的参数是不用横杠的，比如python test.py，test.py是python命令的参数。比如ls -l Documents中，l是选项，Documents是参数。
执行py文件传参数时，如果不用argparse则不需要加-，如果用了则需要-argname value的格式

2.shell中命令的选项用-还是--的问题：
选项名只有一个字符，用-。选项名是一个字符串用--。
但java特殊，一般都用-，比如java -version    java -help

3.linux中wc命令用于计算文件的字节数或者行数或者字数，-c统计字节数、-l统计行数、-w统计字数
crontab命令是cron table的简写，是作业列表 crontab -e编辑内容，每一行由时间+动作构成
[Linux Crontab 定时任务 | 菜鸟教程](https://www.runoob.com/w3cnote/linux-crontab-tasks.html)

#### 12.29
1.conda install时候报错：Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve. Solving environment: failed，配置清华镜像源即可
之后在base环境中conda install时候在Solving environment时候非常慢，原因是conda会计算当前环境与要安装的包的一致性与兼容性，如果当前环境已经有了很多的包，conda会花费更多时间计算兼容的版本
解决：创建一个新环境或者用pip，因为pip不会进行依赖解析

