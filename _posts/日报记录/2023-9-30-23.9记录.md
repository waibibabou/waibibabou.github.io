---
layout: post
title: 2023年9月日报
subtitle: 冲冲冲
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.jpg
---

##### 9.1

1.同一个py文件无法同时在ide中运行并且debug，但是可以在命令行运行，在ide debug

2.<实验记录>actor最后一层的tanh激活函数去掉，换成linear、 correct判断小改、 令state中全是>3的电影，将这三处都改了后via代码训练过程中，q_loss变得非常大，precision全是0

而后续让actor最后一层还是tanh，训练变得正常

[via训练过程]: ../实验记录/via训练.md

3.<实验记录>via的评测阶段，如果对所有user的ndcg做平均：ndcg@10为0.413，如果对所有次的evaluation直接做平均ndcg@10为0.638

##### 9.2

1.<实验记录>再改了两个小问题使得状态中都是>3的电影后via代码的评测，对所有user做平均结果：结果反而下降了ndcg@10为0.391，后续又train了一次ndcg为0.400，还是略微下降了

[via改 eval]: ../实验记录/via改.md

不过后续用训练中途得到的网络权重以及随机权重做了eval，ndcg分别为0.36、0.052说明训练还是有效果的

##### 9.3

1.python中阶乘使用math.factorial()，计算排列组合$A_m^n$使用math.perm函数   计算$C_m^n$使用math.comb函数

2.非零值都为True，即-1也是True

##### 9.4

1.使用下划线开头的函数或类无法使用from xxx import *导入，除此之外还可以使用模块提供的$\_\_all\_\_$变量，该变量为一个list存储当前模块中的变量、函数、类名称，在其他模块用from xxx import *导入时只能导入all中指定的成员

2.python中@是装饰器用来标记函数或类方法，@staticmethod与@classmethod都是类方法的装饰器，@staticmethod不能访问或修改类状态或者实例状态，可以通过类名调用或者实例调用。@classmethod接受一个特殊的第一个参数cls，表示类本身，该方法可以访问修改类状态，但不能访问修改实例状态，可以通过类名调用或者实例调用。

总结：如果需要访问修改类的状态，用@classmethod，如果只执行与类和实例都无关的任务，用@staticmethod

3.nn.Parameter生成一个tensor，requires_grad默认就为true，并且自动加到模型参数列表中，后续通过optimizer.step()更新

##### 9.5

1.类中的特殊方法可以使得该类的实例与python内置函数相互作用，比如$\_\_getitem\_\_$使得可以使用索引操作符[]获取元素，$\_\_len\_\_$使得可以使用len()函数

2.模型有可学习的参数时用nn.Module，否则可以用nn.Module也可以用nn.functional比如激活函数、池化层

dropout虽然没有参数但是需要用nn.Module因为其在train与eval阶段行为有差别，使用nn.Module能够通过model.eval()操作加以区分

3.bfs中的状态可能不仅需要记录位置，还需要记录额外信息比如上一步的执行方向

##### 9.7

1.taobao.txt这数据集每个time最后一个都是click

##### 9.10

1.python中虽然int可以存储无限大，计算到最后再mod也能得到正确结果，但是当数字过大时候运算会很耗时，并且由于数字过长可能出现超出内存限制的情况，所以最好还是计算过程中就mod

##### 9.12

1.直接与原文结果比较虽然说不是绝对公平的，可能测试集分割的不同，但是当数据量足够大的情况下，认为数据的好不好预测的程度相同，所以直接比也是可以的，但是如果有源码还是需要自己跑一遍源码看看结果

##### 9.14

1.nn.ModuleList()将list中各个层的参数自动注册到model中，如果用原生list不会自动注册。不会自动执行向前传播，需要手动一个个调用其中的layer

nn.Sequential()会自动执行前向传播，而不需要一个个调用其中的layer，但这就会缺乏灵活性，不易于进行复杂的向前传播逻辑

##### 9.15

1.线性dp中的经典问题

[数值域小]: https://leetcode.cn/problems/maximum-earnings-from-taxi/

这种情况可以对数值域用双指针进行dp或者对给的n个taxi用二分进行dp



[数值域大]: https://leetcode.cn/problems/maximum-profit-in-job-scheduling/

而这种情况不能对数值域dp，只能对n个job用二分进行dp了

2.github又clone不了了有时报错connection was reset或者time out，但是github.com可以ping通，过了一天之后又不报错了

##### 9.20

1.defaultdict(defaultdict(int))会报错，使用defaultdict(Counter)或者defaultdict(lambda :defaultdict(int))

2.dfs判断何时需要还原现场，比如将$visited[i][j]=0$，如果在dfs的另一条路径中也会访问到该状态，则将该状态的$visited[i][j]=0$，如果不需要再访问该状态了，则其一直为1即可

##### 9.21

1.torch.nn中的常用loss函数：

回归任务（Regression Tasks）

1. Mean Squared Error Loss (`nn.MSELoss`): 均方误差损失（L2 loss），适用于回归问题。
2. L1 Loss (`nn.L1Loss`): 绝对值误差（L1范数）损失，也用于回归。
3. Smooth L1 Loss (`nn.SmoothL1Loss`): 平滑L1损失，用于处理离群点（outliers）结合了L1 loss以及L2 loss。
4. Huber Loss (`nn.HuberLoss`): 类似于 Smooth L1 Loss，但提供了一个额外的`delta`参数来调整敏感度。

分类任务（Classification Tasks）

1. Cross Entropy Loss (`nn.CrossEntropyLoss`): 交叉熵损失，用于多类别分类。=log_softmax+nn.NLLLoss(负对数似然loss)
2. Binary Cross Entropy (`nn.BCELoss`): 二分类问题中的交叉熵损失。
3. BCE With Logits Loss（nn.BCEWithLogitsLoss）：结合Sigmoid激活函数和Binary Cross Entropy损失函数
4. NLL Loss (`nn.NLLLoss`): 负对数似然损失，通常在网络的最后一层使用 softmax 之后使用。

生成模型（Generative Models）

1. Kullback-Leibler Divergence (`nn.KLDivLoss`): 用于衡量两个概率分布的相似度。

其他特定任务

1. Cosine Embedding Loss (`nn.CosineEmbeddingLoss`): 用于衡量两个向量的余弦相似度。
2. Margin Ranking Loss (`nn.MarginRankingLoss`): 用于排序和推荐系统。
3. Hinge Loss (`nn.MultiMarginLoss` or `nn.HingeEmbeddingLoss`): 用于支持向量机（SVM）。

##### 9.25

1.推荐根据任务分类：

ctr预测：在广告推荐中常见，任务是预测用户是否会点击某个推荐的广告

评分预测：预测用户对尚未评分的物品可能给出的评分

topn推荐：为用户推荐一个n个物品组成的列表，常用协同过滤、矩阵分解等等

序列推荐：预测用户下一步最可能与哪个物品进行交互，这类推荐系统特别关注用户行为的时间顺序，主要有时间的概念，比如GRU4Rec、Transformer、强化学习等，要对用户在当前时刻前的历史记录做嵌入

分组推荐：为一个用户群体(而不是单一用户)推荐物品

多目标推荐：在满足多个目标（如准确性、多样性、解释性）的前提下进行推荐

实时推荐：在用户与系统实时互动过程中生成推荐，比如新闻推送

跨域推荐：使用一个领域（如电影）的数据来推荐另一个不同领域（如书籍）的物品

解释性推荐：生成可以解释或理解的推荐，以增加用户的信任和满意度



leetcode  2806   1099    codefun  78  total  1177
