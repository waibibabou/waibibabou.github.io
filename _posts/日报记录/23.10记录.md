##### 10.6

1.L2正则化项通常不加到loss函数中，而是直接在定义optimizer时候指定weight_decay参数，因为两者等价，并且可以减少计算loss以及反向传播的计算量。

而L1正则化项不能通过optimizer定义，需要加到loss中

2.L2正则化==权重衰减(weight decay)

##### 10.7

1.tensorboard打开网站后没有图像，显示没有画任何数据。可能是输入tensorboard --logdir=。。。时候的路径有问题，退回到记录数据的上一级目录即可

2.我发现ddpg中计算critic的td误差时，q_now用的是经验回放中存储的action，这个action是加了噪声的，而计算q_target是将target_actor直接的输出的action没加噪声的

3.在不同的py文件往一个tensorboard上画时候报错：E1007 22:13:09.027264 25924 directory_watcher.py:254] File runs\experiment_1\events.out.tfevents.1696687683.zyk8p.2144.0 updated even though the current file is runs\experiment_1\events.out.tfevents.1696687683.zyk8p.2144.1

将画图的步骤放在一个文件中不再报错

##### 10.8

1.评测指标MAP（mean average precision）：

首先计算AP：
$$
AP=\frac{1}{Rel}\sum_{k=1}^nP(k)*rel(k)
$$
P(k)是在前k个推荐结果中相关项的比例，rel(k)是个指示函数，当第k个结果是相关项时为1，否则为0。Rel是结果的相关项总数，n是推荐结果的数量
$$
MAP=\frac{1}{Q}\sum_{q=1}^QAP_q
$$
Q是查询（或用户）的总数，AP是第q个用户的AP

2.<实验记录>

[my_original]: ../实验记录/my/original.md

##### 10.12

1.dfs递归题目有两种类型，一个是递到树的某一位置或者到底部时候计算一个全局的指标，另一个是在归的过程中把指标一层一层return回去

2.用gpu将my项目original跑了，变快非常多，但模型训练极其不稳定

3.由于推荐里done信号也是人为加上的，尝试了下在agent计算q_target时候将done信号去掉，结果episode_reward根本跑不上去。又跑了几次，没有done信号真不行。但是好像在测试集上的指标还行啊

##### 10.17

1.chardet库可以检测出文件的编码方式

##### 10.21

1.tqdm用法：

（1）自动更新：

将迭代对象包裹在tqdm()中即可，每次迭代，进度条都会自动更新  for i in tqdm(range(100)):

（2）手动更新：

```python
#手动更新常用with关键字
with tqdm(total=10) as pbar:
    for i in range(10):
        time.sleep(0.5)
        # 更新进度
        pbar.update(1)
        # 在进度条后面添加或修改后缀信息
        pbar.set_postfix(stage=f"step {i+1}", current_value=i**2)
```

注意只有手动更新时候用set_postfix才很方便

##### 10.22

1.模型可视化主要有：

(1)直接print模型：这种如果某一层的输入输出没有对上不会报错

(2)torchsummary：可以在终端输出模型的各个层，各个层输入输出不匹配会报错，但这种对于模型如果有多个输入的情况不友好

(3)tensorboard：利用add_graph函数，可以处理多个输入的情况

2.运行一个py文件时，os.getcwd()返回当前工作目录，这是启动脚本的地方，而不是被调用的脚本文件的位置。也就是，如果一个py文件调用了另一个py文件中的函数，两者的当前路径是一样的，所以导致了找某文件的时候，ctrl+左键可以找到，但是用路径找却找不到的情况

3.在online测试的时候，对于同一个用户在liked_items不同的情况下，推荐的结果都一样，思考后感觉是因为：在训练时候得到的r相当于只与uid有关，所以会对同一user推荐相同的item因为agent认为这些item这个user会喜欢，并没有考虑user此时的状态。debug后发现同一user在不同状态下得到的state_embed不一样，但是action_embed非常相近，导致推荐的物品都一样：

![Snipaste_2023-10-17_16-53-15](D:\TyporaPicture\23.10记录\Snipaste_2023-10-17_16-53-15.png)

应该是actor网络的第一层和state_embed中的item_embed的连接的权重都被训得非常小了

##### 10.23

1.log没有下标通常为ln而不是$log_{10}$

##### 10.26

1.判断一个点是否在多边形内部：射线法，从该点发射一条水平射线，统计与多边形边界的交点数，如果交点为奇数，点在多边形内；若为偶数，点在多边形外。为确保交点有效，我们检查线段是否跨越射线并确保交点在该点的右侧

##### 10.27

1.



