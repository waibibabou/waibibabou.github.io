##### 5.2

1.看完了修师姐论文

2.渐进时间复杂度一致，但是如100n就不能通过，50n就可以

##### 5.3

1.消消乐问题考虑用栈解决

2.堆优化的dijkstra，当pop出来后判断是否为终点直接返回会快很多

##### 5.4

1.推荐组会：用自然语言处理解决多种推荐任务，推荐中很少用预训练，nlp中常用

2.NDCG：Normalized Discounted Cumulative Gain归一化折损累计增益，用于评估排序结果
$$
DCG=\sum_{i=1}^k\frac{rel(i)}{log_2(i+1)}
$$
IDCG是根据rel(i)降序排列，即最好序列
$$
NDCG=\frac{DCG}{IDCG}
$$

3.BPR loss要求模型将用户更喜欢的物品i排在更不喜欢的物品j之前

4.python中for while没有自身作用域

5.python中负数可以%mod，负数自动加mod

6.隐式反馈：用户的浏览、购买、点击 ；显式反馈：用户的评分

##### 5.5

1.model-based在RS用的少的原因

![image-20230505145647334](D:\TyporaPicture\5月日报\image-20230505145647334.png)

2.model-based中的P不一定是显式的：

![image-20230505172241297](D:\TyporaPicture\5月日报\image-20230505172241297.png)

3.子数组：一个连续的数组，子序列：不一定连续

4.子数组关于统计个数的，双指针也能统计，双指针或者前缀和+哈希表选哪个主要看是否具有二段性

##### 5.7

1.《Generative Adversarial User Model for Reinforcement Learning Based Recommendation System》

model-based RL采样效率更高，提出了一个model-based模型，学习了一个user behavior model以及reward function

用户行为模型作为生成器，奖励函数作为判别器用于区分用户真实行为以及生成器生成的行为。

lc  1576 882  codefun 49  total 931

##### 5.8

1.ndarra矩阵乘法np.matmul()或者@，矩阵对应位置相乘np.multiply()或者*，不要用np.dot()

2.显式反馈的reward就可以直接是评分，隐式的需要自己设定

3.一般可以通过pre-train一个MF模型得到用户以及物品的特征向量embedding，并且感觉embedding向量可以预先训练后固定，也可以跟着一起训练

4.《Interactive Recommendation with User-Specific Deep Reinforcement Learning》$s_{t+1}$的embedding每次都要用MF得到

对于没有评分的位置，文中用了两种方法处理，一是将推荐候选集限定在有分数的物品中，二是认为评分是0

5.<img src="D:\TyporaPicture\5月日报\image-20230508162056109.png" alt="image-20230508162056109" style="zoom:67%;" />

6.隐式反馈的数据集：对于数据集中没有的item，可以将推荐候选集限定为session中出现的物品，也可以用list-wise论文中构建一个模拟器，也可以用历史数据构建一个model-based强化学习模型，就可以将候选集扩充到所有物品

显示反馈的数据集：对于数据集中没有的item，可以将推荐候选集限定为该用户出现过的评分item，也可以认为评分是0(但是效果可能不太好)，或者用MF填充一下评分矩阵

7.jupyter notebook debug不好用啊，断点打在不同cell识别不到，不如整体复制到py文件中debug

8.推荐中model-free的模型不知道P是因为不知道采取动作后用户的反馈，所以不知道下一时刻的状态，不知道反馈也就不知道采取这个推荐动作后的奖励值，即不知道奖励函数，并不是说不知道用户做出反馈后的下一个时刻的转移方式，转移方式都是用户喜欢哪个，将其加在s后面，不喜欢则不影响s

##### 5.9

1.云计算组会，分层强化学习，先输出目标，再针对该目标输出动作

amortized Q-learning（2020），动作空间太大情况下，找最大值困难，学习一个分布，选出动作的候选项集，计算这个候选集中的Q值找最大值

2.python海象运算符:=，py3.8版本引入，可以看成赋值运算符，整体的运算结果是海象运算法左边的值，引入该运算符是因为python中赋值运算符是没有运算结果的，海象运算符有结果。

3.pypy3比python3快非常多，如果有该选项选pypy3，代码不需要修改

4.tf2报错ValueError: Input tensors to a Functional must come from tf.keras.Input. Received: 0 (missing previous layer metadata). tensorflow版本问题导致，使用1.0版本方法，禁用2.0版本方法即可，不需要配置1.0的虚拟环境

import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()

5.看了一下LIRD-master代码，对应论文list-wise，用DDPG那篇，不理解state,action,next_state生成的方式，并且电影embedding生成方式很随意，网络定义与训练用tensorflow1.0版本写的，根本看不懂。作者zhao xiangyu，经常看到

6.pycharm程序点终止后依然没有结束，可以在左侧将其运行页面叉掉即可

7.np.save()可以保存所有类型数据，用np.load()加载。torch.save()保存tensor类型数据，用torch.load()加载

##### 5.10

1.求一个很长的数字除k的余数，可以通过(前n-1位数字的余数*10+int(最后一位))%k得到

2.英文缩写:  e.g.举个例子    i.e.即    etc.等等   et al.等等,列举人   w.r.t. with respect to关于,谈到    s.t. subject to受限制于 RQ research question要回答的问题

##### 5.11

1.推荐组会，解耦长期、短期兴趣

##### 5.12

1.背包问题

背包分类：

01背包：每个物品数量为1，选择装与不装，正序遍历物品，倒序遍历空间

完全背包：每个物品数量无穷，正序遍历物品，正序遍历空间

多重背包：每个物品数量有限，将每个物品展开，变成01背包，正序遍历物品，倒序遍历空间

混合背包：综合前三种背包，将数量有限的展开成01背包，数量无穷的按照完全背包做

分组背包：每组物品若干，同一组内物品最多选一个，最外层遍历所有组，倒序遍历空间，正序遍历物品，注意因为同一组内物品相互排斥，需要先遍历空间再遍历物品

任务分类：

最值问题：$dp[i]=max/min(dp[i],dp[i-cost[j]]+value[j])$

存在问题：$dp[i]=dp[i]\ or\ dp[i-cost[j]]$

组合问题：$dp[i]+=dp[i-cost[j]]$

2.codefun上遇到runtime error不懂为什么，看不到样例

##### 5.13

1.康师姐论文《Imitation Learning Enabled Fast and Adaptive Task Scheduling in Cloud》将模仿学习应用到离线以及在线schedule过程，其与cloudsim交互认为是在线过程。离线训练时候的专家轨迹由启发式算法得到（这部分没懂）

2.python中根本不需要担心int溢出问题，mod可以最后$return\ ans\%mod$甚至比每一次加时候都mod还要快

3.位运算的左移<<  右移>> 比乘法除法快非常多

4.有序是一个非常好的性质，如果排序不影响结果则可以优先考虑排序

##### 5.14

1.A\^B=B\^A  (A\^B)\^C=A\^(B\^C)

2.并查集对所有的边执行完union后parent数组不是直接都指向root，需要调用find函数才能知道root节点

3.TD3相对于DDPG的三点优化：1）critic部分：只有在计算critic的更新目标时才需要用到target network，其中包含一个policy network以及两个Q network，$Q1(A')$和$Q2(A')$取min代替DDPG中的Q计算target，$target=r+\gamma*min(Q1,Q2)$

target是两个Q网络的target，虽然更新目标一样，两个网络都会逐渐与实际Q值相同，但是参数初始值不同，导致计算得到的Q值不同，还是有一定空间选择较小的值去估算Q值，避免Q值被高估。

2）actor的延迟更新，让critic更新多次后再对actor更新。让critic更加确定后，actor再行动

3）在计算target时，输入s'到actor输出a后，给a增加噪声，相当于用一小块范围内去估算target，使得估计出值更准确。在DDPG中噪声只加在action的获取中，在计算target时候不加噪声。

lc  1599  899  codefun 68  total 967

##### 5.15

1.《Continuous Input Embedding Size Search For Recommender Systems》之前的寻找最优嵌入维度的方法都是在一个小的离散候选集中选一个，而这个提供一个细粒度的连续的空间进行选择，当然这里的连续指的是连续的整数。

embedding维度高，准确率更高，消耗内存更高，利用RL寻找两者的平衡。

对于每个user/item得到最优的嵌入维度后，embedding table其中大多数为0，存储稀疏矩阵，对于0的存储几乎不消耗空间，达到减小内存使用的目的。可你用RL找最优的嵌入维度过程中不也是需要保存矩阵中所有值的吗，内存没有减少使用，是不是有点多此一举。想了想还是有用的，这个算法中虽然内存没有减少，但是得到的对于每个user以及item的embedding来说，内存减小了，并且是在没有过多牺牲准确性的情况下。
$$
eval(e_u|\cdot)=\frac{\sum_{k\in{K}}Recall@k_u+NDCG@k_u}{2|K|},u\in{U}
$$
对于item的eval，由与其交互过的user的eval取平均得到。

对于reward的设计，需要体现出推荐质量以及空间复杂度：
$$
r_n=q_n-\lambda(\frac{d_n}{d_{max}})^2
$$
可以对于a进行随机游走的原因，在更新网络参数时，s,a,r,s'，对于actor网络需要将s输入actor得到a在用critic得到q值，所以在这个四元组中，与环境交互时执行的a不需要能对actor的输出值可导。

可以考虑利用随机游走增大动作的探索程度。

最后在actor执行过的动作中，挑选出稀疏度大于c的，效果最好的embedding维度用于推荐模型，得到recall以及NDCG。

质疑：这真的是一个序列问题吗，在不同s下执行相同的a，s'不是一样的吗，老师讲说这是之前那种的特殊情况相当于把前面的全挤出去了，相当于维护了一个长度为1的list作为状态。

##### 5.16

1.python中*与**总结：

*乘法、重复次数，**次方

1）收集列表中多余的值

无需确保值与变量个数相同:$a,b,*c=[1,2,3,4]$，c是[3,4]注意这里收集到的是list

而涉及到函数时，*、**作用在于收集参数或者分配参数

2）在定义函数时，*收集参数到tuple，**收集关键字参数到dict

$def\ myprint(*params):$将收集到的参数放在一个tuple中

$def\ myprint2(**params):$得到一个dict$\{'z':3,'x':1,'y':2\}$用于收集关键字参数

3）调用函数时，*分配tuple中的参数，**分配dict中的参数

$def\ myprint(x,y):$

params=(1,2)可以通过$myprint(*params)$调用

params={'x':1,'y':2}可以通过$myprint(**params)$调用，与形参名称匹配

4）定义和调用函数都用*没有实际意义，不如不用

2.python下划线总结

1）单前置下划线的命名：

作为类名或函数名时：会阻止其他文件通过from module import *导入该名字，即改名字不会与星号匹配，可以通过import该名字导入

作为类的属性名或者方法名时：不希望直接访问该名称，只是一种命名约定，但是python解释器不会对这种属性名做特殊处理

2）前后均带有双下划线__的命名：

用于特殊方法的命名，用来实现对象的一些行为或功能：

${\_\_new()\_\_}$方法用于创建实例

$\_\_init\_\_()$方法用来初始化对象

x+y操作被映射为方法$x.\_\_add\_\_(y)$，序列或者字典的索引操作x[k]映射为$x.\_\_getitem\_\_(k)$

$\_\_len\_\_()$、$\_\_str\_\_()$分别被len()、str()调用

3）仅开头带双下划线__的命名：

用于对象的封装，以此命名的属性或者方法为类的私有属性或私有方法，无法从外界直接访问，通过自动变形实现，$\_\_name$自动变为‘ $\_类名\_\_name$ ’的新名称。这种机制可以阻止继承类重新定义或者更改方法的实现，调用时还是运行父类中的私有方法，而不是子类重写的方法。

3.跑NCF代码batch_size为256时报错RuntimeError: CUDA out of memory. Tried to allocate 5.91 GiB，将batch_size改为16后不再报错

4.配置服务器流程：

创建一个新python项目，创建temp服务器，取消勾选visible only for this project，配置映射，上传到pythonProject，删除temp服务器。这一步是为了在选择解释器映射路径时，不要用tmp下的路径。

添加python解释器，映射路径选择刚才上传到的pythonProject，不要选择tmp的路径。分别添加pytorch以及transformer的解释器，看到ide下面一条的左侧有两个lab_server，分别对应两个解释器，有几个解释器，就有几个lab_server，后面执行时候虽然只需要改变解释器但是也不要将lab_server删除，否则会报错：

Error running 'main': Cannot find remote credentials for target config com.jetbrains.plugins.remotesdk.target.webDeployment.WebDeploymentTargetEnvironmentConfiguration@37f07bdf并且与该lab_server对应的解释器会显示nothing to show，并且无法测试是否能成功连接

将lab_server、lab_server(2)改名为lab_server_pytorch、lab_server_transformer，顺序与解释器顺序一致，两者root路径自动检测，改下映射路径，注意都要取消勾选visible only for this project。将解释器改名为lab_server_pytorch、lab_server_transformer

配置一个新项目时候，如果只选择了pytorch的lab_server，那么用transformer解释器时会报错：Failed to prepare environment，所以需要用哪个解释器就配置一下哪个的lab_server的映射路径即可

5.服务器跑代码没有本机快的原因可能是用的显卡别人也在用，也可能是强化学习相较于深度学习更吃cpu，cpu大家都在用所以很慢

6.NCF中embedding用的nn.embedding但是初始化可以是随机初始化也可以是pre_train了MLP以及GMF后的embedding层参数，之后参数可以微调，效果更好，没有将梯度设为False

7.在训练模型时，for x,target in dataloader，dataloader知道返回的是什么是因为在自己继承dataset的类中改写的$\_\_getitem(self,index)\_\_$方法，返回的便是x与target

8.对于embedding table的理解，nn.embedding中维护的权重矩阵也叫embedding table，pre-trained之后保存起来的也叫embedding table

##### 5.17

1.《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》

生成器认为是RL中的agent，state是已经生成的tokens，action是当前生成的token。

2.想法：

1）model-based对用户建模+EA算法增大探索，状态embedding用GRU+attention，训练用SAC/TD3

2）利用GAIL，r用D得到，用PPO训练

3.《Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs》

推荐系统中用的知识图谱中的（实体，关系，实体）为一些属性关系还有数据集中的交互记录：

![image-20230517194727378](D:\TyporaPicture\5月日报\image-20230517194727378.png)

##### 5.18

1.讲了组会

[组会文件]: ../组会文件/23.5.18.md

2.py文件中注释前面写TODO是还没有完成的部分，可以通过左侧栏快速找到

##### 5.19

1.建立了一下个人网站waibibabou.github.io

2.python字符串前面加：

f：f-strings其中的{}包含的表达式会进行值替换

r：可防止字符串转义$s=r'abc\nabc' $ print s 为abc\nabc

u：表示unicode 字符串

l：表示宽字符，即每个字符占两个字节

3.除号'/'是正斜杠，python的路径表示中都用正斜杠就好了，就不需要考虑转义字符了

4.os模块

![image-20230519171155733](D:\TyporaPicture\5月日报\image-20230519171155733.png)

5.带有yield关键字的函数返回的是一个生成器，当调用next()是该函数从上一次next停止的地方（如果第一次next则从函数开头）开始执行，当遇到yield时候return生成的数，此步next()就结束

##### 5.21

1.DRR模型的代码Recommender_system_via_deep_RL里的embedding方式是首先根据，movies以及各自的genres训练得到，movie有某个genre则target为1，否则为0。之后得到movies的embedding后再根据用户与电影的喜好程度训练用户的embedding，喜欢某电影则target为1，否则为0。还有一种训练方式是直接用用户与电影的喜好训练，不用genres提前训movie的embedding。这种训练embedding的方式是不是泄露了test数据，DRR代码使用所有数据训的。但是后面好像没用训练得到的embedding

lc 1618 910  codefun 69  total 979

##### 5.22

1.pip install -r requirements.txt安装其中的依赖，依赖如果没有指定版本则会安装最新版

2.python的类名后面如果没有继承的父类则不需要加括号

3.如果需要对网络中各层参数初始化可以定义一个initialize()在$\_\_init\_\_$函数中调用，将初始化过程与网络层定义分开。

4.recsys-rl代码中，用户相同，在不同的状态下采取相同的推荐动作的r相同，这是否合理，不过想到组会讲的搜索embedding维度的论文中，也是在不同状态下采取相同的动作得到的r相同，其甚至是下一个状态只和上一时刻的动作有关，所以说对于强化学习定义的理解不要太死板，动作能够改变状态就行了。

##### 5.23

1.需要to(device)的：网络、需要输入到网络中的tensor，因为在tensor定义时默认是在cpu中的、需要与网络输出运算的tensor。注意如果网络在gpu中，则其输出也在gpu中。.to(device)==.cuda()

2.actor的动作空间也可以通过环境得到

3.输入到forward函数时候第一个维度不要求是batch_size，但是输入到网络层时候必须是。但最好输入到forward中的就是带有batch_size维度的数据，这样不管是只输入batch_size为1还是多少的数据，forward函数代码不需要改，通用性高。

在$\_\_init\_\_$定义网络层时不需要考虑第一个维度的batch_size，在forward中需要考虑

4.报错：ValueError: only one element tensors can be converted to Python scalars，在将一个list中包含多个值的tensor转为tensor时候报错如上，修改方式为将list中的tensor先变为ndarray形式，再将该list转为tensor

5.带有梯度的tensor即requires_grad=True的tensor不能直接.numpy()，先.detach()将梯度改为False再.cpu().numpy()

6.float64==double，网络模型的数据默认使用float32训练

##### 5.24

1.将有参数的网络层都定义在$\_\_init\_\_$中，如果定义在forward中每调用该函数都会重新定义网络，于是无法训练。不带参数的网络层可以定义在$\_\_init\_\_$中或者forward中

2.实验记录：DRR模型，也就是最基本的DDPG用movielens1M训练，开始时候的平均reward从0.5上升到0.67，之后突然下降到只有0.3左右

<img src="D:\TyporaPicture\5月日报\image-20230524175530259.png" alt="image-20230524175530259" style="zoom: 33%;" />

<img src="D:\TyporaPicture\5月日报\image-20230524175545069.png" alt="image-20230524175545069" style="zoom:33%;" />

而训练的最后阶段reward又突然升高：

<img src="D:\TyporaPicture\5月日报\image-20230524212514907.png" alt="image-20230524212514907" style="zoom:33%;" />

最终画图的时候报错Error: failed to send plot to http://127.0.0.1:43393

##### 5.27

1.多臂老虎机(MAB)没有状态的概念，只有一台赌博机，有多个arm。

上下文赌博机(Contextual Bandits)有状态的概念，但是该状态不会根据动作而改变，其会影响动作的选择，状态与动作共同影响奖励值。相当于有了多个赌博机，每个赌博机都有自己的奖励分布。

完整的强化学习(MDP)其状态会根据动作而改变，状态与动作共同影响奖励值。![2](D:\TyporaPicture\5月日报\2.png)

##### 5.28

lc   1625  913   codefun  69  total  982

##### 5.29

1.《Deep Reinforcement Learning Framework for Category-Based Item Recommendation》

针对action动作空间大的问题，不同于policy-based的RL方法，本文采用双层DQN解决动作空间大的问题。

状态s的设定：包括曾经推荐过的物品以及对应的反馈，这也是一种状态的设定方式，还有就是只考虑曾经推荐的喜欢的物品

其选取categories的方式通过category网络的q值以及下层item网络的q值融合后得到，这反而计算量变大了

训练item的Q网络用了两种方式，引入了一个score function：
$$
v(s,i;\theta^p)=x_i^T[\sum_{a{\in}PI}x_a-\beta\sum_{b{\in}NI}x_b]
$$
通过该分数得到在s下选择物品i的概率，后续用二分类的交叉熵loss训练

##### 5.30

1.强化学习推荐系统的四部分包括：

<img src="D:\TyporaPicture\5月日报\image-20230530162116640.png" alt="image-20230530162116640" style="zoom:50%;" />

2.《Improving ranking function and diversification in interactive recommendation systems based on deep reinforcement learning》

状态的嵌入表示中引入positional encoding 

从actor的输出得到最终的推荐list的过程，其没有与所有的item的embedding算相似度，而是用最近邻的算法ANNoy寻找item

##### 5.31

1.MovieLens数据集

1）100K

*100,000 ratings (1-5) from 943 users on 1682 movies

*每个用户最少都有20个电影评分

*包含用户的人口特征，age, gender, occupation, zip-code

u.data中是所有评分，1-5整数

u.item是电影信息

u.user是用户信息

.base以及.test文件是官方已经根据u.data划分好的训练集与测试集，带数字的是每个用户的数据按照8:2划分的，测试集都不相交，5折交叉验证。不带数字的是每个用户的数据保留10条作为测试集，测试集a与b不相交

2）1M

*1,000,209 ratings from 6,040 users on 3,952 movies

*每个用户最少都有20个电影评分

*包含用户的人口特征，age, gender, occupation, zip-code

ratings.dat中是所有评分，1-5整数

users.dat是用户信息，用户信息中年龄不是准确年龄而是将一段范围内年龄映射为同一值

movies.dat是电影信息

3）10M

*10,000,054 ratings from 71,567 users on 10,681 movies

*每个用户最少都有20个电影评分

*没有用户信息

ratings.dat中是所有评分，评分有.5小数

movies.dat是电影信息

tags.dat是用户关于某电影的标签，标签的含义、内容、目的由每个用户决定，不与ratings一一对应

4）20M

*20,000,263 ratings from 138,493 users on 27,278 movies

*每个用户最少都有20个电影评分

*没有用户信息

ratings.csv是所有评分，评分有.5小数

tags.csv是用户关于某电影的标签，标签的含义、内容、目的由每个用户决定，不与ratings一一对应

movies.csv是电影信息

links.csv用于得到电影的url

genome_tags.csv电影tag的标识符对应关系

genome_scores.csv表示电影ID,tagID,官方便签相关性
