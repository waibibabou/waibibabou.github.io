---
layout: post
title: 2023年前半年记录
subtitle: 加油
tags: [2023日报]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.jpg
share-img: /assets/img/path.jpg
---


##### **3.2**

上午开了组会，武师兄讲的图对比学习

下午将故障字典导入到了mysql中，开始时候报错gbk无法解析字符，原因是要导入的csv文件是utf8格式的，使用notepad++改成gb2312后成功导入

晚上看完了深度强化学习知识.pdf，多智能体强化学习那部分没看

##### **3.3**

上午复习了并查集

下午改之前的电信统计的系统，改成铁科研的数据，已经能够成功生成故障数据

![image-20230303170946418](\TyporaPicture\半年记录串联23_1\image-20230303170946418.png)

已经能够成功存储到hbase中

![image-20230303173133732](\TyporaPicture\半年记录串联23_1\image-20230303173133732.png)

晚上在改MR程序，好难改，快把两部分MR写完了，还没写插入Mysql的过程

##### **3.4**

下午在改MR程序，尝试了许多次在linux中运行MR程序，都是报错block missing，但是并没有datanode挂了

多次尝试后能够成功生成未排序的temp结果

![image-20230304155401734](\TyporaPicture\半年记录串联23_1\image-20230304155401734.png)

但是运行后面的MR时报错

![image-20230304155426679](\TyporaPicture\半年记录串联23_1\image-20230304155426679.png)

在Driver中设置了

```
job2.setMapOutputKeyClass(IntWritable.class);
job2.setMapOutputValueClass(Text.class);
```

后不再报错，得到排序后的结果

<img src="\TyporaPicture\半年记录串联23_1\image-20230304155816507.png" alt="image-20230304155816507" style="zoom:50%;" />

晚上在改插入mysql的部分

##### **3.5**

上午把插入mysql的程序改完了，但是还没有成功运行

下午在本地成功运行了插入mysql的过程，结果如下

![image-20230305233157766](\TyporaPicture\半年记录串联23_1\image-20230305233157766.png)

晚上写了项目我这部分的文档

1369 767

##### **3.6**

上午又重新搞了一遍pycharm连接服务器的流程，先创建映射，再配置解释器，解释器的映射路径也是到自己的路径下，不要映射到temp路径。

下午做了几道题

晚上看了看之前讲的论文，整理了一下问题

##### **3.7**

上午做了两道回溯

下午复习了word2vec，在看Surver of DRL in RS

晚上在看论文

##### **3.8**

下午看完了Pseudo Dyna-Q

晚上看完了end-to-End Deep Reinforcement Learning based Recommendation with Supervised Embedding

##### 3.9

上午开了组会，讲了Pseudo Dyna-Q以及end-to-end

[组会文件]: ../组会文件/23.3.9.md

##### 3.11

下午看了6P的springboot

##### 3.12

晚上看springboot到P22

1381 773

##### 3.14

上午看了看springboot的一些参数和注解

下午把修师姐的论文看完了

晚上在看survey of RS论文

##### 3.15

上午看了springboot的web实验

晚上做了力扣链表

##### 3.16

凌晨把springboot2看完了，用了mybatisplus连sql都不用写了

上午修师姐讲组会，图对比学习

晚上学习了欧拉筛

##### 3.17

上午整理了关于质数的笔记

下午做完了hands-on RL第二章

晚上做了几道力扣

##### 3.18

下午学完了hands-on RL第三四章

晚上打了双周赛

##### 3.19

下午学完了hands-on RL的第五六章

看完了双周赛与周赛讲解

晚上做力扣

1420 800

##### 3.20

上午复习了周赛

下午做完了hands-on RL第7章

晚上总结了数位dp模板

##### 3.21

下午做完了hands-on RL第8章

##### 3.22

下午做完了hands-on RL第9 10章

晚上参加了特斯拉笔试，全是力扣原题，做出了前三道，最后一题是后悔贪心

##### 3.23

上午看了TRPO，但没有实现，实现上都用改进版PPO

下午做了hands on RL第12章

晚上复习了力扣dp 

##### 3.24

上午复习了力扣两个选段问题

下午写完了英语作业

##### 3.25

下午做了几道力扣

有点发烧

##### 3.26

参加了周赛a了三题

下午看了深海和题解

1455 817

##### 3.28

发烧刚好下午做完了hands on RL第13章

晚上在做SAC

##### 3.29

上午在看sortedlist

下午做完了hands on RL第14章

晚上看了下lcp题目复习了数位dp

##### 3.30

上午组会关于去噪、gnn、对比学习

下午复习了数位dp

##### 3.31

做了做题

##### 4.1

下午做了hands on RL行为克隆

晚上做了GAIL

参加了双周赛

##### 4.2

中午见了高中同学

下午去国家速滑馆滑冰了

1485 836

##### 4.4

下午写完了学术英语写作大作业

晚上看完了武师兄论文

##### 4.5

上午发现服务器输入conda list显示conda命令找不到，在bashrc中配置了一下anaconda/bin路径后解决，后面在activate pytorch时候报错CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.输入source activate重新进入虚拟环境即可

下午看了Evolution-Guided Policy Gradient in Reinforcement Learning

查看进程ps aux 查看显卡占用gpustat

##### 4.6

上午找了老师讨论

GAIL源码 EA云计算讲

画图   按照两个大方向归类论文、想法  别人都做了什么我做了什么我还要做什么  找老师  分析可行性

腾讯那个计划都用什么数据集

康师姐论文，用进化算法先做offline，后面是online，用进化算法作为专家指导A-C网络

听云计算组会

##### 4.9

参加了周赛

周杰论文看完了2.2

1505 843

##### 4.10

下午看完了周杰师兄论文

##### 4.11

周赛第四题被re了，第一次被re

上午参加了云计算组会

##### 4.12

学习了最短路算法，包括典型Dijkstra以及堆优化

下午学完了hands on RL 第16章

晚上看完了hands on RL第17章

##### 4.13

上午参加了组会 协同过滤包括基于内存(邻域)以及基于模型的，重点是有邻居的概念。基于神经网络的有人把它归于基于模型的，也有人把它归于新的一类。基于内容的不属于协同过滤

晚上做完了hands on RL第18章

##### 4.14

1.奖励很稀疏是指在训练初期难以完成目标时只能得到-1的奖励，在完成任务时才能得到正数的奖励，从而使整个算法的训练速度缓慢

2.np.hstack（）将多个array水平堆叠，np.vstack（）将多个array竖直堆叠

3.神经网络由三种类型的层组成，输入层、隐藏层、输出层

输入层只保存输入数据，没有计算，所以不使用激活函数

输出层内使用激活函数的选择取决于我们要解决的问题类型

我们从不在输出层中使用tanh函数或者relu函数

必须在多分类任务中的输出层使用softmax函数，不在隐藏层中使用softmax函数

恒等激活函数仅用于回归问题的神经网络模型的输出层，不在隐藏层中使用该函数

4.下午完成了hands on RL第19章，HER适用于rewards很稀疏的情况，并且需要有一个目标

看完了hands on RL，多智能体的两章没看

##### 4.15

1.程序手动终止以后gpustat发现还在占用显存，但是显卡是0%，fuser -v /dev/nvidia*查看对应的pid，此时kill掉对应的PID后再查看发现不再占用显存。程序正常结束不会再占用显存

2.expected sequence of length 3 at dim 1 (got 2)

报这个错误是由于创建tensor时候，需要每一行的列数相同，如果第一行有3列，第二行只给了两列则会报错

gym版本：8p中为0.24.0，air为0.26.1，server为0.26.2，后续都统一成了0.26.2

在新版gym的API下，将原来env.step()返回的done，拆分成了两部分，terminated以及truncated，分别指成功或者失败而终止、到达指定了长度限制而终止两种情况，如果想把这两种情况合并，则加一行done=terminated or truncated即可。并且新API，env.reset()返回了state以及一个附加的dict组成的tuple。

3.设置了automatically upload好像有点问题，一直不自动上传，所以我选择了用ctrl+s上传，后来发现ctrl+s也不上传，不如直接手动上传

##### 4.16

1.使用堆优化的dijkstra算法，pop后不需要判断dis[i]是否等于d，好理解很多

2.矩阵每步向上下左右四个方向走，也能看作是一个图

3.下午做完了周赛以及双周赛的题目，总结了最短路问题

lc1540 864 codefun3  total 867

##### 4.17

1.acm中常说的卡常是指，程序的渐进复杂度满足要求，但是算法本身的时间常数因子较大，使得无法在规定的时限内运行结束。优化方式包括输入输出优化等等

2.使用SortedList最大的问题是，有的oj上没有安装这个包，此时只能考虑改成堆实现

3.acm代码模式下，想让程序主动结束使用sys.exit()，以免使用多个if else

4.学完了floyd算法，用于任意点到任意点的最短路，负权依然适用

##### 4.18

1.看完了transformer

transformer中抛弃了传统的CNN与RNN，提高并行能力，然后用cpu跑了一下transformer代码，太慢了

2.解释器有更改如果用的jupyter，需要重启kernel

torch从1.11.0改到1.8.0后报错，说是3070ti和pytorch版本不兼容，应该是版本太低了

报错No CUDA GPUs are available，与cuda什么的无关，可能是代码中设置了os.environ["CUDA_VISIBLE_DEVICES"]="2"，本机没有第三块显卡，改成0即可

torch与pytorch不能同时安装，这两个东西好像还不太一样，conda list之后发现这两个可以同时出现

安装torchtext需要与pytorch版本对应，并且在0.12.0后面有大更新

在卸载torch以及pytorch后执行conda install pytorch==1.10.0 torchvision==0.11.0 torchtext==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge再用gpu跑transformer成功，cuda占用99%，显存占用7.3G

##### 4.19

1.发现一些包在pycharm解释器中看到的与终端看到的版本不一样，pycharm中的版本是对的，最保准的方法是import后.version看一下

2.看了《Social Attentive Deep Q-network for Recommendation》论文，其用了DQN结构，只是在计算Q值时结合了社交网络信息。最后计算NDCG时候是每个时刻根据所有a的q值排序计算一个ndcg后所有时间下取平均得到，感觉所有的value-based的方法的NDCG都这样计算吗。在训练时居然没有用回放池以及目标网络

3.用本机与服务器的显卡跑了一下transformer，当服务器显卡只有我一个人用时发现每个epoch耗时两者几乎相同，别人也在用时，每个epoch耗时翻倍

4.想找论文的代码可以尝试在arXiv页面下的Code Data Media模块中找找

5.自回归（autoregressive）是把自己当前以及之前的所有输出作为下一次的输入，迭代产生一个输出序列

6.看论文时候记得找代码，找到后在文章名字后面(pycharm项目名字)

##### 4.20

1.离线强化学习是用固定的s,a,r对进行训练，不能与环境交互，推荐数据集相当于就是环境，那我之前将的都不能算离线强化学习，虽然说都没有放到真实网站上用

2.我之前理解的online以及offline是针对推荐系统来说的，offline是使用淘宝这种类似的离线数据集，online是真正放到网站上。而离线强化学习是针对强化学习算法说的，指的是能不能与环境交互。

3.感觉movielens那种矩阵形式的数据集也能用？和淘宝这种基于session的只是表现形式不一样

##### 4.21

1.学习了一下旅行商问题，一个状压dp问题

2.深度强化学习中，CPU用于收集经验数据，GPU用于训练神经网络

3.强化学习中的模型，可以是给定的，也可以是利用在交互过程中的采样对环境进行估计出来的

强化学习中模型的三种利用模式：Simulating the environment、Strengthening the policy、Assisting the learning algorithm

强化学习如何降低训练的采样复杂度，特别是对在线交互数据的需求：Model based RL-online learning with a learned model、imitation learning-online learning with an expert、offline RL-offline learning without extra online interaction

4.求最大公约数gcd(a,b)时间复杂度是O(logn)

5.算法中的离线的概念：不按他读入的顺序来执行操作，而是可以自由的对操作进行排序来处理

##### 4.22

1.tensor.squeeze()将张量shape中的1去除并返回，如果输入的shape为A,1,B,1,C,1,D则输出shape为A,B,C,D

tensor.unsqueeze()扩展维度，返回一个新的张量，对输入的既定位置插入维度1

2.技巧：tensor到第一个数有多少个[就有多少维

3.回溯类型：1排列型 使用一个visited数组  2子集型 选或者不选、选哪个两种写法  3组合型 可以看做长度固定的子集型回溯，加上剪枝即可

4.tensor.numel()用于迅速查看一个张量有多少元素

5.model.train()的作用是启用Batch Normalization以及Dropout，model.eval()作用是不启用Batch Normalization以及Dropout

6.力扣比赛提交时如果显示超出时间限制，但是下面是空白的，没有指定case或者指定hidden case，那说明不是某个case超时，而是总时间超时

##### 4.23

1.算法中离散化指的是排序去重后，将数组映射到他们的下标上，实现数据的压缩

2.两个电脑上的文件传输不需要用微信，因为需要反复登录，直接复制到onedrive文件夹中，可以实现两个电脑的同步

用onedrive实现了8p到air的组会+论文文件同步，8p中有修改时air中从onedrive复制下来就行了，不过发现onedrive文件夹同步得非常慢，手动重启onedrive也可以

3.看过的论文文件夹中日期带括号是讲过的，不带括号是没讲过的

lc1546 865 codefun22  total 887

##### 4.24

1.《RobustDQN》文章中说推荐的环境是动态的，而Atari游戏环境是静态的，该文章主要针对在线环境的动态变化，我有的都是历史数据集是不是不需要考虑环境的变化，环境在我这里类似游戏环境是静态的。其中使用了分层采样的方法训练网络，能够减小抽样的方差，根据用户的一些特征比如年龄等进行分层。这里的动态环境是指，比如在晚上用户对于结果的反馈的r可能远小于白天的r，但这并不是因为晚上推荐的结果不好，而是本身用户特性决定的。但是这是对于在线环境来说的，如果我用离线数据集训练的话应该是不需要考虑的，如果分层采样倒是第一次见。还有个创新点是在获取的r中减去了一个baseline的r，使得在动态环境中能更准确的估计奖励值。

实验是推荐淘宝的tip标签，而不是哪个物品，而且是在线做A/B测试。

2.报错...is not callable说明在...后面加了()当成一个函数用了

3.四舍五入可以将该数+0.5后int()

4.nn中的嵌入层nn.Embedding(num_embeddings,embedding_dim),num_embeddings表示词典的大小,embedding_dim表示嵌入向量的维度，生成一个权重矩阵（embedding table），embedding的过程本质就是lookup这个权重矩阵的过程，等价于一层没有bias项的全连接。一开始矩阵是随机的，在训练的时候会自动更新。除此以外，也可以导入已经训练好的embedding，设置weight.requires_grad=False即可。

nn.embedding参数中的padding_idx是填充的index，用于填充不等长的句子，对应的index那行的embedding都为0

5.《State representation modeling for deep reinforcement learning based recommendation》中的动作不是某个物品，需要与所有物品做点乘后选score最高的动作动作，之前有看到过这样的。这篇文章没什么创新，就是对比了四种不同的获取状态embedding的方式，使用AC框架。用户对于推荐动作如果是负反馈则t+1时候状态s不变。构建状态s方式：

<img src="\TyporaPicture\半年记录串联23_1\image-20230424224101233.png" alt="image-20230424224101233" style="zoom:50%;" />

推荐的物品如果用户从来没评分过则r为0或者最好是用PMF先填充一下，该文章用的数据集是ML、Yahoo!Music、Jester，在网络训练时候可以推荐的物品是整个动作空间，而在评测计算NDCG是只能推荐某个session中的物品进行重排，但数据集并不是基于session的，那看来也能这么说

训练时候的动作空间是整个集合是比较好的，由于这篇论文数据集是有分数，其利用了PMF填充了一下空白位置，没有认为空白的r就是0。而有的数据集是略过、点击、购买的序列，这时候如果想动作空间是整个数据集需要用模拟器机制，没有模拟器就只能在训练时候推荐session中的物品。在评测阶段大部分用NDCG，不管对于分数数据集还是session数据集都是进行物品的重排。

用户以及物品的embedding是根据PMF得到的

这篇论文感觉是最base的，如果写代码可以尝试先写这种

6.状态转移矩阵P只存在于model-based强化学习算法中。

7.element-wise product==element-wise multiplication==Hadamard product两个矩阵对应位置元素进行乘积

##### 4.25

1.云计算组会：珊瑚虫进化算法、多智能体强化学习、状态空间与动作空间是一样的，DQN是下一个状态选哪个的概率，最后目的是选哪个状态最好，可能是根据策略后进行策略评估，选V最大的状态。但是策略评估得有P，这里面没有P啊

2.对于acm模式那种输入n-1条边建立的树，不管是不是二叉树，都用g存储邻居即可

3.python别的进制转十进制int(x,base)  x为字符串或者数字 base表示进制数

##### 4.26

1.对于有两个变量的题目，通常可以枚举其中一个变量，把它视为常量，从而转化为只有一个变量的题目，两数之和就是枚举第二个数，去左边找第一个数，用哈希表优化找第一个数的过程

2.torch.nn.DataParallel(model, device_ids=[0,1])过程是在每次forward时候将gpu0的参数复制到其他gpu的模型中，把一个batch分成几块分别送入不同gpu中，把每个卡的输出gather到gpu0中，只在gpu0中计算loss，把loss scatter到其他gpu上，把每个gpu各自计算的梯度汇总到gpu0中进行参数更新

3.argparse是一个用于命令行参数解析的模块，使用add_argument添加参数，default=不指定参数时的默认值，type=命令行参数应该被转换成的类型，help=参数的帮助信息，-metavar=在usage说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称

4.好像现在python的每个文件夹中不用再有一个init文件了，也可以正常调用

5.ndarray数据类型强转用astype()，tensor强转用 .指定类型

6.python函数参数中Optional[x]==Union[x,None]

7.学习了KMP算法，先是根据pattern字符串构建next数组，再扫描原字符串进行匹配，两部分比较相似

8.子数组/子串问题主要有两种思路：1）双指针，使用条件需要满足两段性，即一侧满足条件，另一侧不满足
2）前缀和+哈希表，常用于统计子数组个数，不需要有两段性

9.dfs且输入是一个数组的两大类题型：
（1）全排列类型，需要用到所有的数字，并考虑数字之间的顺序：
**使用visited数组**，每一层递归函数中，遍历所有没有visited的数字。（见全排列1）
如果输入中有重复数字并且需要去重，则将输入的数组排序，每层函数中，判断前一个数字是否与当前数字相同，如果相同并且前一个数字没有被使用则跳过当前数字（思想就是说当前要选择的数字是没有被使用的第一个从而实现去重）。（见全排列2）
注意：如果限定选出的数组的长度，则同样不需要用到所有的数字（见活字印刷）


（2）得到所有子集类型，不需要用到所有数字，不考虑数字之间的顺序：
**不使用visited数组**，在递归函数的输入中加入begin参数，在for中从begin开始遍历，到最后一个数字，选择出一个满足条件的数字加入到current中，进入下一层。（见组合总数1、子集1）
如果输入中有重复数字并且需要去重，则将输入的数组排序，在for中判断当前要选择的数字是否与前一个相同，如果相同则跳过。（见组合总数2、子集2）
注意：如果在某一层中可以什么都不选择，在for循环后单独调用begin为末尾的dfs函数

还有一种方法就是直接从前往后遍历nums，为每个数字做出类似  选择/不选择  的决定，该方法可以使用题型（2）的方法替换，写在这里只是为了提醒（见火柴拼正方形）

10.广度优先搜索最关键的就是在节点入队后必须立即标记其已经入队（通过visited数组或者改变原来数组中的值），以免产生一个节点两次入队的情况

##### 4.27

1.collections.Deque的insert(index,item)时间复杂度O(n)，remove(item)时间复杂度O(n)，支持in运算符O(n)

2.模型一般是一个epoch训完后进行一次eval

3.使用.cpu()将显存的数据移到内存  .cuda()将内存数据移到显存

4.好像不能简单使用nn.DataParallel使强化学习用多张gpu，因为获取a时候的batch_size为1，是一个序列问题，A3C等框架应该可以

##### 4.30

lc1560  873 codefun 33  total 906

##### 5.2

1.看完了修师姐论文

2.渐进时间复杂度一致，但是如100n就不能通过，50n就可以

##### 5.3

1.消消乐问题考虑用栈解决

2.堆优化的dijkstra，当pop出来后判断是否为终点直接返回会快很多

##### 5.4

1.推荐组会：用自然语言处理解决多种推荐任务，推荐中很少用预训练，nlp中常用

2.NDCG：Normalized Discounted Cumulative Gain归一化折损累计增益，用于评估排序结果
$$
DCG=\sum_{i=1}^k\frac{rel(i)}{log_2(i+1)}
$$
IDCG是根据rel(i)降序排列，即最好序列
$$
NDCG=\frac{DCG}{IDCG}
$$

3.BPR loss要求模型将用户更喜欢的物品i排在更不喜欢的物品j之前

4.python中for while没有自身作用域

5.python中负数可以%mod，负数自动加mod

6.隐式反馈：用户的浏览、购买、点击 ；显式反馈：用户的评分

##### 5.5

1.model-based在RS用的少的原因

![image-20230505145647334](\TyporaPicture\半年记录串联23_1\image-20230505145647334.png)

2.model-based中的P不一定是显式的：

![image-20230505172241297](\TyporaPicture\半年记录串联23_1\image-20230505172241297.png)

3.子数组：一个连续的数组，子序列：不一定连续

4.子数组关于统计个数的，双指针也能统计，双指针或者前缀和+哈希表选哪个主要看是否具有二段性

##### 5.7

1.《Generative Adversarial User Model for Reinforcement Learning Based Recommendation System》

model-based RL采样效率更高，提出了一个model-based模型，学习了一个user behavior model以及reward function

用户行为模型作为生成器，奖励函数作为判别器用于区分用户真实行为以及生成器生成的行为。

lc  1576 882  codefun 49  total 931

##### 5.8

1.ndarray矩阵乘法np.matmul()或者@，矩阵对应位置相乘np.multiply()或者*，不要用np.dot()

2.显式反馈的reward就可以直接是评分，隐式的需要自己设定

3.一般可以通过pre-train一个MF模型得到用户以及物品的特征向量embedding，并且感觉embedding向量可以预先训练后固定，也可以跟着一起训练

4.《Interactive Recommendation with User-Specific Deep Reinforcement Learning》$s_{t+1}$的embedding每次都要用MF得到

对于没有评分的位置，文中用了两种方法处理，一是将推荐候选集限定在有分数的物品中，二是认为评分是0

5.<img src="\TyporaPicture\半年记录串联23_1\image-20230508162056109.png" alt="image-20230508162056109" style="zoom:67%;" />

6.隐式反馈的数据集：对于数据集中没有的item，可以将推荐候选集限定为session中出现的物品，也可以用list-wise论文中构建一个模拟器，也可以用历史数据构建一个model-based强化学习模型，就可以将候选集扩充到所有物品

显示反馈的数据集：对于数据集中没有的item，可以将推荐候选集限定为该用户出现过的评分item，也可以认为评分是0(但是效果可能不太好)，或者用MF填充一下评分矩阵

7.jupyter notebook debug不好用啊，断点打在不同cell识别不到，不如整体复制到py文件中debug

8.推荐中model-free的模型不知道P是因为不知道采取动作后用户的反馈，所以不知道下一时刻的状态，不知道反馈也就不知道采取这个推荐动作后的奖励值，即不知道奖励函数，并不是说不知道用户做出反馈后的下一个时刻的转移方式，转移方式都是用户喜欢哪个，将其加在s后面，不喜欢则不影响s

##### 5.9

1.云计算组会，分层强化学习，先输出目标，再针对该目标输出动作

amortized Q-learning（2020），动作空间太大情况下，找最大值困难，学习一个分布，选出动作的候选项集，计算这个候选集中的Q值找最大值

2.python海象运算符:=，py3.8版本引入，可以看成赋值运算符，整体的运算结果是海象运算法左边的值，引入该运算符是因为python中赋值运算符是没有运算结果的，海象运算符有结果。

3.pypy3比python3快非常多，如果有该选项选pypy3，代码不需要修改

4.tf2报错ValueError: Input tensors to a Functional must come from tf.keras.Input. Receive 0 (missing previous layer metadata). tensorflow版本问题导致，使用1.0版本方法，禁用2.0版本方法即可，不需要配置1.0的虚拟环境

import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()

5.看了一下LIRD-master代码，对应论文list-wise，用DDPG那篇，不理解state,action,next_state生成的方式，并且电影embedding生成方式很随意，网络定义与训练用tensorflow1.0版本写的，根本看不懂。作者zhao xiangyu，经常看到

6.pycharm程序点终止后依然没有结束，可以在左侧将其运行页面叉掉即可

7.np.save()可以保存所有类型数据，用np.load()加载。torch.save()保存tensor类型数据，用torch.load()加载

##### 5.10

1.求一个很长的数字除k的余数，可以通过(前n-1位数字的余数*10+int(最后一位))%k得到

2.英文缩写:  e.g.举个例子    i.e.即    etc.等等   et al.等等,列举人   w.r.t. with respect to关于,谈到    s.t. subject to受限制于 RQ research question要回答的问题

##### 5.11

1.推荐组会，解耦长期、短期兴趣

##### 5.12

1.背包问题

**背包分类：**

01背包：每个物品数量为1，选择装与不装，正序遍历物品，倒序遍历空间

完全背包：每个物品数量无穷，正序遍历物品，正序遍历空间

多重背包：每个物品数量有限，将每个物品展开，变成01背包，正序遍历物品，倒序遍历空间

混合背包：综合前三种背包，将数量有限的展开成01背包，数量无穷的按照完全背包做

分组背包：每组物品若干，同一组内物品最多选一个（注意是否必须要选一个代码也不同）或者是选多个算一种方案(比如选不同的三个算同一种方案)，最外层遍历所有组，倒序遍历空间，正序遍历物品，注意因为同一组内物品相互排斥，需要先遍历空间再遍历物品。

分组背包可以有优化的方式：

[2902. 和带限制的子多重集合的数目 - 力扣（LeetCode）](https://leetcode.cn/problems/count-of-sub-multisets-with-bounded-sum/solutions/2482876/duo-zhong-bei-bao-fang-an-shu-cong-po-su-f5ay/)

但是我觉得这是分组背包优化而不是灵神说的多重背包优化，因为是不区分选哪几个的

**任务分类：**

最值问题：$dp[i]=max/min(dp[i],dp[i-cost[j]]+value[j])$

存在问题：$dp[i]=dp[i]\ or\ dp[i-cost[j]]$

组合问题：$dp[i]+=dp[i-cost[j]]$

2.codefun上遇到runtime error不懂为什么，看不到样例

##### 5.13

1.康师姐论文《Imitation Learning Enabled Fast and Adaptive Task Scheduling in Cloud》将模仿学习应用到离线以及在线schedule过程，其与cloudsim交互认为是在线过程。离线训练时候的专家轨迹由启发式算法得到（这部分没懂）

2.python中pow(,,mod)实现快速幂，虽然python中不需要考虑int的溢出问题，但是在运算过程中先mod而不是到最后mod会大大提高运算速度

3.位运算的左移<<  右移>> 比乘法除法快非常多

4.有序是一个非常好的性质，如果排序不影响结果则可以优先考虑排序

##### 5.14

1.A\^B=B\^A  (A\^B)\^C=A\^(B\^C)

2.并查集对所有的边执行完union后parent数组不是直接都指向root，需要调用find函数才能知道root节点

3.TD3相对于DDPG的三点优化：1）critic部分：只有在计算critic的更新目标时才需要用到target network，其中包含一个policy network以及两个Q network，$Q1(A')$和$Q2(A')$取min代替DDPG中的Q计算target，$target=r+\gamma*min(Q1,Q2)$

target是两个Q网络的target，虽然更新目标一样，两个网络都会逐渐与实际Q值相同，但是参数初始值不同，导致计算得到的Q值不同，还是有一定空间选择较小的值去估算Q值，避免Q值被高估。

2）actor的延迟更新，让critic更新多次后再对actor更新。让critic更加确定后，actor再行动

3）在计算target时，输入s'到actor输出a后，给a增加噪声，相当于用一小块范围内去估算target，使得估计出值更准确。在DDPG中噪声只加在action的获取中，在计算target时候不加噪声。

lc  1599  899  codefun 68  total 967

##### 5.15

1.《Continuous Input Embedding Size Search For Recommender Systems》之前的寻找最优嵌入维度的方法都是在一个小的离散候选集中选一个，而这个提供一个细粒度的连续的空间进行选择，当然这里的连续指的是连续的整数。

embedding维度高，准确率更高，消耗内存更高，利用RL寻找两者的平衡。

对于每个user/item得到最优的嵌入维度后，embedding table其中大多数为0，存储稀疏矩阵，对于0的存储几乎不消耗空间，达到减小内存使用的目的。可你用RL找最优的嵌入维度过程中不也是需要保存矩阵中所有值的吗，内存没有减少使用，是不是有点多此一举。想了想还是有用的，这个算法中虽然内存没有减少，但是得到的对于每个user以及item的embedding来说，内存减小了，并且是在没有过多牺牲准确性的情况下。
$$
eval(e_u|\cdot)=\frac{\sum_{k\in{K}}Recall@k_u+NDCG@k_u}{2|K|},u\in{U}
$$
对于item的eval，由与其交互过的user的eval取平均得到。

对于reward的设计，需要体现出推荐质量以及空间复杂度：
$$
r_n=q_n-\lambda(\frac{d_n}{d_{max}})^2
$$
可以对于a进行随机游走的原因，在更新网络参数时，s,a,r,s'，对于actor网络需要将s输入actor得到a在用critic得到q值，所以在这个四元组中，与环境交互时执行的a不需要能对actor的输出值可导。

可以考虑利用随机游走增大动作的探索程度。

最后在actor执行过的动作中，挑选出稀疏度大于c的，效果最好的embedding维度用于推荐模型，得到recall以及NDCG。

质疑：这真的是一个序列问题吗，在不同s下执行相同的a，s'不是一样的吗，老师讲说这是之前那种的特殊情况相当于把前面的全挤出去了，相当于维护了一个长度为1的list作为状态。

##### 5.16

1.python中*与**总结：

*乘法、重复次数，**次方

1）收集列表中多余的值

无需确保值与变量个数相同:$a,b,*c=[1,2,3,4]$，c是[3,4]注意这里收集到的是list

而涉及到函数时，*、**作用在于收集参数或者分配参数

2）在定义函数时，*收集参数到tuple，**收集关键字参数到dict

$def\ myprint(*params):$将收集到的参数放在一个tuple中

$def\ myprint2(**params):$得到一个dict$\{'z':3,'x':1,'y':2\}$用于收集关键字参数

3）调用函数时，*分配tuple中的参数，**分配dict中的参数

$def\ myprint(x,y):$

params=(1,2)可以通过$myprint(*params)$调用

params={'x':1,'y':2}可以通过$myprint(**params)$调用，与形参名称匹配

4）定义和调用函数都用*没有实际意义，不如不用

2.python下划线总结

1）单前置下划线的命名：

作为类名或函数名时：会阻止其他文件通过from module import *导入该名字，即改名字不会与星号匹配，可以通过import该名字导入

作为类的属性名或者方法名时：不希望直接访问该名称，只是一种命名约定，但是python解释器不会对这种属性名做特殊处理

2）前后均带有双下划线__的命名：

用于特殊方法的命名，用来实现对象的一些行为或功能：

${\_\_new()\_\_}$方法用于创建实例

$\_\_init\_\_()$方法用来初始化对象

x+y操作被映射为方法$x.\_\_add\_\_(y)$，序列或者字典的索引操作x[k]映射为$x.\_\_getitem\_\_(k)$

$\_\_len\_\_()$、$\_\_str\_\_()$分别被len()、str()调用

3）仅开头带双下划线__的命名：

用于对象的封装，以此命名的属性或者方法为类的私有属性或私有方法，无法从外界直接访问，通过自动变形实现，$\_\_name$自动变为‘ $\_类名\_\_name$ ’的新名称。这种机制可以阻止继承类重新定义或者更改方法的实现，调用时还是运行父类中的私有方法，而不是子类重写的方法。

3.跑NCF代码batch_size为256时报错RuntimeError: CUDA out of memory. Tried to allocate 5.91 GiB，将batch_size改为16后不再报错

4.配置服务器流程：

创建一个新python项目，创建temp服务器，取消勾选visible only for this project，配置映射，上传到pythonProject，删除temp服务器。这一步是为了在选择解释器映射路径时，不要用tmp下的路径。

添加python解释器，映射路径选择刚才上传到的pythonProject，不要选择tmp的路径。分别添加pytorch以及transformer的解释器，看到ide下面一条的左侧有两个lab_server，分别对应两个解释器，有几个解释器，就有几个lab_server，后面执行时候虽然只需要改变解释器但是也不要将lab_server删除，否则会报错：

Error running 'main': Cannot find remote credentials for target config com.jetbrains.plugins.remotesdk.target.webDeployment.WebDeploymentTargetEnvironmentConfiguration@37f07bdf并且与该lab_server对应的解释器会显示nothing to show，并且无法测试是否能成功连接

将lab_server、lab_server(2)改名为lab_server_pytorch、lab_server_transformer，顺序与解释器顺序一致，两者root路径自动检测，改下映射路径，注意都要取消勾选visible only for this project。将解释器改名为lab_server_pytorch、lab_server_transformer

配置一个新项目时候，如果只选择了pytorch的lab_server，那么用transformer解释器时会报错：Failed to prepare environment，所以需要用哪个解释器就配置一下哪个的lab_server的映射路径即可

5.服务器跑代码没有本机快的原因可能是用的显卡别人也在用，也可能是强化学习相较于深度学习更吃cpu，cpu大家都在用所以很慢

6.NCF中embedding用的nn.embedding但是初始化可以是随机初始化也可以是pre_train了MLP以及GMF后的embedding层参数，之后参数可以微调，效果更好，没有将梯度设为False

7.在训练模型时，for x,target in dataloader，dataloader知道返回的是什么是因为在自己继承dataset的类中改写的$\_\_getitem(self,index)\_\_$方法，返回的便是x与target

8.对于embedding table的理解，nn.embedding中维护的权重矩阵也叫embedding table，pre-trained之后保存起来的也叫embedding table

##### 5.17

1.《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》

生成器认为是RL中的agent，state是已经生成的tokens，action是当前生成的token。

2.想法：

1）model-based对用户建模+EA算法增大探索，状态embedding用GRU+attention，训练用SAC/TD3

2）利用GAIL，r用D得到，用PPO训练

3.《Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs》

推荐系统中用的知识图谱中的（实体，关系，实体）为一些属性关系还有数据集中的交互记录：

![image-20230517194727378](\TyporaPicture\半年记录串联23_1\image-20230517194727378.png)

##### 5.18

1.讲了组会

[组会文件]: ../组会文件/23.5.18.md

2.py文件中注释前面写TODO是还没有完成的部分，可以通过左侧栏快速找到

##### 5.19

1.建立了一下个人网站waibibabou.github.io

2.python字符串前面加：

f：f-strings其中的{}包含的表达式会进行值替换

r：可防止字符串转义$s=r'abc\nabc' $ print s 为abc\nabc

u：表示unicode 字符串

l：表示宽字符，即每个字符占两个字节

3.除号'/'是正斜杠，python的路径表示中都用正斜杠就好了，就不需要考虑转义字符了

4.os模块

![image-20230519171155733](\TyporaPicture\半年记录串联23_1\image-20230519171155733.png)

5.带有yield关键字的函数返回的是一个生成器，当调用next()是该函数从上一次next停止的地方（如果第一次next则从函数开头）开始执行，当遇到yield时候return生成的数，此步next()就结束

##### 5.21

1.DRR模型的代码Recommender_system_via_deep_RL里的embedding方式是首先根据，movies以及各自的genres训练得到，movie有某个genre则target为1，否则为0。之后得到movies的embedding后再根据用户与电影的喜好程度训练用户的embedding，喜欢某电影则target为1，否则为0。还有一种训练方式是直接用用户与电影的喜好训练，不用genres提前训movie的embedding。这种训练embedding的方式是不是泄露了test数据，DRR代码使用所有数据训的。但是后面好像没用训练得到的embedding

lc 1618 910  codefun 69  total 979

##### 5.22

1.pip install -r requirements.txt安装其中的依赖，依赖如果没有指定版本则会安装最新版

2.python的类名后面如果没有继承的父类则不需要加括号

3.如果需要对网络中各层参数初始化可以定义一个initialize()在$\_\_init\_\_$函数中调用，将初始化过程与网络层定义分开。

4.recsys-rl代码中，用户相同，在不同的状态下采取相同的推荐动作的r相同，这是否合理，不过想到组会讲的搜索embedding维度的论文中，也是在不同状态下采取相同的动作得到的r相同，其甚至是下一个状态只和上一时刻的动作有关，所以说对于强化学习定义的理解不要太死板，动作能够改变状态就行了。

##### 5.23

1.需要to(device)的：网络、需要输入到网络中的tensor，因为在tensor定义时默认是在cpu中的、需要与网络输出运算的tensor。注意如果网络在gpu中，则其输出也在gpu中。.to(device)==.cuda()

2.actor的动作空间也可以通过环境得到

3.输入到forward函数时候第一个维度不要求是batch_size，但是输入到网络层时候必须是。但最好输入到forward中的就是带有batch_size维度的数据，这样不管是只输入batch_size为1还是多少的数据，forward函数代码不需要改，通用性高。

在$\_\_init\_\_$定义网络层时不需要考虑第一个维度的batch_size，在forward中需要考虑

4.报错：ValueError: only one element tensors can be converted to Python scalars，在将一个list中包含多个tensor转为tensor时候报错如上，修改方式为将list中的tensor先变为ndarray形式，再将该list转为tensor。将一个list中包含多个tensor转为ndarray时候报错ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.，同样需要先将其中的tensor转为ndarray

5.带有梯度的tensor即requires_grad=True的tensor不能直接.numpy()，先.detach()将梯度改为False再.cpu().numpy()

6.float64==double，网络模型的数据默认使用float32训练

##### 5.24

1.将有参数的网络层都定义在$\_\_init\_\_$中，如果定义在forward中每调用该函数都会重新定义网络，于是无法训练。不带参数的网络层可以定义在$\_\_init\_\_$中或者forward中

2.实验记录：DRR模型，也就是最基本的DDPG用movielens1M训练，开始时候的平均reward从0.5上升到0.67，之后突然下降到只有0.3左右

<img src="\TyporaPicture\半年记录串联23_1\image-20230524175530259.png" alt="image-20230524175530259" style="zoom: 33%;" />

<img src="\TyporaPicture\半年记录串联23_1\image-20230524175545069.png" alt="image-20230524175545069" style="zoom:33%;" />

而训练的最后阶段reward又突然升高：

<img src="\TyporaPicture\半年记录串联23_1\image-20230524212514907.png" alt="image-20230524212514907" style="zoom:33%;" />

最终画图的时候报错Error: failed to send plot to http://127.0.0.1:43393

##### 5.27

1.多臂老虎机(MAB)没有状态的概念，只有一台赌博机，有多个arm。

上下文赌博机(Contextual Bandits)有状态的概念，但是该状态不会根据动作而改变，其会影响动作的选择，状态与动作共同影响奖励值。相当于有了多个赌博机，每个赌博机都有自己的奖励分布。

完整的强化学习(MDP)其状态会根据动作而改变，状态与动作共同影响奖励值。![2](\TyporaPicture\半年记录串联23_1\2.png)

##### 5.28

lc   1625  913   codefun  69  total  982

##### 5.29

1.《Deep Reinforcement Learning Framework for Category-Based Item Recommendation》

针对action动作空间大的问题，不同于policy-based的RL方法，本文采用双层DQN解决动作空间大的问题。

状态s的设定：包括曾经推荐过的物品以及对应的反馈，这也是一种状态的设定方式，还有就是只考虑曾经推荐的喜欢的物品

其选取categories的方式通过category网络的q值以及下层item网络的q值融合后得到，这反而计算量变大了

训练item的Q网络用了两种方式，引入了一个score function：
$$
v(s,i;\theta^p)=x_i^T[\sum_{a{\in}PI}x_a-\beta\sum_{b{\in}NI}x_b]
$$
通过该分数得到在s下选择物品i的概率，后续用二分类的交叉熵loss训练

##### 5.30

1.强化学习推荐系统的四部分包括：

<img src="\TyporaPicture\半年记录串联23_1\image-20230530162116640.png" alt="image-20230530162116640" style="zoom:50%;" />

2.《Improving ranking function and diversification in interactive recommendation systems based on deep reinforcement learning》

状态的嵌入表示中引入positional encoding 

从actor的输出得到最终的推荐list的过程，其没有与所有的item的embedding算相似度，而是用最近邻的算法ANNoy寻找item

##### 5.31

1.MovieLens数据集

1）100K

*100,000 ratings (1-5) from 943 users on 1682 movies

*每个用户最少都有20个电影评分

*包含用户的人口特征，age, gender, occupation, zip-code

u.data中是所有评分，1-5整数

u.item是电影信息

u.user是用户信息

.base以及.test文件是官方已经根据u.data划分好的训练集与测试集，带数字的是每个用户的数据按照8:2划分的，测试集都不相交，5折交叉验证。不带数字的是每个用户的数据保留10条作为测试集，测试集a与b不相交

2）1M

*1,000,209 ratings from 6,040 users on 3,952 movies

*每个用户最少都有20个电影评分

*包含用户的人口特征，age, gender, occupation, zip-code

ratings.dat中是所有评分，1-5整数

users.dat是用户信息，用户信息中年龄不是准确年龄而是将一段范围内年龄映射为同一值

movies.dat是电影信息

3）10M

*10,000,054 ratings from 71,567 users on 10,681 movies

*每个用户最少都有20个电影评分

*没有用户信息

ratings.dat中是所有评分，评分有.5小数

movies.dat是电影信息

tags.dat是用户关于某电影的标签，标签的含义、内容、目的由每个用户决定，不与ratings一一对应

4）20M

*20,000,263 ratings from 138,493 users on 27,278 movies

*每个用户最少都有20个电影评分

*没有用户信息

ratings.csv是所有评分，评分有.5小数

tags.csv是用户关于某电影的标签，标签的含义、内容、目的由每个用户决定，不与ratings一一对应

movies.csv是电影信息

links.csv用于得到电影的url

genome_tags.csv电影tag的标识符对应关系

genome_scores.csv表示电影ID,tagID,官方便签相关性

##### 6.1

1.以往的用强化学习做推荐的算法中，在不同的状态下推荐相同的物品的r相同，但这是不太合理的，应该针对不同的状态得到的r也不同，所以在ddpg框架下引入一个额外的判别器discriminator，得到额外的奖励r。或者用model-based方式

输入到判别器中的是推荐序列，假样本序列是actor近期输出的序列，真样本是从数据集抽取的，需要预填充矩阵，但如何抽取还没有想好。

##### 6.4

lc 1631 917  codefun 68  total 985

##### 6.5

1.虚拟机集群各个组件启动顺序：hadoop zookeeper kafka hbase关闭时倒序关闭

2.python中报错inconsistent use of tabs and spaces in indentation，原因是混用了4个空格和tab

##### 6.7

1.背包问题中的存在问题可以用set代替数组，就不需要所有位置都更新了

##### 6.8

1.取模运算%mod注意结果范围是0到mod-1，如果从1开始则减1映射到从0开始

2.对于图与树如果边没有属性则用邻接表存储即可，如果边有属性则用邻接矩阵或者邻接表存储

##### 6.11

1.横轴上计算两两点的距离之和，利用前缀和计算

<img src="\TyporaPicture\半年记录串联23_1\image-20230611113622810.png" alt="image-20230611113622810" style="zoom:50%;" />

lc  1655 931 codefun 70  total 1001

##### 6.13

1.mod的运算性质

模(mod)运算规则与基本四则运算类似，但是除法例外：

$(a+b)\%p=(a\%p+b\%p)\%p$

$(a-b)\%p=(a\%-b\%p)\%p$

$(a*b)\%p=(a\%p*b\%p)\%p$

$a^b\%p=((a\%p)^b)\%p$

##### 6.14

1.bisect.insort_right()函数时间复杂度为O(n)因为其通过O(logn)找到插入位置后通过O(n)的时间进行插入

2.policy-based算法中，在分布中采样与重采样（重参数化技巧）都能够得到取某值的概率值，不同点是采样对参数不可导，重采样可导

3.ReinForce以及Actor-Critic可以用于连续动作空间，策略网络的输出为高斯分布的均值与方差，通过log_prob()函数得到动作的概率值的log值，从而可以利用这两种算法训练，但是在Pendulum-v1环境中的结果非常差，连续动作空间建议用DDPG与SAC，存储四元组时可以将log值也存储下来，在训练时就不用在过一遍policy网络了

##### 6.16

1.python运算符优先级：

1)算术运算符，先幂运算、乘除、加减  2)位运算符  3)比较运算符  4)布尔运算符 先and后or  5)赋值运算符

一定注意位运算符优先级低于算术运算符

##### 6.18

1.力扣上最多能跑$10^8$数量级

lc 1673 940 codefun 70 total 1010

##### 6.20

1.《Generative Adversarial User Model for Reinforcement Learning Based Recommendation System》其中奖励函数会根据s与a共同决定，而不是只有a，更加合理，否则在不同状态下采取相同动作会得到相同的奖励

2.在agent执行它的动作之前，他能否对下一步的状态和奖励做出预测，如果可以，则是model-based方法，如果不能，为model-free方法

3.重构别人的代码时，要有明确的Env与Agent

##### 6.22

1.写二分查找代码时候循环条件是left<=right，最后跳出循环时一定是left=right+1，两者一定是横跨在问题的分界上，最后返回left还是right看题目要求

##### 6.23

1.神经网络是通过调用$\_\_call\_\_$函数从而调用的forward函数

2.<代码块>将函数参数自动赋值为实例属性的代码：

[参数自动赋值]: ../代码块/1.md

3.np.array与np.asarray都可以将数组转化为ndarray对象。区别：当参数为一般数组时，两个函数结果相同；当参数本身就是ndarray类型时，array会新建一个ndarray对象，但是asarray不会新建，而是与参数共享同一个内存

4.torch中乘法总结：

torch.dot用于计算两个一维张量的点乘，得到一个标量

torch.inner函数可执行一维张量的内积（此时inner与dot相同），当有高维度张量时，它会把一维内积的结果沿着张量最后的维度方向累加起来。这两个都别用

torch.mul是两个张量对应位置相乘，两个张量形状必须一致

torch.mm与torch.matmul都用于矩阵乘法，但torch.mm只支持二维张量的乘法，不支持广播，torch.matmul支持高维张量的乘法并支持广播

torch.bmm用于批量矩阵乘法，计算两个三维张量的乘积，shape分别是(batch_size,n,m)与(batch_size,m,p)

##### 6.24

1.datetime模块基于time模块进行了封装，有以下几类：timedelta用于计算时间跨度，time处理时分秒，date处理年月日，datetime都能处理

2.torch.cat与torch.stack都是将多个tensor在指定的dim维度上进行拼接，数据形状要求相同。但是stack操作会增加一个维度，cat操作后维度不会变。

$torch.cat()=torch.concat()$

vstack与hstack也不会增加维度，用cat操作指定dim即可替代，后续不要用vstack以及hstack

$torch.cat(dim=0)=torch.vstack()$

$torch.cat(dim=1)=torch.hstack()$

3.torch.view与torch.reshape都用来重塑tensor的shape，view只适合操作满足连续性条件的tensor，而reshape还可以操作不满足连续性条件的tensor。所以后面都用torch.reshape就可以了

##### 6.25

1.记忆化搜索中参数的设定可以是根据已经走过的位置得来的，或者是对于未走过的位置要求的

2.torch.unsqueeze对输入的tensor的指定位置插入维度1。torch.squeeze如果不指定dim则去除tensor.shape中的所有1并返回，如果指定dim则只去除该维度的1，如果该维度不是1则不变

3.torch.tensor与torch.Tensor都用于生成新的张量，Tensor是一个类，而tensor只是一个函数。torch.tensor生成的张量数据类型会从输入数据中进行推断，Tensor使用全局默认类型torch.float32。当输入了几个单独数字时，Tensor创建对应维度数的张量，而tensor会报错

![image-20230625202903127](\TyporaPicture\半年记录串联23_1\image-20230625202903127.png)

4.tensor.uniform(low,high)函数在low到high的均匀分布中抽取tensor.shape的数进行替换

5.定义nn.GRU时候的input_size表示seq中每个位置特征维度，hidden_size表示每层gru的神经元数量，num_layers表示gru层数，batch_first表示第一个维度是batchsize。gru的两个输出 1）output：shape为（batchsize，seq_len，h_out）表示每个样本的每个时间片时候的最后一层gru的隐藏状态。2）h_n：shape为（num_layers，N，h_out）表示每个样本的最后一个时间片的所有神经元的隐藏状态。

注意：隐藏状态==输出值

lc 947 2441 codefun 72 total 1019

##### 6.26

1.使用utf-8读取movies.dat文件时报错UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte。这是因为文件编码不是utf-8，使用Notepad++将该文件编码方式转为utf-8再读取即可

2.跑了一下DRGR代码，user的NDCG@5从0.03提高到0.1后报错

![image-20230626154722722](\TyporaPicture\半年记录串联23_1\image-20230626154722722.png)

是因为p其中有0了，不能有为0的概率

3.recsys-rl代码中是将s与actor的输出送入critic计算q，DRGR文章说将分值最高的item的embedding送入critic，哪种是对的？将item的embedding送入critic那q对于actor还有梯度吗

##### 6.27

1.torch.flatten()如果没有传start_dim与end_dim参数则将输入的tensor展平成一维，有这两个参数则是将start_dim到end_dim之间的所有维度值乘起来，其他的维度保持不变。例如x是一个size为[4,5,6]的tensor, flatten(x, 0, 1)的结果是一个size为[20,6]的tensor

2.weight_decay权重衰减系数，即L2正则化项前面的$\lambda$系数，决定了在更新参数时权重的衰减比例

##### 6.29

1.对于dataframe的合并操作，主要有四种方法，concat()可沿任意轴连接 Pandas 对象，并且可在串联轴上添加一层分层索引；merge() 使用数据库样式的连接合并，连接是基于列或索引；append()与join()是前面两者的简易版，不要使用

2.<实验记录>DRGR代码user的NDCG@5从0.03提高到0.09后下降到0.05

[DRGR实验]: ../实验记录/1.md

原因可能是：train时候的item候选集没有设定、critic的输入不对，改成actor原本的输出试试，train时候actor输出没有加噪声

3.github解决push不上去问题：

先是报错fatal: unable to access 'https://github.com/waibibabou/DRGR.git/': Failed to connect to 127.0.0.1 port 1080 after 2029 ms: Connection refused

这是因为设置了代理的问题，修改C:\Users\27399\\.gitconfig，删除其中的proxy即代理即可

后续报错OpenSSL SSL_rea Connection was reset, errno 10054或者Failed to connect to github.com port 443 after 21148 ms: Timed out尝试换节点即可，本次最后用香港节点成功push

4.github导入别的网站项目需要其状态为public

##### 6.30

1.debug如果想进入到forward函数则在其中打个断点否则无法进入forward函数

2.tensor的复制操作总结：

tensor.clone()函数返回一个完全相同的tensor，开辟新的内存，但是仍然留在计算图中，即有梯度信息

tensor.detach()函数返回一个与旧的tensor共享内存的新的tensor，新tensor脱离计算图

![image-20230630202544171](\TyporaPicture\半年记录串联23_1\image-20230630202544171.png)

copy_()函数完成与clone()类似的功能，但是调用copy\_()的对象是目标tensor，参数是复制操作from的tensor，最后返回目标tensor；而clone()调用对象为源tensor，返回一个新tensor:

![image-20230630202852564](\TyporaPicture\半年记录串联23_1\image-20230630202852564.png)

3.程序在运行时修改代码不会影响运行结果，并且一个py文件可以多个进程共同执行

4.<实验记录>仅仅将DRGR中train时加了ou噪声：

[DRGR]: ../实验记录/2.md

全程没有train上去



