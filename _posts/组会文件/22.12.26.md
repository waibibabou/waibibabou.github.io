# 使用DDPG框架

## 1 Deep Reinforcement Learning for List-wise Recommendations KDD 2019

该论文为DDPG算法在推荐领域的运用，建立了一个用户-智能体交互模拟过程，更好地进行离线训练。提出了LIRD模型，能够运用到物品数量众多即动作空间大的场景中，并且能够有效减少多余的计算开销。

## 1.1 MDP建模

状态s，定义为用户最近的N条浏览记录，根据时间顺序排序：

![img](D:\TyporaPicture\22.12.26\1654057199721-610965cb-bc8b-461e-b591-0ffd4f751592.png)

动作a，定义为一个长度为K的推荐列表：

![img](D:\TyporaPicture\22.12.26\1654057328501-0115d206-88b3-4240-b4fb-2568038cf6a9.png)

奖励r，根据用户对于推荐结果的反馈动作，包括略过、点击、购买，设定不同的奖励值：

![img](D:\TyporaPicture\22.12.26\1654057429036-d28be00b-bd0d-4143-b28f-54c25b9e8472.png)

状态转移概率P，如果用户略过推荐列表中所有物品，则状态不变，如果有点击或者购买动作则状态会更新：

![img](D:\TyporaPicture\22.12.26\1654057763789-2f6b1acb-46d1-4b33-be0c-2dfabc889672.png)

![img](D:\TyporaPicture\22.12.26\1654057773543-20ea92e9-c3a3-4913-94a5-365489a94242.png)

## 1.2 交互环境模拟

有着相似兴趣的用户在同样的物品采取的动作也是相似的，所以可以通过推荐结果去模拟结果中寻找相似的状态以及动作，便可根据模拟结果产生奖励值。

<img src="D:\TyporaPicture\22.12.26\1653366991483-e52183e3-90c5-4e39-bd2f-d11750b4b3cb.png" alt="img" style="zoom:80%;" />

有了模拟结果M后，在后续训练过程中，对于当前的状态和动作元组 pt (st , at )  ，可以通过计算其与M中元组的相似度：

<img src="D:\TyporaPicture\22.12.26\1653367756737-e6f4be56-6748-4dd8-b806-09d76db52d88.png" alt="img" style="zoom:80%;" />

于是我们便可以得到当前状态和动作所得到的奖励值r的概率如下：

![img](D:\TyporaPicture\22.12.26\1653367855466-c816b1a8-b717-48dd-9adf-fe2f2ffd09ba.png)

解决了在数据集中，交互记录占比总物品数过少导致的推荐结果的奖励值不好计算的问题。

## 1.3 网络结构

<img src="D:\TyporaPicture\22.12.26\1653368125649-50b84416-606d-4828-8468-dfbc53a9d3cf.png" alt="img" style="zoom:80%;" />

**Actor网络：**

其中，左侧的Actor网络的输入为用户近期有过交互记录的物品，输出为长度为K的权重向量，K为推荐列表长度。推荐列表的生成过程如下：

![img](D:\TyporaPicture\22.12.26\1653368338126-d9343426-8d81-4c5a-b439-d127d04a3725.png)

在Actor网络输出得到K个权重向量w后，将向量与物品空间中所有可选物品的embedding相乘，得到评分，选取每个权重向量计算得到的最高评分的物品作为推荐结果。得到推荐结果后根据状态、动作，使用模拟结果计算奖励值r。

**Critic网络：**

该网络使用当前状态s以及Actor网络的推荐结果计算Q预测值。

总体训练过程如下：

<img src="D:\TyporaPicture\22.12.26\1653368819151-74813186-1564-44cc-bc4c-d3d853b6c3f5.png" alt="img" style="zoom: 67%;" />

<img src="D:\TyporaPicture\22.12.26\1653368835556-7d4e39b3-1d1e-4e30-9c5b-d91148430c0a.png" alt="img" style="zoom:67%;" />

## 1.4 实验结果

将用户无操作、点击、购买的奖励值设定为0、1、5

数据集：JD在2017年7月的用户数据，挑选出100000个sessions，前70%为训练集，后30%为测试集。

short-term表示session的长度小于50条，long-term表示session的长度大于50条：

![img](D:\TyporaPicture\22.12.26\1653369030084-8e4ec0d1-9558-40ce-892a-1f4414203586.png)



## 2 Deep Reinforcement Learning for Page-wise Recommendations   RecSys   2019

## 2.1 背景/出发点

在商品推荐的app中，例如京东、淘宝，商品都是一个个矩形密铺在页面上，而不像传统的信息检索问题中搜索结果是一条条从上到下顺序排列（例如百度）。现有的对物品的ranking算法来源于信息检索领域，只适用于顺序排列的页面，对于电商网站这样的密铺页面不是很好的建模方式。

## 2.2 本文主要工作

1）提出一个方法，其能够推荐生成一组多样性较高的物品并同时将这些物品展示为一个二维的推荐页面

2）提出一个page-wise的推荐框架DeepPage，其能够根据用户的反馈数据最优化一整个页面的推荐结果

3）在真实数据集中验证了模型的有效性

## 2.3 MDP建模

状态S：设定为某用户的当前偏好，通过浏览历史记录计算得到

动作A：设定为推荐的一个页面的物品信息

![image-20221226114220362](D:\TyporaPicture\22.12.26\image-20221226114220362.png)

奖励值R：在agent做出推荐动作后，用户对一个页面的推荐结果做出反馈，可以选择略过、点击、购买，奖励值分别为0、1、5

## 2.4 Actor网络结构

Actor网络输入为当前状态，输出为推荐的物品信息。

### 2.4.1 得到初始状态

<img src="D:\TyporaPicture\22.12.26\image-20221226114311632.png" alt="image-20221226114311632" style="zoom: 80%;" />

输入为用户在此之前的点击或者购买的物品的信息，e~i~表示一个物品的特征向量。通过一个GRU提取出序列的信息，将GRU最后的状态h~N~作为初始状态s^ini^

### 2.4.2 得到当前状态

<img src="D:\TyporaPicture\22.12.26\image-20221226114329025.png" alt="image-20221226114329025" style="zoom:80%;" />

M为一个推荐页面所包含的物品数量，x~i~为:

![image-20221226114402761](D:\TyporaPicture\22.12.26\image-20221226114402761.png)

其中e~i~为物品i的特征向量，c~i~为物品i的类别，f~i~为用户对于该物品的反馈结果

x~i~经过embedding后得到X~i~，由三部分的embedding向量连接得到：

![image-20221226114712093](D:\TyporaPicture\22.12.26\image-20221226114712093.png)

后续使用CNN二维卷积提取该页面的特征信息：

![image-20221226114813059](D:\TyporaPicture\22.12.26\image-20221226114813059.png)

与得到初始状态类似，使用GRU提取所有页面组成的序列的特征，s^ini^作为GRU的初始状态。

引入Attention机制，来进一步获取用户的当前偏好信息：

![image-20221226114829300](D:\TyporaPicture\22.12.26\image-20221226114829300.png)

其中α~t~为：

![image-20221226114843785](D:\TyporaPicture\22.12.26\image-20221226114843785.png)

### 2.4.3 得到输出的动作

在得到当前状态后，使用反卷积网络得到动作矩阵a^cur^：

![image-20221226114854656](D:\TyporaPicture\22.12.26\image-20221226114854656.png)

a^cur^为一个h*w|E|大小的矩阵，h为页面的行数，w为页面的列数，|E|为特征向量的长度

## 2.5 Critic网络结构

<img src="D:\TyporaPicture\22.12.26\image-20221226114936538.png" alt="image-20221226114936538" style="zoom:80%;" />

a^cur^~pro~代表原始的动作矩阵，其中每个e~i~为一个item的特征向量，然而某些e~i~并不在数据集中，所以需要寻找每个e~i~最相似的在数据集中的item：

![image-20221226115204279](D:\TyporaPicture\22.12.26\image-20221226115204279.png)

从而将a^cur^~pro~映射为a^cur^~val~矩阵

## 2.6 训练过程

训练过程：

<img src="D:\TyporaPicture\22.12.26\image-20221226115715441.png" alt="image-20221226115715441" style="zoom:80%;" />

**Critic网络更新方式：**

loss函数（TD误差）：

![image-20221226115735775](D:\TyporaPicture\22.12.26\image-20221226115735775.png)

从而得到梯度：

![image-20221226115902341](D:\TyporaPicture\22.12.26\image-20221226115902341.png)

**Actor网络更新方式：**

![image-20221226115915510](D:\TyporaPicture\22.12.26\image-20221226115915510.png)

沿着Critic网络输出的Q值增大的方向更新参数。



## 2.7 测试过程

在线测试过程：

<img src="D:\TyporaPicture\22.12.26\image-20221226120049293.png" alt="image-20221226120049293" style="zoom:67%;" />

离线测试过程：

<img src="D:\TyporaPicture\22.12.26\image-20221226120104270.png" alt="image-20221226120104270" style="zoom:67%;" />

该过程对于该session中的所有待推荐物品进行重新排序，只对某一session的物品排序而不对所有物品排序还是因为推荐的物品交互记录不在数据集中的问题。



## 2.8 实验结果

### 2.8.1 实验设定

获取初始状态使用10条最近的记录，一个页面的布局为5行2列，略过、点击、购买的奖励值分别为0、1、5

### 2.8.2 离线测试结果

<img src="D:\TyporaPicture\22.12.26\image-20221226120655675.png" alt="image-20221226120655675" style="zoom:80%;" />

### 2.8.3 在线测试结果

将累计的奖励值作为评测指标：

<img src="D:\TyporaPicture\22.12.26\image-20221226120720633.png" alt="image-20221226120720633" style="zoom:80%;" />





# PG结合知识图谱

## 3 KERL A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation SIGIR 2020

## 3.1 背景/出发点

知识图谱由于其在诸多领域的适用性，并且能够指导基于强化学习的训练方法，其在序列推荐任务中已经有所使用，然而先前的研究中，通常只是利用知识图谱来‘exploitation——利用’，很少有人考虑知识图谱在‘exploration——探索’中的应用，这导致他们无法很好地捕捉用户的兴趣偏好在未来的变化。

## 3.2 本文主要工作

1）将序列化推荐任务进行马尔科夫决策过程建模，并且融合知识图谱提高推荐性能。

2）在框架中进行了三个主要的技术扩展，第一，提出用KG（知识图谱）信息增强状态表示。第二，设计了一个能够同时计算序列级和知识级的奖励值的复合奖励函数。第三，使用一个截断的策略梯度来训练模型，针对感应网络训练的不稳定性，引入了带有模拟子序列的学习机制。将模型命名为Knowledge-guided Reinforcement Learning Model（KERL）

3）在四个真实数据集中进行了实验，在不同的评测标准中推荐效果均有所提升

## 3.3 任务定义与MDP建模

**状态s~t~**设定为用户浏览序列的前t条记录以及知识图谱G：

![image-20221226064547219](D:\TyporaPicture\22.12.26\image-20221226064547219.png)

其中i~1:t~代表浏览序列的前t个物品。

**动作a~t~**设定为推荐的某一物品。

**奖励值r**为推荐的物品序列与数据集中的真实序列的相似度，以及根据知识图谱得到的序列embedding向量的相似度相加得到奖励值。

该推荐任务定义为：根据用户的历史交互记录以及知识图谱G，预测该用户下一个将要产生交互的物品。



## 3.4 模型搭建

### 3.4.1 模型整体框架

![image-20221226064731637](D:\TyporaPicture\22.12.26\image-20221226064731637.png)

### 3.4.2 策略函数定义以及状态转化

策略函数定义：

![image-20221226064748202](D:\TyporaPicture\22.12.26\image-20221226064748202.png)

其中q~ij~代表第j个物品的特征向量，W~1~代表该策略函数的参数，v~st~代表当前状态的特征向量。

该函数的作用类似DDPG算法中的Actor网络，根据当前的状态以及所有物品的特征向量，进行Softmax，得到所有物品的被推荐的概率，后续在模型训练完成后，将使用该策略函数进行推荐。

状态转化：

![image-20221226064937234](D:\TyporaPicture\22.12.26\image-20221226064937234.png)

### 3.4.3 状态特征向量的获取

状态特征向量的表示方法将包括两部分：序列级的表示以及知识级的表示

1）序列级的表示

该部分体现的是用户浏览序列的特征，使用GRU从前向后进行序列的特征提取：

![image-20221226065001915](D:\TyporaPicture\22.12.26\image-20221226065001915.png)

Φ~gru~代表GRU网络参数

2）知识级的表示

在以往的大多数方法中，主要考虑了使用知识图谱来得到用户短期的行为兴趣向量（利用），却很少考虑利用知识图谱来最大化长远的目标（探索），为了平衡探索以及利用，在知识级的特征向量表示中包括两部分：当前的偏好以及未来的偏好

**当前偏好（current preference）：**

一个物品在知识图谱中为一个实体，使用知识图谱的embedding method TransE将图中的一个物品i  embedding得到其特征向量v~i~

使用平均池化并求和得到当前偏好的特征向量：

![image-20221226065022367](D:\TyporaPicture\22.12.26\image-20221226065022367.png)

**未来偏好（future preference）:**

利用得到的当前偏好通过一个全连接网络直接预测未来偏好：

![image-20221226065035956](D:\TyporaPicture\22.12.26\image-20221226065035956.png)

其中f~t:t+k~代表未来的k步之内的偏好，Φ~mlp~代表该MLP的网络参数

3）得到状态特征向量

![image-20221226065048960](D:\TyporaPicture\22.12.26\image-20221226065048960.png)

将序列的特征向量以及当前偏好向量以及未来偏好向量连接得到状态特征向量。



### 3.4.4 奖励值的获取

奖励值同样由序列级的奖励以及知识级的奖励组成：

![image-20221226065111909](D:\TyporaPicture\22.12.26\image-20221226065111909.png)

其中i~t:t+k~代表数据集中的真实序列， i_hat~t:t+k~代表该模型预测的序列

1）序列级的奖励

该奖励通过对比预测出的长度为k的序列与真实序列的相似性计算得到，计算方式为BLEU：

![image-20221226065303468](D:\TyporaPicture\22.12.26\image-20221226065303468.png)

其中的prec~m~为：

![image-20221226065315375](D:\TyporaPicture\22.12.26\image-20221226065315375.png)

M为超参，代表在计算相似度时只考虑最长为M的序列。



2）知识级的奖励

使用相同的公式计算t:t+k时间段的真实序列以及预测序列的偏好特征向量：

![image-20221226065557784](D:\TyporaPicture\22.12.26\image-20221226065557784.png)

使用余弦相似度计算知识级的奖励：

![image-20221226065457170](D:\TyporaPicture\22.12.26\image-20221226065457170.png)

### 3.4.5 训练过程

**模型的整体训练流程：**

<img src="D:\TyporaPicture\22.12.26\image-20221226065651855.png" alt="image-20221226065651855" style="zoom:80%;" />

此处的截断表示在对Φ更新的公式中只对t到t+k时间的序列求和，而不是t时间后的所有序列。

## 3.5 实验结果

**数据集：**

![image-20221226065755811](D:\TyporaPicture\22.12.26\image-20221226065755811.png)

前三个为亚马逊的化妆品、CD、书籍的数据集，最后一个为一个用户听音乐记录的数据集

**实验结果：**

去除历史记录过少的用户与物品，对于每一个用户，按照时间顺序对历史行为记录排序。

在最后模型的对比评测时，模型将根据策略函数得到概率最高的k个物品，成为top-K推荐，评测指标选为Hit-Ratio@k以及NDCG@k。

![image-20221226065822364](D:\TyporaPicture\22.12.26\image-20221226065822364.png)

**消融实验：**

对于状态设定的消融实验：

<img src="D:\TyporaPicture\22.12.26\image-20221226065942777.png" alt="image-20221226065942777" style="zoom:80%;" />

对于奖励值设定的消融实验：

<img src="D:\TyporaPicture\22.12.26\image-20221226070010760.png" alt="image-20221226070010760" style="zoom:80%;" />



# 结合GAN框架

## 4 Leveraging Long and Short-Term Information in Content-Aware Movie Recommendation via Adversarial Training   IEEE transactions  2019

## 4.1出发点

RNN可以捕捉用户短期的兴趣偏好变化，矩阵分解的推荐方法基于用户的长期兴趣进行电影预测，其长期变化相对于时间变化非常缓慢。在本文中提出了一种新的LSIC模型，该模型利用对抗性训练在内容感知电影推荐中利用长期和短期信息，将基于MF和RNN的模型结合起来进行top-k推荐，充分利用每个模型提高最终推荐性能。

## 4.2模型结构

### 4.2.1模型整体结构



##### <img src="D:\TyporaPicture\22.12.26\image-20221016134223859.png" alt="image-20221016134223859" style="zoom:67%;" />

模型定义的符号：

<img src="D:\TyporaPicture\22.12.26\image-20221016134239076.png" alt="image-20221016134239076" style="zoom: 67%;" />

### 4.2.2生成器模型

LSIC-V1:

一个最简单的将MF与RNN结合的模型

<img src="D:\TyporaPicture\22.12.26\image-20221016134246401.png" alt="image-20221016134246401" style="zoom:67%;" />

t时刻使用LSTM得到的用户与电影的隐藏状态：

![image-20221016134252763](D:\TyporaPicture\22.12.26\image-20221016134252763.png)

<img src="D:\TyporaPicture\22.12.26\image-20221016134257631.png" alt="image-20221016134257631" style="zoom:67%;" />

LSIC-V2:

使用隐向量初始化LSTM的初始状态

<img src="D:\TyporaPicture\22.12.26\image-20221016134305176.png" alt="image-20221016134305176" style="zoom:67%;" />

LSIC-V3:

在LSTM每次计算时都再次输入用户与电影的隐向量：

<img src="D:\TyporaPicture\22.12.26\image-20221016134311836.png" alt="image-20221016134311836" style="zoom:67%;" />

LSIC-V4:

使用attention机制，将每一个隐藏状态赋予一个权值：

首先计算上下文向量：

<img src="D:\TyporaPicture\22.12.26\image-20221016134317472.png" alt="image-20221016134317472" style="zoom:67%;" />

其中的α与β通过类似softmax操作得到：

<img src="D:\TyporaPicture\22.12.26\image-20221016134324203.png" alt="image-20221016134324203" style="zoom:67%;" />

得到上下文向量后作为附加信息输入LSTM中：

<img src="D:\TyporaPicture\22.12.26\image-20221016134329062.png" alt="image-20221016134329062" style="zoom:67%;" />

最后通过如下公式得到电影得分：

<img src="D:\TyporaPicture\22.12.26\image-20221016134334963.png" alt="image-20221016134334963" style="zoom:67%;" />

## 4.3 GAN模型

鉴别器试图区分训练数据上的真实高评级电影与G预测的排名或推荐列表，并且发生器试图欺骗鉴别器以生成（预测）排名良好的推荐列表.具体地说,D(判别式模型)和 G(生成式模型)进行如下博弈V(D,G):

<img src="D:\TyporaPicture\22.12.26\image-20221016134343686.png" alt="image-20221016134343686" style="zoom:67%;" />

判别器的参数更新方式：

<img src="D:\TyporaPicture\22.12.26\image-20221016134349240.png" alt="image-20221016134349240" style="zoom:67%;" />

其中D的输出如下：

<img src="D:\TyporaPicture\22.12.26\image-20221112210537810.png" alt="image-20221112210537810" style="zoom:67%;" />

生成器的参数更新方式：

<img src="D:\TyporaPicture\22.12.26\image-20221016134354642.png" alt="image-20221016134354642" style="zoom:67%;" />



但是由于生成器所产生的推荐列表是离散的，不能直接使用上式更新，采用策略梯度下降法更新参数：

<img src="D:\TyporaPicture\22.12.26\image-20221016134401570.png" alt="image-20221016134401570" style="zoom:67%;" />

## 4.4模型整体训练流程

<img src="D:\TyporaPicture\22.12.26\image-20221016134408292.png" alt="image-20221016134408292" style="zoom:67%;" />





## 5 DRCGR：Deep reinforcement learning framework incorporating CNN and GAN-based for interactive recommendation  ICDM 2019

### 5.1 背景与本文贡献

问题背景：

深度强化学习模型在推荐领域已经有了一定的应用，但是在已经提出的模型中还存在一些问题：

1.目前的模型没有考虑到在用户的历史行为序列中的一些跳跃性的行为，历史行为记录中的行为可能在几步之后还有着较强的影响，目前很多模型中使用的RNN无法解决该问题

2.用户对于推荐结果的负反馈非常多且质量很低，需要能够生成一些与当前正反馈更相关的负样本来方便训练

本文贡献：

1.提出了一个使用CNN实现序列embedding的推荐模型，其能够更好地获取用户在序列中体现出的偏好信息

2.使用了GAN框架，一方面能够学习到负样本的分布（生成器），另一方面利用生成器的输出与当前状态下真实的样本来指导生成器的学习（判别器）

3.提出了一个基于深度强化学习的框架DRCGR，其能够利用用户的正负反馈自动地学习到最优的推荐策略

### 5.2 模型构成

<img src="D:\TyporaPicture\22.12.26\image-20221226071501991.png" alt="image-20221226071501991" style="zoom:67%;" />

状态s在此模型中定义为：

<img src="D:\TyporaPicture\22.12.26\image-20221226071554641.png" alt="image-20221226071554641" style="zoom:80%;" />

其中前者为该用户近期做出过正反馈即点击或者购买的物品序列：

<img src="D:\TyporaPicture\22.12.26\image-20221226071609716.png" alt="image-20221226071609716" style="zoom:80%;" />

后者为该用户近期做出过负反馈即略过的物品序列：

<img src="D:\TyporaPicture\22.12.26\image-20221226071618356.png" alt="image-20221226071618356" style="zoom:80%;" />

对于某时刻推荐的物品，如果用户对其做出了正反馈，则添加到正反馈序列中，如果做出负反馈则添加到负反馈序列中。状态中考虑到了负反馈序列这样即使用户都做出负反馈状态也能够发生改变，从而推荐结果产生变化。

<img src="D:\TyporaPicture\22.12.26\image-20221226071700364.png" alt="image-20221226071700364" style="zoom: 67%;" />

在此阶段中，输入为用户此时的状态，包括正反馈序列以及负反馈序列，将两个序列embedding后利用水平卷积核以及竖直卷积核进行特征提取，再通过FC层后得到用户序列的时序特征。

对物品序列embedding后得到如下L*d维矩阵

<img src="D:\TyporaPicture\22.12.26\image-20221226071734728.png" alt="image-20221226071734728" style="zoom:80%;" />

在该矩阵中使用水平与竖直卷积核得到特征向量X与X_hat，将两个结果结合后通过FC层得到序列的特征向量：

![image-20221226072033090](D:\TyporaPicture\22.12.26\image-20221226072033090.png)

<img src="D:\TyporaPicture\22.12.26\image-20221226072118803.png" alt="image-20221226072118803" style="zoom:67%;" />

在该阶段中利用GAN框架，根据当前用户的正反馈的序列得到与其更相关的负反馈的序列的特征向量。

生成器：输入为第一阶段的正反馈的特征向量，输出为fake的负反馈特征向量

判别器：输入为第一阶段得到的正负序列向量对（real sample）以及正反馈向量+生成器的输出负反馈向量对（fake sample），输出为一个标量值

<img src="D:\TyporaPicture\22.12.26\image-20221226072950077.png" alt="image-20221226072950077" style="zoom:80%;" />

<img src="D:\TyporaPicture\22.12.26\9ee799fe0dfbfc17ba603baffe22994.jpg" alt="9ee799fe0dfbfc17ba603baffe22994" style="zoom:80%;" />





GAN整体的目标函数如下：

![image-20221226072152946](D:\TyporaPicture\22.12.26\image-20221226072152946.png)

其中P为判别器D的输出，由如下公式计算得到：

<img src="D:\TyporaPicture\22.12.26\image-20221226072420290.png" alt="image-20221226072420290" style="zoom:80%;" />

对于判别器的参数更新如下：

![image-20221226073032161](D:\TyporaPicture\22.12.26\image-20221226073032161.png)

对于生成器的参数更新如下：

![image-20221226073050280](D:\TyporaPicture\22.12.26\image-20221226073050280.png)

通过第二阶段中的生成器G得到负反馈序列的向量表示后，与第一阶段的正反馈序列以及某个物品动作a共同输入到DQN网络中，在网络最后将两者相结合后经过两层FC层得到最终的对于该动作a的q值。

后续训练时的loss函数为：

<img src="D:\TyporaPicture\22.12.26\image-20221226073223250.png" alt="image-20221226073223250" style="zoom:80%;" />

其中：

<img src="D:\TyporaPicture\22.12.26\image-20221226073227546.png" alt="image-20221226073227546" style="zoom:80%;" />



### 5.3 实验

实验数据集为一个真实的网络商品推荐数据集，其中共包含9136976个物品，1048576个session。

在测试时是对于每个session中的物品进行重新排列，选取NDCG与MAP作为评测标准

<img src="D:\TyporaPicture\22.12.26\image-20221226073305745.png" alt="image-20221226073305745" style="zoom: 67%;" />



## 6 Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation  NeurIPS 2019

## 6.1 背景

把推荐系统建模成为一个强化学习模型，需要智能体不断与真实环境进行交互完成训练，但是在实际情况下，不可能让一个未经过训练的模型在真实的推荐环境下进行训练，那样会导致用户的大量流失。

## 6.2 本文主要工作

利用离线数据建模得到的模型模拟真实环境，实现一个model-based的强化学习模型，并结合了GAN结构进行学习。提出的模型在真实数据集中得到了验证。

## 6.3 问题定义

在该强化学习模型中，状态是用户在某时刻前最近点击过的一个物品列表：

<img src="D:\TyporaPicture\22.12.26\image-20221113154959474.png" alt="image-20221113154959474" style="zoom:80%;" />

智能体的动作是一个长度为k的推荐列表，智能体在某一时刻执行动作，我们能够得到一个推荐序列：

<img src="D:\TyporaPicture\22.12.26\image-20221113152926346.png" alt="image-20221113152926346" style="zoom:67%;" />

其中a^i^~t~是智能体在t时刻采取的动作，c^i^~t~是用户对于该推荐列表的点击动作，r^i^~t~是根据用户反馈得到的奖励值。

为了一定程度上简化模型，本文中做出一些设定：

1）对于那些推荐列表中不被点击的物品，将不会影响用户的下一个状态，即不考虑负反馈序列

2）奖励值只与被点击的物品有关

## 6.4 用户行为建模（model-based）

利用RNN提取序列的特征信息作为状态：

<img src="D:\TyporaPicture\22.12.26\image-20221113155449183.png" alt="image-20221113155449183" style="zoom:67%;" />

模型整体框架如下：

![image-20221113155614004](D:\TyporaPicture\22.12.26\image-20221113155614004.png)

得到状态表示后根据如下公式得到用户点击的概率分布：

![image-20221113155843964](D:\TyporaPicture\22.12.26\image-20221113155843964.png)

其中E^u^~t~是该推荐列表的embedding 矩阵

根据如下公式计算奖励值r：

<img src="D:\TyporaPicture\22.12.26\image-20221113160125149.png" alt="image-20221113160125149" style="zoom:67%;" />

看到奖励值的计算只与被点击的物品有关，其中e^u^~t~是根据数据集得到的在该时刻用户的点击的物品。

通过最大化如下函数更新参数得到用户的交互模型：

<img src="D:\TyporaPicture\22.12.26\image-20221113160337074.png" alt="image-20221113160337074" style="zoom:67%;" />

根据离线数据集得到用户模型后，在后续的训练过程中便可以得到用户对于某推荐动作的反馈以及奖励值。

## 6.5 策略函数

对于智能体采取动作的状态表示以及用户进行点击的状态表示可能不同，

在本文中智能体侧以及用户侧分别有一个状态表示，两者使用相同结构的RNN得到，但网络参数不同。

得到智能体侧的状态表示后根据如下softmax公式得到动作：

<img src="D:\TyporaPicture\22.12.26\image-20221113161733663.png" alt="image-20221113161733663" style="zoom:80%;" />

其中C是所有的候选物品集合

## 6.6 模型训练

本文中使用策略梯度下降中的REINFORCE算法进行训练，并且引入GAN结构，生成器能够得到一个推荐轨迹，在判别器中需要判断这个轨迹是生成器生成的还是数据集中真实的轨迹

通过最小化如下公式训练判别器D:

![image-20221113162559322](D:\TyporaPicture\22.12.26\image-20221113162559322.png)

由于D只能评估一整个序列的真假，无法评估其中的一段序列，所以这里引入如下公式对于使得能够评估一段序列：

![image-20221113163206597](D:\TyporaPicture\22.12.26\image-20221113163206597.png)

其中MC^U,A^(τ~0:t~;N)为：

![image-20221113163900667](D:\TyporaPicture\22.12.26\image-20221113163900667.png)

对于用户侧的参数更新的梯度为：

![image-20221113164110321](D:\TyporaPicture\22.12.26\image-20221113164110321.png)

对于智能体侧的参数更新的梯度为：

![image-20221113164229406](D:\TyporaPicture\22.12.26\image-20221113164229406.png)



个人感觉这样是要将判别器的输出融入进奖励值r的计算中，从而可以采用策略梯度下降的方式更新G

## 6.7 实验结果

![image-20221113191302689](D:\TyporaPicture\22.12.26\image-20221113191302689.png)

评测指标采用Precision@k（P@1和P@10）

对比的算法为：

LSTM：通过离线数据训练用户行为模型

LSTMD：通过GAN训练RecGAN中的用户行为模型

PG：通过离线数据使用策略梯度下降训练agent模型

PGIS：agent模型利用离线数据重要性采样来减小偏差

AC：一个LSTM模型利用actor-critic结构进行训练

PGU：agent模型利用离线数据以及生成数据进行训练，但是没有GAN结构

ACU：actor-critic模型通过离线以及生成数据进行训练，但没有GAN结构



## 7 RecGAN Recurrent Generative Adversarial Networks for Recommendation Systems RecSys 2019

对于某一用户，使用如下向量体现其兴趣偏好：

![image-20221113134149375](D:\TyporaPicture\22.12.26\image-20221113134149375.png)

其中y^u^~t~是一个向量，是该用户在t时刻对于所有物品的偏好得分，可能为显式可能为隐式。

**生成器：**

利用GRU模型提取该兴趣偏好向量的信息：

<img src="D:\TyporaPicture\22.12.26\image-20221113134615463.png" alt="image-20221113134615463" style="zoom:80%;" />



生成器的输出为1到T时刻的偏好向量：

![image-20221113135249785](D:\TyporaPicture\22.12.26\image-20221113135249785.png)

其中H^u^~g~为：

![image-20221113135402290](D:\TyporaPicture\22.12.26\image-20221113135402290.png)

生成器的输出是在1到T时刻下该用户对于所有物品的评分向量。



**判别器：**

与生成器使用相同的GRU结构：

<img src="D:\TyporaPicture\22.12.26\image-20221113140125427.png" alt="image-20221113140125427" style="zoom:80%;" />

真样本是数据集中得到的在1到T时刻之间的用户兴趣偏好向量，假样本是G生成得到的1到T时刻之间的偏好向量。

D根据如下公式得到输出：

![image-20221113141304075](D:\TyporaPicture\22.12.26\image-20221113141304075.png)

最后需要通过sigmoid将输出限制在(0,1)，其中的H^u^~d~如下：

<img src="D:\TyporaPicture\22.12.26\image-20221113141252145.png" alt="image-20221113141252145" style="zoom:80%;" />



GAN的目标函数：

![image-20221113142426601](D:\TyporaPicture\22.12.26\image-20221113142426601.png)

其中D(r|i,j)~real|t~代表数据集中的真实分布，D(r|i,j)~gen|t~代表G生成的分布

Dis(r|i,j,t)表示在t时刻，用户i对于物品j评分为r的真假情况，值越大表示判别器任务该评分越真实

该公式中对于items的求和代表D是对于用户对于所有物品的兴趣偏好的概率进行估计。



判别器D的参数更新为：

<img src="D:\TyporaPicture\22.12.26\image-20221113143345499.png" alt="image-20221113143345499" style="zoom:67%;" />

生成器G的参数更新为：

<img src="D:\TyporaPicture\22.12.26\image-20221113143509852.png" alt="image-20221113143509852" style="zoom:67%;" />



**实验结果**

在MyFitnessPal数据集上的结果：

<img src="D:\TyporaPicture\22.12.26\image-20221113145349407.png" alt="image-20221113145349407" style="zoom:67%;" />

在Netflix数据集上的结果：

<img src="D:\TyporaPicture\22.12.26\image-20221113145619835.png" alt="image-20221113145619835" style="zoom:67%;" />

进行消融实验的结果：

<img src="D:\TyporaPicture\22.12.26\image-20221113145729362.png" alt="image-20221113145729362" style="zoom:67%;" />

其中RecGAN1是在GRU模型中没有RELU激活函数的模型，RecGAN2是有RELU激活函数的模型，即本文提出的模型。



## 8 Simulating User Feedback for Reinforcement Learning Based Recommendations  AAAI 2019

### 8.1 背景

把推荐系统建模成为一个强化学习模型，需要智能体不断与真实环境进行交互完成训练，但是在实际情况下，不可能让一个未经过训练的模型在真实的推荐环境下进行训练，那样会导致用户的大量流失，首先需要进行离线训练。

### 8.2 本文贡献

（1）我们结合GAN来捕捉历史浏览记录中的推荐序列信息的潜在分布，并且利用生成器生成推荐的物品

（2）提出了一个用户的行为模拟器RecSimu，来更方便地进行强化学习模型的离线训练

（3）在真实世界的数据集中验证了建立的用户模型的有效性

### 8.3 模型结构

模型整体结构如下：

<img src="D:\TyporaPicture\22.12.26\image-20221226102547969.png" alt="image-20221226102547969" style="zoom:67%;" />

生成器：

<img src="D:\TyporaPicture\22.12.26\image-20221226102652209.png" alt="image-20221226102652209" style="zoom:67%;" />

生成器中使用GRU提取历史记录的序列特征，最终输出一个推荐物品

判别器:

<img src="D:\TyporaPicture\22.12.26\image-20221226102708774.png" alt="image-20221226102708774" style="zoom:67%;" />

在判别器中，需要能够分辨出真实的<s,a>对以及生成器生成的<s,a>对，并且能够预测用户对于该推荐动作的反馈。

<img src="D:\TyporaPicture\22.12.26\image-20221226103137682.png" alt="image-20221226103137682" style="zoom: 80%;" />

<img src="D:\TyporaPicture\22.12.26\image-20221226103258413.png" alt="image-20221226103258413" style="zoom:80%;" />

### 8.4 模型训练

判别器的loss包括两部分，第一部分为其输出的<s,a>对是否为real或者fake的概率

为real的概率：

<img src="D:\TyporaPicture\22.12.26\image-20221226103547737.png" alt="image-20221226103547737" style="zoom:80%;" />

为fake的概率：

<img src="D:\TyporaPicture\22.12.26\image-20221226103554283.png" alt="image-20221226103554283" style="zoom:80%;" />

<img src="D:\TyporaPicture\22.12.26\image-20221226103603199.png" alt="image-20221226103603199" style="zoom:80%;" />

第二部分为有监督部分得到的loss：

使用交叉熵损失函数：

![image-20221226103826908](D:\TyporaPicture\22.12.26\image-20221226103826908.png)

判别器的loss为两部分的结合：

![image-20221226103845400](D:\TyporaPicture\22.12.26\image-20221226103845400.png)

生成器loss同样包括两部分：

![image-20221226104003974](D:\TyporaPicture\22.12.26\image-20221226104003974.png)



![image-20221226104011646](D:\TyporaPicture\22.12.26\image-20221226104011646.png)

生成器loss为两部分结合：

![image-20221226104018463](D:\TyporaPicture\22.12.26\image-20221226104018463.png)

### 8.5 实验

数据集为一个商品推荐数据集，在每个session中，我们可以得到一系列的（state,action,reward）对，根据这些元组进行生成器与判别器的训练

<img src="D:\TyporaPicture\22.12.26\image-20221226104517798.png" alt="image-20221226104517798" style="zoom: 80%;" />





# 强化学习算法

## 9 Twin Delayed Deep Deterministic policy gradient algorithm （TD3）PMLR 2018

TD3算法是从DDPG算法优化而来，主要有三个优化点：

（1）用类似双Q网络的方法，解决了DDPG中Critic对高估计动作Q值的问题

（2）延迟actor更新，让actor的训练更加稳定

（3）在target_actor中加入了噪声，增加了算法的稳定性

DDPG结构：

<img src="D:\TyporaPicture\22.12.26\image-20221226123932890.png" alt="image-20221226123932890" style="zoom:80%;" />

DDPG中使用一个Critic网络进行动作的评估，其输出可能较高，TD3中增加一个Critic网络，Q取两个网络的输出的较小值：

<img src="D:\TyporaPicture\22.12.26\image-20221226124716415.png" alt="image-20221226124716415" style="zoom:80%;" />



在模型训练时，critic网络更新多次之后，再更新actor网络，可以想象，本来是最高点的，当actor好不容易去到最高点；q值更新了，这并不是最高点。这时候actor只能转头再继续寻找新的最高点。更坏的情况可能是actor被困在次高点，没有找到正确的最高点。

所以可以把Critic的更新频率，调的比Actor要高一点。让critic更加确定，actor再行动。

在计算target时候，在target_policy_net的输出动作中加入噪声，相当于在s‘这条线上的一定范围内估计Q(s,a)，使得Q(s,a)的估计更准确，更健壮

更新actor的时候，不需要加上noise，这里是希望actor能够寻着最大值。加上noise没有意义
