# 1 Deep Reinforcement Learning for Whole-Chain Recommendations    WSDM 2020

## 1.1 背景/出发点

在实际推荐情况的一个session中，用户会遇到不同的推荐场景，比如在首页浏览物品以及在物品详情页浏览物品，不同场景下的推荐策略也应该不同，在首页的推荐策略需要同时注重相似性以及多样性，而在物品详情页更加注重相似性。

当前的大多算法对于这些不同的推荐场景只采用一种推荐策略，又或者对于不同的场景分别使用各自的数据单独训练推荐策略，这会导致找不到全局的最优策略。

而优化多种推荐策略存在以下问题：

1）需要大量的用户历史行为数据，若多种策略共同优化则需要的数据量更多

2）历史行为数据中，绝大多数是skip的行为，click以及purchase的行为很少，导致reward的分布非常的不平衡，使得Q值的更新不准确

## 1.2 主要工作

1）为了解决上述问题，本文提出了一个multi-agent  model-based的强化学习模型(DeepChain)，能够共同训练不同场景下的agent，对于数据的需求量下降并且使得Q值的更新更准确。

2）使用GRU+Attention机制，提取出用户状态中体现出的序列信息

3）在一个真实数据集中进行实验，验证了结果的准确性

## 1.3 MDP建模

状态S的设定：将用户最近所点击或者购买的物品序列按照时间排序，代表该用户最近的偏好

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658167564016-a06a489b-3f13-4760-b9a9-1da3d4fba03b.png" alt="img" style="zoom:67%;" />

动作A的设定：在某一状态下推荐的物品列表作为动作，在本文中假定每次只推荐一个物品

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658167694581-5f3023d1-9e73-4764-818b-cd439e76d702.png" alt="img" style="zoom:67%;" />

奖励值R的设定：在做出推荐动作后，用户浏览推荐结果并且做出相应的反馈，包括略过、点击、离开，对应的奖励值分别为0、1、-2

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658168045191-71cc77c5-f366-47f3-b3a6-9930eb954a23.png" alt="img" style="zoom: 80%;" />



## 1.4 模型结构

### 1.4.1 模型整体结构

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658170121003-a47e288b-6a8b-4b6b-9a71-195bc3f5b560.png" alt="img" style="zoom: 67%;" />

使用DDPG作为模型的整体框架，设定两个推荐场景：用户在首页浏览、用户在物品详情页浏览，每个推荐场景分别对应一个actor，将actor输出的动作以及状态s输入到唯一的一个critic网络中得到Q值。

### 1.4.2 Actor网络结构

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658170479947-452e58d5-268a-4458-a388-239d1b64aae5.png" alt="img" style="zoom: 67%;" />

两个actor网络结构相同，只是参数不同。

网络的输入是用户最近点击或者购买的物品的embedding向量，按照时间排序，使用GRU提取序列的特征信息，并且使用item-level attention机制得到a~t~  ，之后将a~t~与所有的候选物品的特征向量相乘得到score，取score最高的物品作为输出。

### 1.4.3 Critic网络结构

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658170962774-ee816f27-9e34-4595-9c82-e920434f0533.png" style="zoom: 67%;" />

critic网络只有一个，其输入是状态以及actor网络输出的动作，对于状态的处理方式与actor相同，使用两种不同的attention机制来区分两种场景，1~m~与1~d~控制使用哪种机制，同一时刻只有一种机制生效。p~C~代表用户的偏好向量。



### 1.4.4 概率模型的建立与参数更新

在首页推荐场景中，用户对推荐结果有三种选择：略过并留在首页、点击进入详情页、离开，在物品详情推荐场景中，用户对推荐结果有三种选择：点击并留在详情页、略过并退出到首页、离开。

利用该模型计算yt，即Q值的目标值：

<img src="https://cdn.nlark.com/yuque/0/2022/png/25581050/1658171988035-d7e98594-bafc-4bca-84e9-0f0df054e93f.png" alt="img" style="zoom: 67%;" />

其中1~m~与1~d~控制两种推荐场景各自的激活，π~m~^'^表示首页场景的actor的target网络参数，π~d~^'^代表物品详情场景的actor的target网络参数，μ^'^代表critic的target网络参数。

使用与critic网络结构相同的网络作为概率的生成网络，该网络的输出层中使用softmax，分别得到s、c、l行为的概率，训练时使用交叉熵损失函数更新网络参数。

## 1.5 模型训练与测试流程

**模型的训练流程：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719034519356.png" alt="image-20220719034519356" style="zoom:67%;" />

critic网络的更新方式如下：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719040653933.png" alt="image-20220719040653933" style="zoom:67%;" />

actor网络的更新方式如下，沿着Q上升的方向：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719041110913.png" alt="image-20220719041110913" style="zoom:67%;" />

**模型的离线测试流程：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719035137240.png" alt="image-20220719035137240" style="zoom:67%;" />

**模型的在线测试流程：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719035204616.png" alt="image-20220719035204616" style="zoom:67%;" />

## 1.6 实验结果

数据集信息：数据来源于2018年12月的一个电子商务平台的数据，挑选出了500000个sessions，前80%作为训练集，后20%作为测试集

**离线实验结果：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719041403429.png" alt="image-20220719041403429" style="zoom:70%;" />

MA是一个multi-agent  model-free的模型，本文的DeepChain模型超越MA是因为model-based的模型能够更加准确地更新Q值。

**在线测试结果：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719042403245.png" alt="image-20220719042403245" style="zoom: 45%;" />

其中DC-o是DeepChain的一个agent的版本，DC-f是DeepChain的model-free的版本，即不会使用用户行为概率模型。



# 2 Value-aware Recommendation based on Reinforced Profit Maximization in E-commerce Systems   WWW 2019

## 2.1 背景

目前存在的推荐算法大多都更加关注传统的推荐结果评测指标，例如：做评分预测方向的算法使用RMSE指标，top-k问题使用precision、recall、MAP等指标衡量推荐结果列表。然而对于一款商业推荐系统，所获得利润的提升也是一个非常重要的衡量标准，对于预测问题以及top-k问题的优化目标都不是与系统利润直接相关。

## 2.2 主要工作

1）将click conversion rate(CVR)扩展到用户的所有动作上，因此用户的每个动作都能够被量化为利润

2）提出value-aware推荐模型，使用利润作为奖励值，来直接最大化一个推荐系统的利润

3）整合所有量化后的用户动作为奖励值r，构建强化学习模型并进行训练

## 2.3 扩展CVR

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719101022502.png" alt="image-20220719101022502" style="zoom:67%;" />

CVR为在用户点击某物品的前提下，会购买该物品的概率：

p~CVR~=p(conversion|click,impression)

此时系统利润可由CVR计算得到：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719101333573.png" alt="image-20220719101333573" style="zoom:67%;" />

XVR为用户在采取某一动作x的前提下，会购买该物品的概率。

动作x包括click、add to cart、add to wishlist、purchase，此时系统利润如下：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719101838039.png" alt="image-20220719101838039" style="zoom:67%;" />

## 2.4 模型构建

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719101951352.png" alt="image-20220719101951352" style="zoom:200%;" />

**状态S的设定：**

状态用于描述用户的请求信息，包括用户端、物品端以及上下文的特征信息，用户端包括用户年龄、性别、购买力等信息，物品端包括CTR、CVR、物品价格等信息，上下文信息包括pageid、请求时间

**动作A的设定：**

在agent得到状态s后输出动作a，该动作用于计算物品得分，将所有待推荐物品排序：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719102830764.png" alt="image-20220719102830764" style="zoom:67%;" />

**动作a表示为：**

​                                                       <α~click~，α~cart~，α~fav~，β~click~，β~cart~，β~fav~，γ>

如果只考虑点击动作，则对于物品i的评分为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719103639361.png" alt="image-20220719103639361" style="zoom:67%;" />

**奖励值R的设定：**

将预期的利润作为奖励值，对于一个物品的奖励值为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719103942806.png" alt="image-20220719103942806" style="zoom:67%;" />

在本文中只使用click与pay组成奖励值：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719104710564.png" alt="image-20220719104710564" style="zoom: 33%;" />

整个页面所有物品的奖励值为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719104823906.png" alt="image-20220719104823906" style="zoom: 33%;" />

## 2.5 模型训练

为了体现出推荐结果中物品顺序的影响，推荐结果的奖励值修改为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719105238716.png" alt="image-20220719105238716" style="zoom:50%;" />

模型采用了非常简单的evolution strategy算法，根据输入的状态线性得到输出的动作向量，在用户请求到达时，该算法产生n个子agent，每个agent（用j表示）向当前参数θ中添加噪声项ε^j^~t~，每个agent都会将状态映射到动作上，根据该动作计算所有物品得分，排序后呈现给用户。

对于这n个结果，分别计算奖励值，并用下式更新参数：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719110435410.png" alt="image-20220719110435410" style="zoom:67%;" />



## 2.6 实验结果

数据集使用一个电商平台的真实数据集，其中的每条数据包含当前状态以及用户对于推荐结果的反馈信息。

离线测试结果：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719111339282.png" alt="image-20220719111339282" style="zoom:67%;" />

在线测试结果：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220719111400932.png" alt="image-20220719111400932" style="zoom:67%;" />

CTR是点击率，IPV是平台上的被点击的物品数量占比



# 3 学期论文总结

## 3.1 对于DQN模型的优化

1）Dueling Network Architectures for Deep Reinforcement Learning **(Dueling Network)**

提出了一种新型DQN网络结构，将Q值分解为状态价值V以及优势函数A，在网络的最后将二者组合，输出所有动作的Q值。基于这样的网络结构，能够更快地学习状态价值V，模型收敛速度提高

2）Prioritized Experience Replay

引入优先经验回放机制，改变抽取样本时的概率，减低对于训练意义不大的样本被抽取到的概率，提高模型收敛速度

3）Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation **(Robust DQN)**

提出了分层样本采样以及对于奖励值的估计方式，使得在更为复杂的动态推荐环境下，使得奖励值的估计更加准确，提高模型准确率



## 3.2 value-based DRL推荐系统

1）A Deep Reinforcement Learning Framework for News Recommendation **(DRN)**

使用Dueling网络作为Q网络，在奖励值r的设定中同时考虑用户行为以及用户的活跃度，能够提供更准确的奖励值信息。

提出了一种新的探索方式DBGD，缓解了使用ϵ-greedy导致的准确率下降的问题

2）Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning **(DEERS)**

在状态的设定中，同时考虑了用户近期点击以及略过的物品信息，使得用户在未点击任一物品的情况下，状态也能够有所改变。

在loss函数中，加入了pairwise正则化项，提高了训练效果

3）Value-aware Recommendation based on Reinforced Profit **(Value-aware)**

将click conversion rate(CVR)扩展到用户的所有动作上，因此用户的每个动作都能够被量化为利润

提出value-aware推荐模型，使用利润作为奖励值，来直接最大化一个推荐系统的利润。

## 3.3 policy-based DRL

1）Deterministic Policy Gradient Algorithms **(DPG)**

最大贡献是调整了策略参数θ朝着值函数Q梯度上升的方向，为DDPG算法提供基础

2）Continuous Control With Deep Reinforcement Learning **(DDPG)**

相比于DQN，该算法使用了一个actor网络解决了动作空间过大的问题，actor网络输出一个动作a，将动作a与状态s输入到critic网络中得到Q值。

在目标网络的更新中，提出了软更新方法，使得目标网络参数的更新不会产生过大的突变



## 3.4 policy-based DRL推荐算法

1）Deep Reinforcement Learning for List-wise Recommendations **(LIRD)**

使用了DDPG算法，相比于value-based方法，推荐时的计算消耗降低，actor网络的输出结果与所有物品embedding向量相乘得到得分，根据得分选取出推荐列表。

根据数据集中的历史数据，建立了user-agent交互模拟器，使得在离线训练时所有物品都有可能被推荐，对于数据集中未出现的物品，利用模拟器计算得到奖励值

2）Deep Reinforcement Learning for Whole-Chain Recommendations **(DeepChain)**

提出了一个multi-agent  model-based模型，首页以及物品详情页两个不同的推荐场景，分别使用一个actor网络进行推荐，并在网络训练中。使用相同的更新策略对两个actor网络同时更新。

建立了用户行为概率模型，使用该模型计算y值，相比于model-free的模型，动作价值函数的更新更加准确，提高推荐效果





