之前遗留的问题：

# 1  Leveraging Long and Short-term Information in Content-aware Movie Recommendation

![image-20221112205732484](D:\TyporaPicture\22.11.15\image-20221112205732484.png)

GAN的目标函数：

<img src="D:\TyporaPicture\22.11.15\image-20221112205846899.png" alt="image-20221112205846899" style="zoom:67%;" />

判别器的参数更新：

<img src="D:\TyporaPicture\22.11.15\image-20221112210319192.png" alt="image-20221112210319192" style="zoom:67%;" />

其中D的输出如下：

<img src="D:\TyporaPicture\22.11.15\image-20221112210537810.png" alt="image-20221112210537810" style="zoom:67%;" />

之前的理解是，D的两个输入差异越小输出越大，但这样对于判别器更新公式的前半部分理解有误。

现在的理解是，这里D的输入不是将真假样本同时输入，而是要么输入真样本要么输入假样本，即真样本对是<m-,m+>，假样本对是<m~gt~,m+>，m~gt~被D当成m-，D是要判断这两个样本对的真假。

不考虑生成器G的输出的话，衡量样本对的真假应该是m+得分越高，m-得分越低则样本对越真，判别器D是不认可G输出的得分的所以在D的输出中取反。



# 2  DRCGR:Deep reinforcement learning framework incorporating CNN and GAN-based for interactive recommendation

<img src="D:\TyporaPicture\22.11.15\image-20221112213143546.png" alt="image-20221112213143546" style="zoom:67%;" />

GAN目标函数：

![image-20221112213236213](D:\TyporaPicture\22.11.15\image-20221112213236213.png)

其中P为判别器输出值，其估计出与该正反馈序列对应的负反馈序列的概率：

![image-20221112213309113](D:\TyporaPicture\22.11.15\image-20221112213309113.png)

将前面卷积得到的正负反馈的embedding作为真样本对，将卷积得到的正反馈embedding与G生成的负反馈embedding作为假样本对，D用于衡量这两个样本对的真假。

其中D输入中还要包括正反馈序列，因为如果只有负反馈序列输入，有的负反馈序列可能为真样本，但并不是该正反馈序列所对应的，D要衡量的是在该正反馈序列的前提下，负反馈序列的真假。

后续的几篇论文中也采用了类似的输入，采用条件概率的形式，引入附加信息使得D更好判断。



# 3 GAN for Recommendation System   JPCS 2019

## 3.1 总体框架

<img src="D:\TyporaPicture\22.11.15\image-20221113105712918.png" alt="image-20221113105712918" style="zoom:67%;" />

GAN的目标函数：

![image-20221113105912782](D:\TyporaPicture\22.11.15\image-20221113105912782.png)

图中y是用户历史选择，z是在生成器的输入中添加的噪声，G是预测的新的选择。

判别器中x是数据集中真实的下一个选择，y是历史选择。

## 3.2 生成器G

生成器的输入包括两部分：用户在某时刻前历史选择以及额外的噪声,输出是预测的用户未来的选择。

## 3.3 判别器D

判别器用来判断输入是数据集中真的未来的选择还是G生成的，判别器的输入包括两部分：某时刻前用户的历史选择以及未来的选择，输出一个[0,1]的标量值。

## 3.4 实验结果

<img src="D:\TyporaPicture\22.11.15\image-20221113124251236.png" alt="image-20221113124251236" style="zoom:67%;" />

DSSM中没有引入GAN结构

将预测的下一选择推送给用户，统计用户转换率



# 4 RecGAN: Recurrent Generative Adversarial Networks for Recommendation Systems RecSys 2018

对于某一用户，使用如下向量体现其兴趣偏好：

![image-20221113134149375](D:\TyporaPicture\22.11.15\image-20221113134149375.png)

其中y^u^~t~是一个向量，是该用户在t时刻对于所有物品的偏好得分，可能为显式可能为隐式。

**生成器：**

利用GRU模型提取该兴趣偏好向量的信息：

<img src="D:\TyporaPicture\22.11.15\image-20221113134615463.png" alt="image-20221113134615463" style="zoom:80%;" />



生成器的输出为1到T时刻的偏好向量：

![image-20221113135249785](D:\TyporaPicture\22.11.15\image-20221113135249785.png)

其中H^u^~g~为：

![image-20221113135402290](D:\TyporaPicture\22.11.15\image-20221113135402290.png)

而从t=1到t=t~train~之间的ground-truth已知，所以将这部分G的生成替换为真实偏好向量，t=t~train~到t=T之间的是生成器G预测得到的。

**判别器：**

与生成器使用相同的GRU结构：

<img src="D:\TyporaPicture\22.11.15\image-20221113140125427.png" alt="image-20221113140125427" style="zoom:80%;" />

真样本是数据集中得到的在1到T时刻之间的用户兴趣偏好向量，假样本是G生成得到的1到T时刻之间的偏好向量。

D根据如下公式得到输出：

![image-20221113141304075](D:\TyporaPicture\22.11.15\image-20221113141304075.png)

最后需要通过sigmoid将输出限制在(0,1)，其中的H^u^~d~如下：

<img src="D:\TyporaPicture\22.11.15\image-20221113141252145.png" alt="image-20221113141252145" style="zoom:80%;" />



GAN的目标函数：

![image-20221113142426601](D:\TyporaPicture\22.11.15\image-20221113142426601.png)

其中D(r|i,j)~real|t~代表数据集中的真实分布，D(r|i,j)~gen|t~代表G生成的分布

该公式中对于items的求和代表D是对于用户对于所有物品的兴趣偏好的概率进行估计。



判别器D的参数更新为：

<img src="D:\TyporaPicture\22.11.15\image-20221113143345499.png" alt="image-20221113143345499" style="zoom:67%;" />

生成器G的参数更新为：

<img src="D:\TyporaPicture\22.11.15\image-20221113143509852.png" alt="image-20221113143509852" style="zoom:67%;" />



**实验结果**

在MyFitnessPal数据集上的结果：

<img src="D:\TyporaPicture\22.11.15\image-20221113145349407.png" alt="image-20221113145349407" style="zoom:67%;" />

在Netflix数据集上的结果：

<img src="D:\TyporaPicture\22.11.15\image-20221113145619835.png" alt="image-20221113145619835" style="zoom:67%;" />

进行消融实验的结果：

<img src="D:\TyporaPicture\22.11.15\image-20221113145729362.png" alt="image-20221113145729362" style="zoom:67%;" />

其中RecGAN1是在GRU模型中没有RELU激活函数的模型，RecGAN2是有RELU激活函数的模型，即本文提出的模型。



# 5 Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation  NeurIPS 2019

## 5.1 背景

把推荐系统建模成为一个强化学习模型，需要智能体不断与真实环境进行交互完成训练，但是在实际情况下，不可能让一个未经过训练的模型在真实的推荐环境下进行训练，那样会导致用户的大量流失。

## 5.2 本文主要工作

利用离线数据建模得到的模型模拟真实环境，实现一个model-based的强化学习模型，并结合了GAN结构进行学习。提出的模型在真实数据集中得到了验证。

## 5.3 问题定义

在该强化学习模型中，状态是用户在某时刻前最近点击过的一个物品列表：

<img src="D:\TyporaPicture\22.11.15\image-20221113154959474.png" alt="image-20221113154959474" style="zoom:80%;" />

智能体的动作是一个长度为k的推荐列表，智能体在某一时刻执行动作，我们能够得到一个推荐序列：

<img src="D:\TyporaPicture\22.11.15\image-20221113152926346.png" alt="image-20221113152926346" style="zoom:67%;" />

其中a^i^~t~是智能体在t时刻采取的动作，c^i^~t~是用户对于该推荐列表的点击动作，r^i^~t~是根据用户反馈得到的奖励值。

为了一定程度上简化模型，本文中做出一些设定：

1）对于那些推荐列表中不被点击的物品，将不会影响用户的下一个状态，即不考虑负反馈序列

2）奖励值只与被点击的物品有关

## 5.4 用户行为建模（model-based）

利用RNN提取序列的特征信息作为状态：

<img src="D:\TyporaPicture\22.11.15\image-20221113155449183.png" alt="image-20221113155449183" style="zoom:67%;" />

模型整体框架如下：

![image-20221113155614004](D:\TyporaPicture\22.11.15\image-20221113155614004.png)

得到状态表示后根据如下公式得到用户点击的概率分布：

![image-20221113155843964](D:\TyporaPicture\22.11.15\image-20221113155843964.png)

其中E^u^~t~是该推荐列表的embedding 矩阵

根据如下公式计算奖励值r：

<img src="D:\TyporaPicture\22.11.15\image-20221113160125149.png" alt="image-20221113160125149" style="zoom:67%;" />

看到奖励值的计算只与被点击的物品有关，其中e^u^~t~是根据数据集得到的在该时刻用户的点击的物品。

通过最大化如下函数更新参数得到用户的交互模型：

<img src="D:\TyporaPicture\22.11.15\image-20221113160337074.png" alt="image-20221113160337074" style="zoom:67%;" />

根据离线数据集得到用户模型后，在后续的训练过程中便可以得到用户对于某推荐动作的反馈以及奖励值。

## 5.5 策略函数

对于智能体采取动作的状态表示以及用户进行点击的状态表示可能不同，

在本文中智能体侧以及用户侧分别有一个状态表示，两者使用相同结构的RNN得到，但网络参数不同。

得到智能体侧的状态表示后根据如下softmax公式得到动作：

<img src="D:\TyporaPicture\22.11.15\image-20221113161733663.png" alt="image-20221113161733663" style="zoom:80%;" />

其中C是所有的候选物品集合

## 5.6 模型训练

本文中使用策略梯度下降中的REINFORCE算法进行训练，并且引入GAN结构，生成器能够得到一个推荐轨迹，在判别器中需要判断这个轨迹是生成器生成的还是数据集中真实的轨迹

通过最小化如下公式训练判别器D:

![image-20221113162559322](D:\TyporaPicture\22.11.15\image-20221113162559322.png)

由于D只能评估一整个序列的真假，无法评估其中的一段序列，所以这里引入如下公式对于使得能够评估一段序列：

![image-20221113163206597](D:\TyporaPicture\22.11.15\image-20221113163206597.png)

其中MC^U,A^(τ~0:t~;N)为：

![image-20221113163900667](D:\TyporaPicture\22.11.15\image-20221113163900667.png)

对于用户侧的参数更新的梯度为：

![image-20221113164110321](D:\TyporaPicture\22.11.15\image-20221113164110321.png)

对于智能体侧的参数更新的梯度为：

![image-20221113164229406](D:\TyporaPicture\22.11.15\image-20221113164229406.png)



个人感觉这样是要将判别器的输出融入进奖励值r的计算中，从而可以采用策略梯度下降的方式更新G

## 5.7 实验结果

![image-20221113191302689](D:\TyporaPicture\22.11.15\image-20221113191302689.png)

评测指标采用Precision@k（P@1和P@10）

对比的算法为：

LSTM：通过离线数据训练用户行为模型

LSTMD：通过GAN训练RecGAN中的用户行为模型

PG：通过离线数据使用策略梯度下降训练agent模型

PGIS：agent模型利用离线数据重要性采样来减小偏差

AC：一个LSTM模型利用actor-critic结构进行训练

PGU：agent模型利用离线数据以及生成数据进行训练，但是没有GAN结构

ACU：actor-critic模型通过离线以及生成数据进行训练，但没有GAN结构
