## 1. Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning     KDD 2018

该篇论文主要创新点：

1）在推荐系统中考虑了用户正反馈以及负反馈的信息，正反馈例如用户对于商品的点击与购买操作，负反馈例如用户对浏览到的商品不做任何操作而直接划走

2）提出了一个深度强化学习推荐模型DEERS，可以自动通过结合用户的正负反馈得到最优推荐策略

![img](D:\TyporaPicture\22.3.22\1647909279245-06f23684-b8bd-4a62-ac24-2c04d36fbdca.png)

在传统推荐系统中，不考虑用户skip item的行为，那么Agent状态并不会发生变化，则推荐系统推荐的物品不会发生较大变动。在提出的DEERS模型中考虑到了skip item的行为，状态发生变化，此时的推荐结果也会有较大变化。



在该推荐问题中，状态空间S定义为在时间t之前用户的浏览历史记录，动作空间A定义为在时间t系统向用户推荐的商品，奖励R为在状态s采取推荐动作a后得到的用户的反馈奖励，包括划过商品、点击商品、购买商品对应的奖励。



### The Basic DQN Model

![img](D:\TyporaPicture\22.3.22\1647910477128-d4fd8377-74f8-425f-b790-8a2718e53627.png)

Q网络：

![img](D:\TyporaPicture\22.3.22\1647910559455-a46fc649-099d-4656-b4c7-baa69a032a1f.png)



### The Proposed DEERS Framework

被用户跳过的商品也应该作为用户的偏好被利用起来，此时的状态中不应该只包含被用户点击或者购买的商品，也应该包含被跳过的商品。但如果只是在Bisic DQN的状态s中加入跳过的商品，由于跳过的商品数量远大于点击以及购买的数量，会将s中点击或购买的商品信息淹没，所以将正负反馈的商品分开表示。

此时的状态以及状态的转变如下：

![img](D:\TyporaPicture\22.3.22\1647911195347-97f384bf-e81f-4622-b96c-dd1215b58aa9.png)

Q网络：

![img](D:\TyporaPicture\22.3.22\1647911244515-6b115d26-7b8a-4f34-b803-749179fa84d2.png)

在 s+  与 s−  中的商品记录按照时间顺序排列，使用GRU模型得到序列特征。



###  The Pairwise Regularization Term  （两两正则化项）

![img](D:\TyporaPicture\22.3.22\1647912765620-2306157c-f51f-4d3d-a532-dd227fe0035f.png)

其中 a2  与 a5  都属于B类别，并且用户做出的行为不同，我们可以将 Q(s2, a2)与   Q(s5, a5)  的差值最大化。在time 2 时，将 a5  定义为a2  的竞争项(competitor item)。通过三个标准寻找target item的竞争项：

1）与竞争项属于同一商品类别

2）用户对于target item 和竞争项的反馈不同

3）竞争项与target item 的时间距离最近



此时我们在DQN loss函数中加入正则化项：

![img](D:\TyporaPicture\22.3.22\1647913328412-33544974-21bc-475c-9c9e-0408b056d0b1.png)

我们认为用户的偏好在短时间内稳定，所以可以在状态s下计算 a C  的Q值



总体训练过程如下：

![img](D:\TyporaPicture\22.3.22\1647914055418-2cba61f4-aed2-44cb-be49-3886aea5ba04.png)

### 实验

将positive state的长度与negative state的长度都设置为10，skipped item的奖励为0，clicked item的奖励为1，ordered item的奖励为5，实验结果如下：

![img](D:\TyporaPicture\22.3.22\1647914721997-5ef1b0c1-d194-4beb-baaa-9568a6f58e5b.png)

约经过500000sessions的训练后模型收敛，其中GRU为只考虑正反馈以及即时奖励的模型，DEERS-p为只考虑正反馈并平衡即时奖励以及未来奖励的模型，DEERS为综合考虑正反馈以及负反馈的模型，其效果最好。

（Q网络不需要输入用户特征吗）

## 2. DRN: A Deep Reinforcement Learning Framework for News Recommendation  WWW2018

本文提出了一种新的新闻推荐深度强化学习框架。由于新闻特性和用户偏好的动态性，新闻推荐是一个具有挑战性的话题。目前的在线推荐模型来解决新闻推荐的动态性存在以下问题：

1. 很少有研究考虑使用点击/不点击标签以外的用户反馈(例如，用户返回的频率)来帮助改进推荐。
2. 这些方法倾向于不断向用户推荐类似的新闻，这可能会让用户感到无聊。



创新点：

1. 进一步考虑用户活跃度作为点击/不点击标签的补充，以获取更多的用户反馈信息。
2. 加入了有效的探索策略，为用户寻找新的有吸引力的新闻。





深度强化学习top-K推荐模型框架如下：

![img](D:\TyporaPicture\22.3.22\1646041735676-41796f75-c77d-4360-a718-382f738655b6.png)

用户与新闻共同组成环境，推荐算法为智能体。当前状态定义为用户的特征向量，动作定义为推荐的新闻特征向量。当有用户请求推荐新闻时，用户的特征向量以及新闻候选池的新闻特征向量输入到Q网络中，通过Q网络的预测输出得到长度为N的推荐列表返回给用户，并且根据用户click/no click以及用户活跃的获得该推荐动作的奖励值。将状态与动作、奖励元组存入经验回放池中用于后续模型的训练。



### 1.模型框架



![img](D:\TyporaPicture\22.3.22\1646043005082-8e7d0429-8bb7-4ec0-b783-58bce9f5f3d7.png)

模型由离线部分和在线部分组成。

离线部分，模型采用了用户记录日志的新闻级别和用户级别的4类特征作为输入，计算DQN的reward。该网络使用离线用户-新闻单击日志进行训练。

在线学习部分，我们的推荐代理G将与用户进行交互，并按照以下四步更新网络:



PUSH：t1时刻用户u发出请求，agent利用u和新闻候选池作为输入，生成top k个推荐列表L

FEEDBACK：当用户有点击行为时就生成了反馈信息B

MINOR UPDATE（小调）：t1时刻后，agent利用该用户u，生成的L和反馈信息B，比较exploitation(开发)网络Q和exploration(探索)网络Q’，选择更好的那个

MAJOR UPDATE（大调）：在经过某一个固定的时期Tr后，agent利用反馈信息B和memory存储的用户活跃度采样来更新Q（memory中存储近期的历史点击记录和用户活跃度分值）

重复步骤1-4。



### 2.Q网络输入特征

Q网络的输入所用到的四类特征如下：

1.新闻特征：417维的one hot特征，包括标题，供应商，排名，实体名称，类别，主题类别，以及在过去1小时，6小时，24小时，1周，和1年内的点击数。这里每个点击数（click count）是1维，其余的特征总共412维，不知道分别是多少维的。

2.用户特征：413*5维： 在1 hour, 6 hours, 24 hours, 1 week, and 1 year里用户点击过的新闻的标题、提供者、排名、实体名称、类别、主题类别等（这些是412维） 再加上一个总的点击数，共413维。

3.用户-新闻特征：25维，在该用户的所有浏览记录中category, topic category and provider的出现频率，描述了用户和某条新闻之间的交互情况。

4.上下文特征：32维，描述了新闻请求发生时的上下文，包括时间、工作日和新闻的新鲜度(请求时间和新闻发布时间之间的差距)等。

[

](https://blog.csdn.net/weixin_45459911/article/details/105655488)

### 3.Q网络设计

将Q函数拆分成价值函数V(s)以及优势函数A(s,a)。价值函数由静态的用户特征和环境特征构成，优势函数由静态动态的全部特征构成，即，**V(s)仅由状态特征决定，A(s,a)由状态特征和动作特征共同决定。**

![img](D:\TyporaPicture\22.3.22\1646043448413-b50668cd-f483-48ed-9a32-a95284e5781d.png)



### 4.用户活跃度

一个用户的活跃度量化：

![img](D:\TyporaPicture\22.3.22\1646044090415-8d3f0c36-0a36-478b-b74c-6ccf60023e49.png)

用户活跃度初始值为0.5，在用户每次点击时，活跃度增加一个常量S，在不点击时指数衰减，最大不超过1

将用户活跃度的影响添加到reward中，用户点击新闻的奖励以及用户活跃度组成总奖励值：

![img](D:\TyporaPicture\22.3.22\1646044415103-dde215d5-2816-4afe-9075-7c7dbbaba24a.png)、

### 5.探索算法

在传统DQN算法中，使用 ϵ-greedy选择动作，这在短时间内会降低推荐系统的准确率，并且可能向用户推荐毫不相关的内容。

提出 Dueling Bandit Gradient Descentj 算法：

![img](D:\TyporaPicture\22.3.22\1646045007383-9a978321-64d5-4274-b52e-6463e1d20c9d.png)

应用此算法时，在向用户推荐新闻时，向当前的Q网络的参数W加入少量噪声 ∆W  变成探索网络。

![img](D:\TyporaPicture\22.3.22\1646045216298-7e31a3fe-1906-46eb-a874-88e0c2007f63.png)

通过两个网络的输出得到新闻推荐列表L以及L’，最终的推荐列表是随机的在L和L’中选择，比如第一条新闻在L中选择，那么根据L列表各个新闻的Q值选择一个新闻。之后将最终列表推荐给用户，用户会产生点击行为和反馈信息B，如果发现探索网络的推荐效果好则更新当前网络参数：

![img](D:\TyporaPicture\22.3.22\1646045566525-f5a4e45b-67d3-48af-89b1-272a0dff2bfb.png)

### 6.实验结果

![img](D:\TyporaPicture\22.3.22\1646045676031-35ffdba7-a286-4730-912d-0ce40e529aef.png)

其中DN为没有考虑未来奖励的使用dueling-structure结构的DDQN

U表示在奖励中加入用户活跃度的影响

EG表示使用 ϵ-greedy 策略

DBGD表示使用本文提出的 Dueling Bandit Gradient Descent





## 3. PRIORITIZED EXPERIENCE REPLAY    ICLR2016



### 1.Prioritized Replay DQN之前算法的问题

​        在引入Prioritized Replay DQN之前，已经有许多DQN模型以及他们的改进算法，比如Nature DQN，DDQN等，他们都是通过经验回放来采样，进而计算目标Q值。采取经验回放是因为深度神经网络作为有监督学习模型，要求样本数据满足独立同分布，但是DQN算法得到的样本是前后有关系的，经验回放机制可以通过存储-采样的方法打破样本之间的关联性。



​        之前的模型在采样的时候，在经验回放池里面的所有的样本都有相同的被采样到的概率。但是在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，对反向梯度的计算影响不大。

​       这样如果TD误差的绝对值|δ(t)|较大的样本更容易被采样，则我们的算法会比较容易收敛。



### 2.Prioritized Replay DQN算法的建模

​       Prioritized Replay DQN根据每个样本的TD误差绝对值|δ(t)|，给定该样本的优先级正比于|δ(t)|，将这个优先级的值存入经验回放池。之前的DQN算法，仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。

​       使用SumTree这样的二叉树结构来存储带有优先级的经验回放的样本数据：



![img](D:\TyporaPicture\22.3.22\1644890948608-01b46c9c-4ee8-49c2-8a36-302bb312bb34.png)

​       所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。

​       这样可以通过查看根节点中保存的优先级值判断从什么区间中均匀采样，比如上图根节点为42，就在[0,42]中均匀采样，采样到哪个区间，就是哪个样本，显然优先级大的叶子节点更容易被采集到。



​       除了经验回放值，现在Q网络的算法损失函数也有优化，考虑了样本优先级的损失函数为：

![img](D:\TyporaPicture\22.3.22\1644891594107-081117d1-0fa6-4657-9f51-eb6dc22ff7fa.png)

​       其中wj为第j个样本的优先级权重，由TD误差|δ(t)|归一化得到。



​       此外，在每次对Q网络的参数进行更新后，需要重新计算每个样本的TD误差，并将TD误差更新到SunTree上面。



### 3. Prioritized Replay DQN算法流程

![img](D:\TyporaPicture\22.3.22\1644891983934-6761665d-32dc-4def-af9a-0aa2152b8629.png)

​       Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，同时其可以直接集成DDQN算法。



### 4.实验结果

![img](D:\TyporaPicture\22.3.22\1647908346797-16c271ae-5748-4d12-867e-12e980cf9df6.png)