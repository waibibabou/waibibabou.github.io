1.Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems

这篇论文是推荐的环境认为不像gym环境一样，奖励r由状态与动作完全确定性地决定，相同用户在相同状态下对于同一个推荐物品的反馈也会不一样，可能收到一些系统中没有观察到的因素的影响，比如周围的环境。这导致得到的奖励r具有随机性。

为了处理这种奖励的随机性的影响，本文提出了一个Stochastic Reward Stabilization(SRS) framework，将agent得到的奖励值替换为这个SRS模块得到的奖励值。

![image-20231023090312664](D:\TyporaPicture\23.10.23 1v1\image-20231023090312664.png)

这个SRS部分是与模型无关的：

![image-20231023090304414](D:\TyporaPicture\23.10.23 1v1\image-20231023090304414.png)



2.LIRD

根据数据集得到很多的元组$(s,a)->r$

![image-20231023101418864](D:\TyporaPicture\23.10.23 1v1\image-20231023101418864.png)



通过计算当前agent的$(s,a)$对 与每个元组的相似度得到奖励r：

![image-20231023101536921](D:\TyporaPicture\23.10.23 1v1\image-20231023101536921.png)

![image-20231023101655722](D:\TyporaPicture\23.10.23 1v1\image-20231023101655722.png)







DEERS中是在测试阶段才有一个simulator，输入s、a对得到r，但也没有说这个simulator的训练过程

![image-20231023092611499](D:\TyporaPicture\23.10.23 1v1\image-20231023092611499.png)





1.Generative Adversarial User Model for Reinforcement Learning Based Recommendation System

训练一个GAN网络作为模拟环境，action是一个list的物品，生成器的输入是状态以及这个list，输出是分布，表示用户点击这个list上某个物品的概率，判别器的输入是状态以及用户点击的某个物品，输出是一个标量作为奖励r

![image-20231023130329738](D:\TyporaPicture\23.10.23 1v1\image-20231023130329738.png)







强化学习推荐系统的四大组成部分：

(1)State Representation

(2)Policy Optimization

(3)Reward Formulation：R1:the reward function is a simple,sparse numerical reward  R2:the reward is a function of one or several observations from the env

(4)Environment Building：offline，simulation，online

![image-20231023133913396](D:\TyporaPicture\23.10.23 1v1\image-20231023133913396.png)





2.Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation

![image-20231023142938376](D:\TyporaPicture\23.10.23 1v1\image-20231023142938376.png)



与上一篇的最大区别是，这里训练的GAN网络中，生成器相当于actor，其输出的是推荐结果，判别器的输出作为辅助的奖励

环境是通过极大似然估计的方式构建：

![image-20231023151229362](D:\TyporaPicture\23.10.23 1v1\image-20231023151229362.png)



![image-20231023151247190](D:\TyporaPicture\23.10.23 1v1\image-20231023151247190.png)



![image-20231023151333187](D:\TyporaPicture\23.10.23 1v1\image-20231023151333187.png)





3.Leveraging Long and Short-term Information in Content-aware Movie Recommendation

生成器作为actor使用，输出的是推荐的list，判别器用于衡量生成器的输出与真实值之间的差异

![image-20231023153917154](D:\TyporaPicture\23.10.23 1v1\image-20231023153917154.png)

把$logD(u_i,m_k,m_+|t)$看做奖励值，更新生成器的方式与Policy Gradient一致





目前训练时候遇到的奇怪的现象：

对于同一个用户在浏览过的物品不同的情况下，推荐的物品的类别很相似，比如用户浏览过的卡通的电影多，而推荐结果可能没有几个卡通的电影，可能是因为：在训练时候得到的r只与uid相关，如果数据集中某用户对于卡通电影评分都较高，agent学习到的结果是，对于该用户，在不同状态下推荐卡通电影的r都较高，所以后续测试时候受到uid影响较大，浏览序列影响较小。

让奖励由uid与浏览序列共同决定应该会好很多