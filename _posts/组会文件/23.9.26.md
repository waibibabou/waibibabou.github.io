# 1.Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems   KDD 2019

## 1.1 出发点

传统的基于监督学习的推荐系统往往不考虑长期收益，而是仅考虑短期的收益，例如此刻推荐什么物品点击率最高，或者推荐什么物品转化率最高。而强化学习在训练agent时候，优化的目标是最大化决策所能带来的长期奖励，比如在app上的停留时间、翻页的深度和连续两次使用的时间间隔，可以使得决策模型眼光更长远，长期来看可能带来更高的收益。

## 1.2 本文工作

1）本文提出了一个强化学习训练模型FeedRec，通过在奖励中加入长期回报的因素，使得该模型能够直接优化用户在短期以及长期的活跃度

2）使用DQN构建Q-Network学习值函数，并通过一个分层的LSTM网络实现状态的嵌入

3）提出了一个额外的S-Network用来模拟环境，用来缓解off-policy中Q网络训练时候的不稳定以及收敛困难的问题

4）先在一个人造的数据集中验证了S网络的有效性，并后续在一个真实的数据集中开展实验，结果表明该FeedRec超过了SOTA的算法

## 1.3 MDP建模

### 1.3.1 状态

状态空间是所有可行状态的集合，在t时刻的状态为$s_t=X_{t-1}$。其中$X_t$为：

<img src="D:\TyporaPicture\23.9.26\image-20230925162044759.png" alt="image-20230925162044759" style="zoom: 80%;" />

其中的每个元组$(i_t,f_t,d_t)$表示在t时刻推荐的物品$i_t$，用户对其的反馈$f_t$，包括点击、购买、略过、离开，产生的停留时间$d_t$

在开始$s_1={u}$，只包含该用户的个人信息，在t时刻$s_t=s_{t-1}\oplus\{(i_{t-1},f_{t-1},d_{t-1})\}$

### 1.3.2 动作

动作为每次agent推荐的一个物品

在t时刻的动作集合为$A(s_t)$：
$$
A(s_t)=A(s_{t-1})-\{a_t\}
$$
### 1.3.3 转移概率

转移概率$p(s_{t+1}|s_t,a_t)$表示在$s_t$状态下执行$a_t$动作后状态变为$s_{t+1}$的概率

### 1.3.4 奖励函数

由于该模型需要考虑用户的长期收益，所以奖励函数中包含两部分：即时奖励以及延时奖励

**即时奖励：**

在t时刻推荐的物品的反馈，如果用户点击或者购买则为1

![image-20230925164916741](D:\TyporaPicture\23.9.26\image-20230925164916741.png)

**延时奖励：**

如果用户没有退出系统，还在系统中浏览则为1

![image-20230925164946945](D:\TyporaPicture\23.9.26\image-20230925164946945.png)

用于衡量用户的使用频率：

![image-20230925165611472](D:\TyporaPicture\23.9.26\image-20230925165611472.png)

其中$v^r$是两次进入该系统的间隔，$\beta$为超参数



这三部分奖励加权得到最终奖励：

$\textbf{m}_t=[m_t^c,m_t^d,m_t^r]^\top$，$r_t=\omega^{\top}\textbf{m}_t$



## 1.4 模型总览

### 1.4.1 Q网络

<img src="D:\TyporaPicture\23.9.26\image-20230925170123457.png" alt="image-20230925170123457" style="zoom:80%;" />

输入的状态$s_t$中包括用户信息$u$，以及历史的推荐物品、反馈以及停留时间，首先将物品做嵌入，再与对应的反馈的投影矩阵相乘：$i_j^\prime=F_{f_j}i_j$

之后输入到LSTM中得到$h_{r,j}$：

<img src="D:\TyporaPicture\23.9.26\image-20230925171008882.png" alt="image-20230925171008882" style="zoom:80%;" />

对于连续值停留时间的处理是，利用其控制一个time gate：$g_j$

<img src="D:\TyporaPicture\23.9.26\image-20230925171219964.png" alt="image-20230925171219964" style="zoom:90%;" />

最后将$c_j$与$o_j$做哈达玛积得到$h_{r,j}$：

![image-20230925171401252](D:\TyporaPicture\23.9.26\image-20230925171401252.png)

在上面的LSTM中，所有类型的历史记录都被无差别地输入到网络中，但在实际情况中，用户的各个行为的数量是及不平衡的，点击的次数要远小于略过的次数，购买的次数更少，这样得到的状态将失去对用户稀疏行为的表现。

为了更好地表示用户状态，本文采取了一个分层的LSTM网络结构，对点击、略过、购买行为分别做特征提取：

![image-20230925171437129](D:\TyporaPicture\23.9.26\image-20230925171437129.png)

最终与用户的embedding共同组成状态：

$s_t=concat[h_{r,t},h_{s,t},h_{c,t},h_{o,t},u]$

最终将状态embedding以及某个物品embedding，通过一个MLP得到Q值。

<img src="D:\TyporaPicture\23.9.26\image-20230925185555243.png" alt="image-20230925185555243" style="zoom:90%;" />

### 1.4.2 S网络

<img src="D:\TyporaPicture\23.9.26\image-20230925185716307.png" alt="image-20230925185716307" style="zoom:80%;" />

为了提高离线训练的稳定性，本文引入了一个S-Network来模拟强化学习的环境。该网络会对Q网络得到的推荐物品输出用户的反馈$f_t$、停留时间$d_t$、下一次访问的间隔时间$v^r$，一个二元变量$l_t$表示用户是否离开平台。

网络的前半部分使用与Q网络相同的嵌入部分，但参数不同。

后续经过MLP后，根据不同的输出形式采取不同的激活函数得到输出：

<img src="D:\TyporaPicture\23.9.26\image-20230925190919346.png" alt="image-20230925190919346" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.9.26\image-20230925190925030.png" alt="image-20230925190925030" style="zoom:80%;" />

S-Network的训练：

该网络的训练利用监督学习的方式，其loss函数为：

<img src="D:\TyporaPicture\23.9.26\image-20230925192119958.png" alt="image-20230925192119958" style="zoom:80%;" />

其中的$\Psi$为交叉熵损失，$\lambda$为控制各个部分重要程度的超参

因为从Q网络得到的策略随着网络训练一直在改变，为了能够更好地辅助Q网络训练，S网络除了预训练，在后续的整体训练过程中也会通Q网络一同更新。



## 1.5 模型训练流程

<img src="D:\TyporaPicture\23.9.26\image-20230925192713552.png" alt="image-20230925192713552" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.9.26\image-20230925192730421.png" alt="image-20230925192730421" style="zoom:80%;" />



## 1.6 模型有效性证明

本文通过构造一个人工数据集来证明这个FeedRec模型对于不同的多样性建模都可以直接学习到最优策略：

有以下两种用户的活跃度与推荐列表多样性关系的建模方式：

**1）线性模型**

用户继续停留在平台的概率：

<img src="D:\TyporaPicture\23.9.26\image-20230925193853124.png" alt="image-20230925193853124" style="zoom:90%;" />

在这种假定中，推荐的物品的多样性越高，用户的活跃度越高，在这里推荐列表的多样性通过计算两两物品之间的熵得到。a、b用来将值限定在(0,1)

用户下次再访问平台的间隔时间：

![image-20230925194153307](D:\TyporaPicture\23.9.26\image-20230925194153307.png)

V、d用来将值限定为正数



**2）二次模型**

<img src="D:\TyporaPicture\23.9.26\image-20230925194303904.png" alt="image-20230925194303904" style="zoom:90%;" />

<img src="D:\TyporaPicture\23.9.26\image-20230925194350849.png" alt="image-20230925194350849" style="zoom:90%;" />

关于用户浏览深度的结果：

<img src="D:\TyporaPicture\23.9.26\image-20230925194505313.png" alt="image-20230925194505313" style="zoom:80%;" />

关于用户下一次访问间隔的结果：

<img src="D:\TyporaPicture\23.9.26\image-20230925194552311.png" alt="image-20230925194552311" style="zoom:80%;" />

观察到随着训练的进行，Q网络得到的推荐策略不管在线性模型还是二次模型中都在朝着用户浏览深度加大，访问时间间隔变小的方向训练，说明Q网络的训练有效

而其训练的分布与数据集中的假设基本相符，说明S网络的学习是有效的

## 1.7 实验结果

<img src="D:\TyporaPicture\23.9.26\image-20230925195005708.png" alt="image-20230925195005708" style="zoom:80%;" />

FeedRec(C)：在奖励函数中去掉click一项

FeedRec(D)：在奖励函数中去掉用于指示用户是否退出系统一项

FeedRec(R)：在奖励函数中去掉用户下次访问间隔一项



# 2.disentangling Long and Short-Term Interests for Recommendation   WWW  2022

## 2.1 出发点

用户的兴趣很难追踪，因为其往往既有稳定的长期兴趣，也有动态的短期兴趣。例如一个爱好技术的用户可能总是愿意浏览电子产品（长期兴趣）,但也可能在短期内表现出对书籍的兴趣（短期兴趣）。因此，准确建模和区分用户的长短期（LS-term）兴趣至关重要。

在之前的工作中，基于CF的推荐模型主要用于捕获长期兴趣而忽略行为序列的顺序特征，因此难以捕捉到用户的短期兴趣。因此，有人提出了序列推荐模型，利用CNN或者RNN来学习用户兴趣的序列特征。但这种方法往往具有短期记忆，会更倾向于与用户近期行为相关的物品。最近提出的将CF与序列推荐相结合的方法以覆盖长期和短期特征，但这些方法不能保证能够有效的对其学习到长短期兴趣，因为没有对学习到的特征施加明确的监督。在这些方法中学习到的长短期兴趣可能相互纠缠。

## 2.2 本文工作

1）文章着重解释了对于用户长期与短期分开建模的重要性

2）提出一个对比学习框架来分别捕捉长期和短期兴趣。通过与从原始交互序列构建的代理表示进行比较，通过自监督学习实现解耦表示

3）进一步设计了一个基于注意力的融合网络，能够自适应地聚合长期和短期兴趣以预测交互

4）在真实的数据集中进行了广泛实验，结果验证了CLSR对SOTA方法的显著改进

## 2.3 方法的整体理解

长期兴趣是相对稳定的，而短期兴趣是动态的、经常变化的，每次交互的结果由两种兴趣与目标物品共同决定。所以将用户兴趣建模构建为以下三个独立的机制：

<img src="D:\TyporaPicture\23.9.26\image-20230926111523573.png" alt="image-20230926111523573" style="zoom:90%;" />

对应表示用户的长期兴趣、短期兴趣和预测，其中$U$表示用户id以及历史交互记录信息

<img src="D:\TyporaPicture\23.9.26\image-20230926111626140.png" alt="image-20230926111626140" style="zoom: 67%;" />

1）长期兴趣可以从整个历史交互序列中推断出来

2）短期兴趣随时间而改变，因此在$f_2$中短期兴趣$U_s^{(t)}$是根据$U_s^{t-1}$、最后一个交互过的label $Y^{t-1}$与item $V^{t-1}$得到

3）交互预测：长短期兴趣的重要程度取决于多个方面，包括目标item $V_t$和交互历史 $x^u$，因此最终将长短期兴趣与$U$、$x^u$做融合得到最终的预测结果

## 2.4 模型实现

提出了一个对比学习框架，能够通过自监督来实现长短期兴趣的解耦。

<img src="D:\TyporaPicture\23.9.26\image-20230926112848832.png" alt="image-20230926112848832" style="zoom: 80%;" />

构建长期和短期兴趣的Query Vector：

<img src="D:\TyporaPicture\23.9.26\image-20230926113000763.png" alt="image-20230926113000763" style="zoom:80%;" />

长期兴趣的query是userid的embedding，短期兴趣是GRU在历史记录中提取的特征

使用用户的历史序列作为注意力编码器的key：

<img src="D:\TyporaPicture\23.9.26\image-20230926113946384.png" alt="image-20230926113946384" style="zoom:80%;" />

### 2.4.1 长期兴趣编码器

<img src="D:\TyporaPicture\23.9.26\image-20230926114121976.png" alt="image-20230926114121976" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.9.26\image-20230926114238365.png" alt="image-20230926114238365" style="zoom: 67%;" />

做一个self-attention得到每个item对应权重后加权求和得到长期兴趣：

<img src="D:\TyporaPicture\23.9.26\image-20230926114341194.png" alt="image-20230926114341194" style="zoom:80%;" />

用户的历史序列同样作为注意力编码器的value

### 2.4.2 短期兴趣编码器

<img src="D:\TyporaPicture\23.9.26\image-20230926114129911.png" alt="image-20230926114129911" style="zoom:80%;" />

在RNN之上使用了另一个注意力网络，将历史item的embedding输入到RNN中，并使用其在每个时间片的输出作为key

<img src="D:\TyporaPicture\23.9.26\image-20230926114708276.png" alt="image-20230926114708276" style="zoom:80%;" />

利用$q_s^{u,t}$作为query向量，得到每部分的权重$b_k$后加权得到短期兴趣向量：

<img src="D:\TyporaPicture\23.9.26\image-20230926115056696.png" alt="image-20230926115056696" style="zoom:80%;" />

尽管使用了两个单独的编码器，但$u_l^t$和$u_s^t$都是以无监督方式提取的，没有标记数据来监督学习，不能保证长短期兴趣的完全解耦。因此，利用对比学习，通过自监督方式实现解耦。

### 2.4.3 长短期兴趣的自监督解耦

长期兴趣提供了用户偏好的整体视图，总结了整个历史交互，而短期兴趣随时间动态演变，反映了最近的交互记录。所以考虑从交互序列本身获取两个兴趣的代理来监督两个编码器。

<img src="D:\TyporaPicture\23.9.26\image-20230926120127910.png" alt="image-20230926120127910" style="zoom:80%;" />

$E(x)$表示物品x的embedding

在编码器的输出与代理之间进行对比学习，要求编码器学习到的长短期兴趣与对应的代理更相似，而与相反的代理更不相似

<img src="D:\TyporaPicture\23.9.26\image-20230926120454833.png" alt="image-20230926120454833" style="zoom:67%;" />

有以下四个对比任务：

<img src="D:\TyporaPicture\23.9.26\image-20230926120706353.png" alt="image-20230926120706353" style="zoom: 67%;" />

通过四个关于编码器输出和代理之间相似度的对称对比任务，在长短期兴趣建模上添加了自监督，可以实现更强的解耦。

损失函数采用BPR或者triplet loss：

<img src="D:\TyporaPicture\23.9.26\image-20230926121143771.png" alt="image-20230926121143771" style="zoom: 80%;" />

其中$<\cdot,\cdot>$表示向量内积，函数d表示欧几里得距离，m为控制正负样本之间距离差的最小值的超参数。a是锚点，p是正样本，q是负样本，使得a与正样本p更相似

最终的对比loss函数为：

<img src="D:\TyporaPicture\23.9.26\image-20230926121556588.png" alt="image-20230926121556588" style="zoom:80%;" />

f可以是两种loss之一

### 2.4.4 自适应融合

长期与短期兴趣的重要程度由历史交互序列与目标item共同决定：

<img src="D:\TyporaPicture\23.9.26\image-20230926121711436.png" alt="image-20230926121711436" style="zoom:80%;" />



<img src="D:\TyporaPicture\23.9.26\image-20230926121738142.png" alt="image-20230926121738142" style="zoom:80%;" />



## 2.5 最后的预测过程

在得到$u_t$后与目标item的embedding通过MLP得到预测结果：

![image-20230926122121581](D:\TyporaPicture\23.9.26\image-20230926122121581.png)

loss函数采用负对数似然函数：

![image-20230926122221931](D:\TyporaPicture\23.9.26\image-20230926122221931.png)

与上面对比学习的loss相结合得到模型整体的loss：

![image-20230926122304246](D:\TyporaPicture\23.9.26\image-20230926122304246.png)

## 2.6 实验

![image-20230926122402532](D:\TyporaPicture\23.9.26\image-20230926122402532.png)

观察到：

1）短期模型通常比长期模型表现更好

2）长短期兴趣的联合建模并不总能带来性能提升，SLi-Rec提取出的长短期兴趣相互纠缠，增加了模型冗余并导致性能降低

3）长短期兴趣的解耦建模可以实现显著的改进

















第一个点：

对用户的长期兴趣建模，添加到状态s中，状态的嵌入表示采用rnn+attention的方式



第二个点：

利用GAN pre-train一个强化学习的环境，能够对agent训练过程中的动作返回反馈信息与奖励值，以提高训练过程的稳定性，后续在agent训练时，同步训练该环境模型



进展：

目前完成了movielens数据集中的基础ddpg+per的训练

<img src="D:\TyporaPicture\23.9.26\user_ndcg.png" alt="user_ndcg" style="zoom:67%;" />

<img src="D:\TyporaPicture\23.9.26\user_recall.png" alt="user_recall" style="zoom:67%;" />

计划：

十月完成将GAN的预训练模型加入ddpg训练环境中，并在状态嵌入中添加长期兴趣

十一月边改实验边写论文



