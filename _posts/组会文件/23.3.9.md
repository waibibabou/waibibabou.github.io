# 1 End-to-End Deep Reinforcement Learning based Recommendation with Supervised Embedding   WSDM 2020

## 1.1 出发点

对于强化学习中的user以及item的embedding，如果同时训练embedding以及状态表示、策略网络会导致训练的不稳定，结果较差。对于该问题，大多数工作的做法是将embedding预训练后固定不变(pre-trained and fixed)，然而这有个缺点：

其不能够在动态环境中很好地体现用户的特征以及物品之间的关系。一方面，如果用户与物品频繁交互，用户的偏好会快速变化，此时用户的特征向量也应该随之改变。另一方面，随着与用户的交互，物品之间的关系也在变化，所以pre-trained and fixed的设置不能够适用于动态环境

## 1.2 本文工作

1）提出了End-to-End Deep Reinforcement learning based Recommendation framework（EDRR），其中引入了一个监督信号来更新embedding向量

2）设计和比较了三种将监督信号引入RL的策略，实验结果表明EDRR-V3的训练过程最稳定

3）在三个真实的数据集中开展了实验，结果表明在policy-based与value-based的模型中，EDRR-V3实现了端到端的训练目的，并且实验结果更好

## 1.3 MDP建模

状态空间S：一个状态s包含某用户最近交互过的评分较高的物品list以及该用户的特征

动作空间A：一个动作代表一个推荐的物品，本文假定动作是一个物品而不是物品列表

奖励函数R：在某状态s下采取动作a后，用户会对推荐结果做出反馈作为奖励值r

## 1.4 统一强化学习推荐系统框架

<img src="D:\TyporaPicture\23.3.9\image-20230308231339236.png" alt="image-20230308231339236" style="zoom:67%;" />

虽然不同的RL推荐模型采用了不同的方法，但是它们的结构可以总结成如图

其中包括三部分：embedding component(EC), state representation component(SRC), policy component(PC)

**EC:**该部分将用户的特征以及浏览的物品做embedding，得到低维向量

![image-20230308231929271](D:\TyporaPicture\23.3.9\image-20230308231929271.png)

Ht为t时刻前的浏览记录

**SRC:**该部分根据EC输出的embedding向量得到t时刻的状态表示st，实现方式上可以有MLP、RNN，本文的实现中结合了attention机制

**PC:**该部分根据状态st得到动作a，对于value-based的模型，首先计算所有Q值后选择Q最大的a执行，对于policy-based的模型，直接输出动作。之后根据反馈得到奖励值，使用TD误差更新网络参数：

<img src="D:\TyporaPicture\23.3.9\image-20230308232456913.png" alt="image-20230308232456913" style="zoom: 80%;" />

其中：

<img src="D:\TyporaPicture\23.3.9\image-20230308232509829.png" alt="image-20230308232509829" style="zoom: 80%;" />

## 1.5 EC训练策略

我们这里比较四种对于EC（即embedding部分）的训练策略，分别如下：

1）RU：随机初始化（Random）embedding后与RL其他模块一同训练更新（Update）

2）RF：随机初始化（Random）embedding后，在训练RL模型时，embedding保持不变（Fixed）

3）PU：Pre-train embedding后，在后续与其他模块一同训练更新

4）PF：Pre-train embedding后，后续训练模型时不变(Fixed)

<img src="D:\TyporaPicture\23.3.9\image-20230309001542597.png" alt="image-20230309001542597" style="zoom:80%;" />

实验结果中，RF与PF分别超过了RU与PU，并且PF的效果最好，这是因为RF与PF在训练时保持EC不变，这缓解了RL中训练时的不稳定性。

虽然大多数工作用PF的方式，但其不能在动态环境中体现出用户的偏好变化以及物品间的关联。本文提出端到端的模型，在训练时embedding同样变化，达到了较好的实验效果

## 1.6 模型定义

### 1.6.1 监督信号（SL）定义

在该强化学习模型中，除了用户对于结果的反馈，不存在其他任何监督信息。我们考虑在EDRR中引入Supervised Learning Component（SLC），其为一个二分类器，来预测用户对于推荐结果的反馈，loss函数采用交叉熵loss：

<img src="D:\TyporaPicture\23.3.9\image-20230309002811930.png" alt="image-20230309002811930" style="zoom:80%;" />

其中y~i~是用户对于该推荐物品的真实反馈，y~i~'为模型的预测结果

### 1.6.2 结合监督信号

**1）EDRR-V1** 

<img src="D:\TyporaPicture\23.3.9\image-20230309003252961.png" alt="image-20230309003252961" style="zoom:67%;" />

SLC的输入是状态s与推荐物品的embedding，SRC根据RL signal更新，EC根据SL signal更新

**2）EDRR-V2**

<img src="D:\TyporaPicture\23.3.9\image-20230309003310237.png" alt="image-20230309003310237" style="zoom:67%;" />

与V1不同的是SRC根据RL与SL更新

**3）EDRR-V3**

<img src="D:\TyporaPicture\23.3.9\image-20230309003329736.png" alt="image-20230309003329736" style="zoom:67%;" />

V3中有两个相同结构但参数不同的SRC，左侧的根据RL signal更新，右侧的根据SL signal更新

在MovieLens（1M）中对比三个版本模型的稳定性，稳定性定义如下：

<img src="D:\TyporaPicture\23.3.9\image-20230309003924321.png" alt="image-20230309003924321" style="zoom:80%;" />

为两个相邻状态的向量对应位置值之差，差越大说明状态的波动越大，越不稳定

结果如下：

<img src="D:\TyporaPicture\23.3.9\image-20230309004114584.png" alt="image-20230309004114584" style="zoom: 80%;" />

发现V3的训练过程比V1与V2更加稳定，后续采用V3模型

### 1.6.3 EDRR结构

1）EC：embedding部分的输出会送入两个SRC模块，在EC的输入中，可以考虑交互过的物品以及对应的反馈信息，以及用户的自身特征

2）SRC：

<img src="D:\TyporaPicture\23.3.9\image-20230309004553923.png" alt="image-20230309004553923"  />

该部分的输出为状态表示s：

<img src="D:\TyporaPicture\23.3.9\image-20230309004702634.png" alt="image-20230309004702634" style="zoom: 80%;" />

函数g代表带有权重的平均池化层，引入attention机制：

<img src="D:\TyporaPicture\23.3.9\image-20230309004851102.png" alt="image-20230309004851102" style="zoom: 80%;" />



3）PC：这部分可以使用任何value-based以及policy-based 算法实现，例如DQN，则梯度如下：

<img src="D:\TyporaPicture\23.3.9\image-20230309005330485.png" alt="image-20230309005330485" style="zoom: 80%;" />

如果为DDPG则梯度如下：

<img src="D:\TyporaPicture\23.3.9\image-20230309005346153.png" alt="image-20230309005346153" style="zoom:80%;" />

4）SLC：该部分为MLP，输入是状态s以及推荐物品的embedding，输出为用户对该物品做出正面反馈的概率，损失函数为交叉熵损失

## 1.7 实验

数据集采用MovieLens(100K)，MovieLens(1M)，Jester三个数据集，对于所有RL的baselines采用PMF的方式pre-train用户与物品的embeddings

![image-20230309005939737](D:\TyporaPicture\23.3.9\image-20230309005939737.png)

其中，DRR是一个基于actor-critic的模型，DRR-att是其在状态表示部分（即SRC）采用attention机制的模型。

DRR-att(EDRR)与DQN-att(EDRR)都是使用了本文提出的训练结构的模型，取得了最好的实验效果。



个人理解：本文虽然在强化学习中引入了监督信号，但是该监督信号只用于更新一侧的SRC以及embedding的参数，不会影响到策略函数的训练，没有直接监督策略函数的输出结果



# 2 Pseudo Dyna-Q-A Reinforcement Learning Framework for Interactive  WSDM 2020

## 2.1 出发点

将强化模型的agent未经过训练直接在线推荐会极大影响到用户的体验，本文根据离线数据构建了一个用户模拟器，使得agent可以通过与用户模拟器交互进行训练，通过这样的方式，能够提供无限制的模拟经验来训练策略函数。然而先前的模拟器，都是在策略函数训练前提前建立，并且后续不会变化，我们认为用户模拟器应该与目标策略保持同步更新，来得到最为准确的模拟结果。

## 2.2 本文工作

1）本文提出Pseudo Dyna-Q(PDQ)，提供了一个通用的框架，能够应用于不同结构的神经网络

2）对于模拟器（即world model）提出了一个loss函数，使得在训练策略函数时也同样能更新模拟器参数

3）介绍了一个简单的PDQ实例，并且在两个真实数据集中验证了模型的有效性

## 2.3 模型结构

在该模型中包含两部分：

world model（即模拟器），它需要根据数据集数据训练，使得其能够近似一个真实用户的反馈结果。

recommendation policy（即策略函数）,该策略决定着在状态s下应该采取的推荐动作，其需要通过训练来最大化累计奖励值。

### 2.3.1 world model

由于该部分需要模拟用户反馈，而奖励值与反馈相关，所以world model可以通过预测奖励与实际奖励进行训练：

<img src="D:\TyporaPicture\23.3.9\image-20230309073730487.png" alt="image-20230309073730487" style="zoom:80%;" />

其中η(π）是真实数据中的折扣奖励，η(π; θ~M~)是在world model中预测的奖励。

<img src="D:\TyporaPicture\23.3.9\image-20230309074120548.png" style="zoom:80%;" />

来衡量真实奖励与预测奖励的差距

### 2.3.2 策略函数

使用DQN网络作为策略函数，使用ϵ-greedy选择动作，利用TD误差更新网络参数：

<img src="D:\TyporaPicture\23.3.9\image-20230309074643194.png" alt="image-20230309074643194" style="zoom:80%;" />

对于网络参数梯度：

![image-20230309074655794](D:\TyporaPicture\23.3.9\image-20230309074655794.png)

整个算法的训练流程如下：

<img src="D:\TyporaPicture\23.3.9\image-20230309074806318.png" alt="image-20230309074806318" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.3.9\image-20230309074817649.png" alt="image-20230309074817649" style="zoom:80%;" />

注意到DQN在选择动作后没有推荐给真实用户，而是与建立的world model交互，得到奖励值与下一个状态s~t+1~

并且在训练DQN网络时，同时利用了数据集中的交互记录以及与world model，之前都是只利用与模拟器的交互记录，在这里其利用数据集中的交互记录，有点像是将数据集作为智能体了，而一般都是将数据集作为环境使用，不过这里由于在建立模拟器时没有充分利用数据集中的数据，在训练时需要加以使用。

在训练模拟器时，只使用数据集中的数据

## 2.4 一个应用实例

### 2.4.1 状态表示

<img src="D:\TyporaPicture\23.3.9\image-20230309080254467.png" alt="image-20230309080254467" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.3.9\image-20230309080441729.png" alt="image-20230309080441729" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.3.9\image-20230309080921362.png" alt="image-20230309080921362" style="zoom:80%;" />

在状态的表示中包括浏览记录以及对于这些物品的反馈结果

后续结合attention机制，赋予每个浏览记录一个权重

<img src="D:\TyporaPicture\23.3.9\image-20230309081300585.png" alt="image-20230309081300585" style="zoom:67%;" />

其中i~t~是将要在t时刻推荐的物品，由于利用了DQN模型，这里需要对于所有候选物品进行计算

状态s~t~：

<img src="D:\TyporaPicture\23.3.9\image-20230309081407130.png" alt="image-20230309081407130" style="zoom: 50%;" />

其中结合了用户的自身特征向量u~t~

后续的输出为：

<img src="D:\TyporaPicture\23.3.9\image-20230309081454471.png" alt="image-20230309081454471" style="zoom: 67%;" />





### 2.4.2 Q网络

<img src="D:\TyporaPicture\23.3.9\image-20230309080323203.png" alt="image-20230309080323203" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.3.9\image-20230309081538624.png" alt="image-20230309081538624" style="zoom: 67%;" />

经过一层全连接层后得到在状态s~t~下动作i~t~的Q值

后续Q网络利用TD误差更新



### 2.4.3 world model

<img src="D:\TyporaPicture\23.3.9\image-20230309080338478.png" alt="image-20230309080338478" style="zoom:80%;" />

world model的输入是状态s~t~以及动作a~t~，输出为反馈结果以及交互是否到达终态信号

<img src="D:\TyporaPicture\23.3.9\image-20230309081948030.png" alt="image-20230309081948030" style="zoom: 67%;" />

loss函数采用交叉熵损失，用于衡量预测反馈结果与真实反馈结果的差距：

![image-20230309082654135](D:\TyporaPicture\23.3.9\image-20230309082654135.png)

## 2.5 实验

实验使用两个数据集：Taobao、Retailrocket，都是基于session的数据集

![image-20230309083036789](D:\TyporaPicture\23.3.9\image-20230309083036789.png)

PDQ(N)、PDQ(IM)与本文提出的最终模型采用了不同的状态表示方式。

评价标准包括每条轨迹的平均点击次数(Clicks)、推荐结果的平均多样性(Diversity)、平均交易次数(Horizons)

结果发现PDQ(IM+R)效果最好

训练图像：

<img src="D:\TyporaPicture\23.3.9\image-20230309083841152.png" alt="image-20230309083841152" style="zoom: 67%;" />

<img src="D:\TyporaPicture\23.3.9\image-20230309083856409.png" alt="image-20230309083856409" style="zoom:67%;" />





simulator与model-based 还不太一样，像LIRD中根据数据集建立的模拟器，只是存储了一些s、a对，在训练过程中找到最相近的进行配对，得到奖励值，而并没有建立一个(s,a)->r的映射函数，不能叫做model-based方法。



为什么model-based的模型少？我觉得还是建模的过程太过于复杂，很难找到合适的奖励函数与状态转移矩阵，并且训练量较大，需要额外训练一个奖励映射网络。



![image-20230309085102729](D:\TyporaPicture\23.3.9\image-20230309085102729.png)

问：KERL那篇论文中，将策略函数输出的t+1到t+k的序列与真实序列相比较得到r，那都有真实序列了直接做成监督模型，算loss不就行了吗？

答：我觉得算loss与作为r还是有很大不同的，作为监督模型算loss意思就是在这个s就应该输出这个a，但是算r，在这个s下并不是要输出r最大的a，强化学习模型需要长远考虑，考虑后面的q值

我觉得获取r的过程是可以类似监督型获得的，但是从s获得a的过程不能有监督信号，不是会执行r最高的动作而是q最高的，这是强化学习的优势，会考虑以后的回报
ddpg的两篇和知识图谱那个本质都是与数据集中的一些已有记录比较得到r，这并不是监督学习，没有说利用s对应的标签输出进行训练，一个动作的r高也不是一定在策略函数中输出该动作
