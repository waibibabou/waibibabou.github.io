##  DRN: A Deep Reinforcement Learning Framework for News Recommendation   WWW2018



本文提出了一种新的新闻推荐深度强化学习框架。由于新闻特性和用户偏好的动态性，新闻推荐是一个具有挑战性的话题。目前的在线推荐模型来解决新闻推荐的动态性存在以下问题：

1. 很少有研究考虑使用点击/不点击标签以外的用户反馈(例如，用户返回的频率)来帮助改进推荐。
2. 这些方法倾向于不断向用户推荐类似的新闻，这可能会让用户感到无聊。



创新点：

1. 进一步考虑用户活跃度作为点击/不点击标签的补充，以获取更多的用户反馈信息。
2. 加入了有效的探索策略，为用户寻找新的有吸引力的新闻。





深度强化学习top-K推荐模型框架如下：

![img](D:\TyporaPicture\22.2.28\1646041735676-41796f75-c77d-4360-a718-382f738655b6.png)

用户与新闻共同组成环境，推荐算法为智能体。当前状态定义为用户的特征向量，动作定义为推荐的新闻特征向量。当有用户请求推荐新闻时，用户的特征向量以及新闻候选池的新闻特征向量输入到Q网络中，通过Q网络的预测输出得到长度为N的推荐列表返回给用户，并且根据用户click/no click以及用户活跃的获得该推荐动作的奖励值。将状态与动作、奖励元组存入经验回放池中用于后续模型的训练。



## 1.模型框架



![img](D:\TyporaPicture\22.2.28\1646043005082-8e7d0429-8bb7-4ec0-b783-58bce9f5f3d7.png)

模型由离线部分和在线部分组成。

离线部分，模型采用了用户记录日志的新闻级别和用户级别的4类特征作为输入，计算DQN的reward。该网络使用离线用户-新闻单击日志进行训练。

在线学习部分，我们的推荐代理G将与用户进行交互，并按照以下四步更新网络:



PUSH：t1时刻用户u发出请求，agent利用u和新闻候选池作为输入，生成top k个推荐列表L

FEEDBACK：当用户有点击行为时就生成了反馈信息B

MINOR UPDATE（小调）：t1时刻后，agent利用该用户u，生成的L和反馈信息B，比较exploitation(开发)网络Q和exploration(探索)网络Q’，选择更好的那个

MAJOR UPDATE（大调）：在经过某一个固定的时期Tr后，agent利用反馈信息B和memory存储的用户活跃度采样来更新Q（memory中存储近期的历史点击记录和用户活跃度分值）

重复步骤1-4。



## 2.输入特征

Q网络的输入所用到的四类特征如下：

1.新闻特征：417维的one hot特征，包括标题，供应商，排名，实体名称，类别，主题类别，以及在过去1小时，6小时，24小时，1周，和1年内的点击数。这里每个点击数（click count）是1维，其余的特征总共412维，不知道分别是多少维的。

2.用户特征：413*5维： 在1 hour, 6 hours, 24 hours, 1 week, and 1 year里用户点击过的新闻的标题、提供者、排名、实体名称、类别、主题类别等（这些是412维） 再加上一个总的点击数，共413维。

3.用户-新闻特征：25维，在该用户的所有浏览记录中category, topic category and provider的出现频率，描述了用户和某条新闻之间的交互情况。

4.上下文特征：32维，描述了新闻请求发生时的上下文，包括时间、工作日和新闻的新鲜度(请求时间和新闻发布时间之间的差距)等。

[

](https://blog.csdn.net/weixin_45459911/article/details/105655488)

## 3.Q网络设计

将Q函数拆分成价值函数V(s)以及优势函数A(s,a)。价值函数由静态的用户特征和环境特征构成，优势函数由静态动态的全部特征构成，即，**V(s)仅由状态特征决定，A(s,a)由状态特征和动作特征共同决定。**

![img](D:\TyporaPicture\22.2.28\1646043448413-b50668cd-f483-48ed-9a32-a95284e5781d.png)



## 4.用户活跃度

一个用户的活跃度量化：

![img](D:\TyporaPicture\22.2.28\1646044090415-8d3f0c36-0a36-478b-b74c-6ccf60023e49.png)

用户活跃度初始值为0.5，在用户每次点击时，活跃度增加一个常量S，在不点击时指数衰减，最大不超过1

将用户活跃度的影响添加到reward中，用户点击新闻的奖励以及用户活跃度组成总奖励值：

![img](D:\TyporaPicture\22.2.28\1646044415103-dde215d5-2816-4afe-9075-7c7dbbaba24a.png)、

## 5.探索算法

在传统DQN算法中，使用 ϵ-greedy选择动作，这在短时间内会降低推荐系统的准确率，并且可能向用户推荐毫不相关的内容。

提出 Dueling Bandit Gradient Descentj 算法：

![img](D:\TyporaPicture\22.2.28\1646045007383-9a978321-64d5-4274-b52e-6463e1d20c9d.png)

应用此算法时，在向用户推荐新闻时，向当前的Q网络的参数W加入少量噪声 ∆W  变成探索网络。

![img](D:\TyporaPicture\22.2.28\1646045216298-7e31a3fe-1906-46eb-a874-88e0c2007f63.png)

通过两个网络的输出得到新闻推荐列表L以及L’，最终的推荐列表是随机的在L和L’中选择，比如第一条新闻在L中选择，那么根据L列表各个新闻的Q值选择一个新闻。之后将最终列表推荐给用户，用户会产生点击行为和反馈信息B，如果发现探索网络的推荐效果好则更新当前网络参数：

![img](D:\TyporaPicture\22.2.28\1646045566525-f5a4e45b-67d3-48af-89b1-272a0dff2bfb.png)

## 6.实验结果

![img](D:\TyporaPicture\22.2.28\1646045676031-35ffdba7-a286-4730-912d-0ce40e529aef.png)

其中DN为没有考虑未来奖励的使用dueling-structure结构的DDQN

U表示在奖励中加入用户活跃度的影响

EG表示使用 ϵ-greedy 策略

DBGD表示使用本文提出的 Dueling Bandit Gradient Descent