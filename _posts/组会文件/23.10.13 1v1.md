LIRD：

有着相似兴趣的用户在同样的物品采取的动作也是相似的，所以可以通过推荐结果去模拟结果中寻找相似的状态以及动作，便可根据模拟结果产生奖励值。

![img](D:\TyporaPicture\汇报\1653366991483-e52183e3-90c5-4e39-bd2f-d11750b4b3cb.png)

有了模拟结果M后，在后续训练过程中，对于当前的状态和动作元组 pt (st , at )  ，可以通过计算其与M中元组的相似度：

![img](D:\TyporaPicture\汇报\1653367756737-e6f4be56-6748-4dd8-b806-09d76db52d88.png)

于是我们便可以得到当前状态和动作所得到的奖励值r的概率如下：

![img](D:\TyporaPicture\汇报\1653367855466-c816b1a8-b717-48dd-9adf-fe2f2ffd09ba.png)

解决了在数据集中，交互记录占比总物品数过少导致的推荐结果的奖励值不好计算的问题。



强化学习推荐模型四部分：

State Representation、Policy Optimization、Reward Formulation、Environment Building



之前的一些想法：

1.（1）GAN的生成器视为actor，判别器作为critic

（2）利用GAN对用户行为建模，作为simulator参与后续agent训练

2.可信用户引导强化学习

差小的时候需要反转梯度使其变大，差大时候不反传梯度使其不变

3.将以下两篇文章结合起来，其思想都是利用奖励值的差异进行训练，不同的是，一个利用差异监督embedding的学习，另一个是用来训练simulator

23.3.9.md



4.利用进化算法优化强化学习：

设立多个actor，加强探索力度



5.能否套在reward shaping下面
