# 1 Continuous Control With Deep Reinforcement Learning（DDPG）    ICLR   2016

## 1.1 前言

![img](D:\TyporaPicture\22.6.1\1654055489215-f64dabeb-be7b-4b57-b451-b51bbe741bac.jpeg)

在强化学习领域，根据智能体是否能对下一步的状态和回报做出预测，将强化学习算法分为Model-based以及Model-free。在Model-free的算法中，又可以根据网络模型的输出是Q预测值还是直接的动作概率分布，分为Value-based与Policy-based，本文所要介绍的DDPG算法属于Model-free、Policy-based算法。



Value-based算法的缺陷：

（1）Value-based算法虽然可以解决连续状态空间的问题，但是其动作空间仍然离散，无法解决动作空间过大或者连续的强化学习问题。

（2）对受限状态下问题处理能力不足，如果只考虑使用贪婪法则，那么只要是网络输入的状态相同，则采取的动作也一定相同。然而在很多问题中，在使用特征描述状态空间的某个状态时，可能因为观测的限制，导致在真实环境下原本不同的两个状态有着相同的特征描述，进而导致Value-based方法无法得到最优解。

![img](D:\TyporaPicture\22.6.1\1654056254618-1cbe92c1-a28e-4f55-83b4-15d996d49a31.png)

（3）无法满足随机策略的需求，有时需要策略具有随机性，而Value-based算法的策略是确定的。

## 1.2 DQN的问题

DQN是通过更新网络参数实现对于动作Q值的更新，其训练的结构如下：

![img](D:\TyporaPicture\22.6.1\1653360140858-d4088ab9-733a-4c3b-9b92-6a146845a38a.png)

在其中，因为在计算目标Q值时需要maxQ'(s,a)找到最大的Q值，所以动作空间不能连续。

为能够处理连续动作空间，在DDPG算法中，使用额外的一个Actor网络，代替maxQ'(s,a)的功能，这个网络的输入为状态s，输出Q值最大的动作，而Critic网络取代DQN中的Q网络。

## 1.3 DDPG算法

![img](D:\TyporaPicture\22.6.1\1653361020411-acef1969-d1a1-41ad-9c9f-317c737b1dd2.png)

**Actor网络：**

该网络的作用是，根据网络输入的状态，输出一个特定的动作A，使得这个动作在输入到Critic网络中获得最大的Q值。

**Critic网络：**

该网络的输入是状态以及Actor网络输出的动作，输出为预测的Q值，作用与DQN中的Q网络类似。

考虑到DDPG与DQN一样，在更新网络参数时，目标Q值在不断移动，造成更新困难，所以也使用了DQN中建立目标网络的方法，在更新参数时先固定住目标网络的参数，在一定次数的更新后将两个现实网络的参数复制到目标网络中。

此时，一共有四个网络：Actor、Critic、Actor_target、Critic_target

![img](D:\TyporaPicture\22.6.1\1653362244050-643dbac5-344a-4964-bb4b-9003741391c9.png)

DDPG算法总体流程如下：

![img](D:\TyporaPicture\22.6.1\1653362312703-e26ad2a7-1cf9-436d-b464-5535cb00db90.png)

由于在训练过程中，智能体需要充分探索环境以找到更优策略，在DQN中，使用ϵ-greedy法则实现探索，而在DDPG中，Actor网络直接输出动作而非Q值，不能使用ϵ-greedy法则，此时需要在Actor网络的输出中添加噪声实现探索。

![img](D:\TyporaPicture\22.6.1\1653362828380-2772037b-ed5b-4f13-b857-50ec09d3369c.png)

此时使用Actor网络输出的动作a作为一个正态分布的平均值，加上标准差作为超参数，构造一个正态分布，然后从该正态分布中随机选取一个新的动作代替a，此时原动作a也是一个概率最大的值，类似ϵ-greedy。

关于交互过程中产生样本的存储与采样过程与DQN相同，不过多叙述。

**Critic网络参数更新：**

![img](D:\TyporaPicture\22.6.1\1653363383849-6f03a234-8866-4bb5-a6b2-a36e3034896b.png)

对于Critic网络的更新与DQN中Q网络更新类似，首先需要计算样本的目标Q值，利用Actor_target以及Critic_target网络进行计算，loss函数设定为均方误差函数使用梯度下降进行更新。

**Actor网络参数更新：**

该网络参数是沿着Q值梯度上升的方向：

![img](D:\TyporaPicture\22.6.1\1653364666867-48b29ace-04f3-43fa-816f-98347f1cb948.jpeg)

经过以上推导得到如下公式：

![img](D:\TyporaPicture\22.6.1\1653363391889-69d73165-86a7-4d70-8be5-d9165ed797ca.png)

得到梯度后便可更新网络参数。

**目标网络参数更新：**

在DQN中，把更新后的网络参数拷贝到目标网络中，将这种称为“硬更新”，在DDPG中，采用“软更新”：

![img](D:\TyporaPicture\22.6.1\1653364974947-54b7d732-8ffb-4029-a6d5-e31f469d3f91.png)

将新的参数和旧的参数按照一定比例进行混合，成为目标网络的新的参数。

使用目标网络的作用是，在更新网络参数时，将“靶子”固定住，减少更新的难度。但使用硬更新的方式，“靶子”在更新后距离原位置很远，同样不利于更新，所以DDPG中通过软更新的方式，限制了前后两个“靶子”的差距，不要产生巨大的跳变，更加便于更新参数。

## 1.4 实验结果

将DDPG算法应用到众多强化学习游戏环境中，得到如下结果：

![img](D:\TyporaPicture\22.6.1\1653365322320-f8078585-6dcf-4b46-9717-d21bebdecfa9.png)

使用的两个baselines分别是：1.在动作空间中随机抽取一个动作执行。 2.使用iLQG，该方法知晓模型的底层物理模型，效果较好 。将两个baselines的平均分数分别设定为0和1

## 1.5 总结

DDPG中，在DQN基础上引入Actor网络用于解决动作空间连续的问题。Actor网络给Critic网络提供行为策略，Critic网络给Actor网络提供准确的Q值，从而更新Actor网络参数，使得最终Critic网络能够估计出Q真实值，而Actor网络能够得到最优策略。

# 2 Deep Reinforcement Learning for List-wise Recommendations    KDD    2019

该论文为DDPG算法在推荐领域的运用，建立了一个用户-智能体交互模拟过程，更好地进行离线训练。提出了LIRD模型，能够运用到物品数量众多即动作空间大的场景中，并且能够有效减少多余的计算开销。

## 2.1 MDP建模

状态s，定义为用户最近的N条浏览记录，根据时间顺序排序：

![img](D:\TyporaPicture\22.6.1\1654057199721-610965cb-bc8b-461e-b591-0ffd4f751592.png)

动作a，定义为一个长度为K的推荐列表：

![img](D:\TyporaPicture\22.6.1\1654057328501-0115d206-88b3-4240-b4fb-2568038cf6a9.png)

奖励r，根据用户对于推荐结果的反馈动作，包括略过、点击、购买，设定不同的奖励值：

![img](D:\TyporaPicture\22.6.1\1654057429036-d28be00b-bd0d-4143-b28f-54c25b9e8472.png)

状态转移概率P，如果用户略过推荐列表中所有物品，则状态不变，如果有点击或者购买动作则状态会更新：

![img](D:\TyporaPicture\22.6.1\1654057763789-2f6b1acb-46d1-4b33-be0c-2dfabc889672.png)

![img](D:\TyporaPicture\22.6.1\1654057773543-20ea92e9-c3a3-4913-94a5-365489a94242.png)

## 2.2 交互环境模拟

有着相似兴趣的用户在同样的物品采取的动作也是相似的，所以可以通过推荐结果去模拟结果中寻找相似的状态以及动作，便可根据模拟结果产生奖励值。

![img](D:\TyporaPicture\22.6.1\1653366991483-e52183e3-90c5-4e39-bd2f-d11750b4b3cb.png)

有了模拟结果M后，在后续训练过程中，对于当前的状态和动作元组 pt (st , at )  ，可以通过计算其与M中元组的相似度：

![img](D:\TyporaPicture\22.6.1\1653367756737-e6f4be56-6748-4dd8-b806-09d76db52d88.png)

于是我们便可以得到当前状态和动作所得到的奖励值r的概率如下：

![img](D:\TyporaPicture\22.6.1\1653367855466-c816b1a8-b717-48dd-9adf-fe2f2ffd09ba.png)

解决了在数据集中，交互记录占比总物品数过少导致的推荐结果的奖励值不好计算的问题。

## 2.3 网络结构

![img](D:\TyporaPicture\22.6.1\1653368125649-50b84416-606d-4828-8468-dfbc53a9d3cf.png)

**Actor网络：**

其中，左侧的Actor网络的输入为用户近期有过交互记录的物品，输出为长度为K的权重向量，K为推荐列表长度。推荐列表的生成过程如下：

![img](D:\TyporaPicture\22.6.1\1653368338126-d9343426-8d81-4c5a-b439-d127d04a3725.png)

在Actor网络输出得到K个权重向量w后，将向量与物品空间中所有可选物品的embedding相乘，得到评分，选取每个权重向量计算得到的最高评分的物品作为推荐结果。得到推荐结果后根据状态、动作，使用模拟结果计算奖励值r。

**Critic网络：**

该网络使用当前状态s以及Actor网络的推荐结果计算Q预测值。

总体训练过程如下：

![img](D:\TyporaPicture\22.6.1\1653368819151-74813186-1564-44cc-bc4c-d3d853b6c3f5.png)

![img](D:\TyporaPicture\22.6.1\1653368835556-7d4e39b3-1d1e-4e30-9c5b-d91148430c0a.png)

## 2.4 实验结果

将用户无操作、点击、购买的奖励值设定为0、1、5

数据集：JD在2017年7月的用户数据，挑选出100000个sessions，前70%为训练集，后30%为测试集。

short-term表示session的长度小于50条，long-term表示session的长度大于50条：

![img](D:\TyporaPicture\22.6.1\1653369030084-8e4ec0d1-9558-40ce-892a-1f4414203586.png)