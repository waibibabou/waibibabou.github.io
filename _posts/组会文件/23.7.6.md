# **1.强化学习算法的分类方式**

<img src="D:\TyporaPicture\23.7.6\image-20230704133002453.png" alt="image-20230704133002453" style="zoom:80%;" />

## **1.1第一种分类方式（常用）：**

分为Model-free RL与Model-based RL，两者最大的区别是：是否有对环境建模。Model-free的算法不会对环境进行建模，直接根据和环境交互过程中产生的数据$(s_t,a_t,r_t,s_{t+1})$来迭代算法。而Model-based类的算法则会对环境建模，建立的模型一般称为World Model，该模型包括两部分，1）状态转移概率$P(s_{t+1}|s_t,a_t)$，即该模型指明了在执行了特定的动作$a_t$之后的下一个状态$s_{t+1}$ 是什么  2）奖励函数$r(s_t,a_t)$，即智能体采取动作得到的奖励值。

在Model-free的算法中，状态转移概率以及奖励函数都是未知的，不会预先定义或者学习，只能通过不断地交互进而训练智能体，在Model-based算法中，这两部分已知。用一句来区分两者：在agent执行它的动作之前，它能否对下一步的状态与奖励做出预测，如果可以则是Model-based方法，如果不能，则是Model-free方法。



## 1.2**第二种分类方式（常用）：**

<img src="D:\TyporaPicture\23.7.6\aa93623cdd442a05ce5c67084835925.png" alt="aa93623cdd442a05ce5c67084835925" style="zoom:80%;" />

根据模型的表示方式可以将RL算法分为三类：Value-based算法(即Critic-only)、Policy-based算法(即Actor-only)、Hybrid算法(即Actor-critic)。

在Critic-only类算法中，模型只有值评估函数，通过学习值评估函数间接学习策略函数。比如用Q函数$Q_t{(s_t,a_t)}$衡量在特定的状态$s_t$下执行$a_t$后累计奖励值的期望。在训练好Q函数后用于决策：在特定状态$s_t$下枚举动作空间下所有动作a，选择Q值最大的动作执行，即$a^*=argmax_aQ(s,a),a\in{A}$

在Actor-only类算法中，模型直接学习策略函数$\pi(a_t|s_t)$，这类算法的决策方式很直观，$s_t$输入给策略函数$\pi$后，模型直接给出一个$a_t$来执行

在Actor-critic算法中，模型同时学习值评估函数以及策略函数，在训练完成后利用策略函数做策略。



## 1.3**第三种分类方式（不常用）：**

<img src="D:\TyporaPicture\23.7.6\image-20230704170034763.png" alt="image-20230704170034763" style="zoom:80%;" />

根据模型的学习特性可以将RL算法分为两类：On-policy（同策略）与Off-policy（异策略）。

算法中有两种策略，1）行为策略(behavior policy)：专门负责获取训练数据，具有一定的随机性，总有一定概率选出潜在的最优动作。2）目标策略(target policy)：借助行为策略收集到的样本提升自身性能，并最终成为最优策略。

对于Off-policy的算法来说，行为策略与目标策略不同，例如Q-learning中数据收集部分用的是由Q函数构造的$\epsilon-greedy$策略$\pi_\epsilon(s)$（行为策略），而它的最终目标是greedy策略$\pi(s)$（目标策略）

对于On-policy的算法来说，行为策略与目标策略相同，例如Sarsa中，只有一个策略$\epsilon-greedy$

用于快速区分算法是On-policy或者Off-policy：训练时用经验回放机制的为Off-policy，若不能则为On-policy



## **1.4第四种分类方式（几乎不用）：**

根据模型的更新方式可以将RL算法分为两类：回合更新(Monte-Carlo update)与单步更新(Temporal-Difference update)

回合更新是会先让agent与环境进行一系列的交互，收集到一整个回合的经验后进行学习，相较于单步更新更加稳定，但训练效率低。例如Reinforce算法是用蒙特卡洛对$Q_\pi$近似，需要得到整个回合的所有奖励值后计算出折扣回报：$u_t=\sum_{k=t}^n\gamma^{k-t}\cdot{r_k}$，从而更新策略函数。

单步更新是会在agent与环境每一步后就进行一次更新，相比于回合更新，其学习速度更快但是不太稳定。例如Actor-Critic算法是用一个额外的价值网络对$Q_\pi$近似，后续利用TD误差更新该价值网络，有了该网络便不需要等到回合结束才知道动作的Q值，从而实现单步更新。



# 2.强化学习策略学习（Policy-Based）总结

## 2.1策略梯度方法

### 2.1.1策略函数定义

策略函数的输出主要有两种形式：确定性策略与随机策略

<img src="D:\TyporaPicture\23.7.6\image-20230704190050529.png" alt="image-20230704190050529" style="zoom:80%;" />

上图的游戏中，一个agent在迷宫中移动，只能看到相邻格子的信息，所以agent分辨不出两个灰色格子的区别。

如果采用确定性的策略（deterministic policy），是对每个s都有一个确定性的a，即$\pi(s)=a$，那么最后学习的结果可能如下：

<img src="D:\TyporaPicture\23.7.6\image-20230704190240136.png" alt="image-20230704190240136" style="zoom:80%;" />

很明显这不是最优策略，在左上角两个格子中出现死循环。

但是随机策略（stochastic policy）可以缓解该问题，随机策略每个s下每个a都有一定的概率，策略函数的输出为采取每个a的概率值，那么最后学习的结果可能如下：

<img src="D:\TyporaPicture\23.7.6\image-20230704190914833.png" alt="image-20230704190914833" style="zoom:80%;" />

很明显该策略优于上面的确定性策略，不会出现死循环，agent在任意位置都能达到目标。

两者核心的区别在于最终的策略是学出$\pi(s)=a$还是$\pi(a_i|s)=p_i$



### 2.1.2策略学习的目标函数

从状态价值函数开始：

状态价值函数的定义是：

![image-20230704192650727](D:\TyporaPicture\23.7.6\image-20230704192650727.png)

观察到状态价值同时依赖于当前状态$s_t$，也依赖于策略网络$\pi$的参数$\theta$：

1）当前状态$s_t$越好，则状态价值就越大，例如智能体如果已经接近终点，那么回报的期望值就越大

2）策略$\pi$越好，那么状态价值也会越大，例如从同一起点出发打游戏，高手（好的策略）的期望回报远高于初学者（差的策略）

所以说如果一个策略很好，则对于所有的状态S，状态价值的均值应当越大，所以我们定义目标函数：

![image-20230704195526495](D:\TyporaPicture\23.7.6\image-20230704195526495.png)

所以策略学习可以描述为一个优化问题：

![image-20230704195639167](D:\TyporaPicture\23.7.6\image-20230704195639167.png)

求解最大化问题，可以用梯度上升更新$\theta$，使得$J(\theta)$增大：

![image-20230704195820918](D:\TyporaPicture\23.7.6\image-20230704195820918.png)

其中的梯度为：

![image-20230704200351107](D:\TyporaPicture\23.7.6\image-20230704200351107.png)

被称为策略梯度。

**下面将会推导策略梯度的表达式：**

状态价值函数$V_\pi(s)$可以写成：

![image-20230704201614165](D:\TyporaPicture\23.7.6\image-20230704201614165.png)

对策略函数$\theta$求梯度：

<img src="D:\TyporaPicture\23.7.6\image-20230704201651919.png" alt="image-20230704201651919" style="zoom:80%;" />

应用链式法则：

<img src="D:\TyporaPicture\23.7.6\image-20230704201742970.png" alt="image-20230704201742970" style="zoom:80%;" />

右边一项x分析非常复杂，不具体分析，由上面公式得：

<img src="D:\TyporaPicture\23.7.6\image-20230704201842715.png" alt="image-20230704201842715" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230704201906609.png" alt="image-20230704201906609" style="zoom:80%;" />

根据定义$J(\theta)=E_S[V_\pi(S)]$得：

<img src="D:\TyporaPicture\23.7.6\image-20230704202032312.png" alt="image-20230704202032312" style="zoom:80%;" />

我们无法求解出这个期望值，因为并不知道状态S的概率密度函数，所以我们从环境中抽样来近似策略梯度的期望值。每次从环境中观察到一个状态s，相当于随机变量S的观测值，然后再根据当前的策略网络随机抽样一个动作，计算策略梯度：

![image-20230704203342794](D:\TyporaPicture\23.7.6\image-20230704203342794.png)

因此可以通过梯度上升来更新$\theta$，使得目标函数$J(\theta)$逐渐上升：

![image-20230704203439770](D:\TyporaPicture\23.7.6\image-20230704203439770.png)

此时我们只需要知道$Q_\pi(s,a)$便能够训练策略函数。后续有两种方法对$Q_\pi(s,a)$做近似：1）Reinforce：用整个一个轨迹的实际观测的回报u近似$Q_\pi(s,a)$  2）Actor-Critic：用神经网络$q(s,a;w)$近似$Q_\pi(s,a)$



### 2.1.3 Reinforce算法

按照分类：Policy-based、On-policy、回合更新

可用于：离散动作空间、连续动作空间(较少)

注：如果某算法的策略函数是随机策略而想应用于连续动作空间，则需要让策略函数输出动作的均值与方差，之后利用重参数化技巧在高斯分布中采样得到动作，从而可得$\pi(a_i|s_i)$的值，如果像ddpg一样直接输出动作则无法得到动作概率值。

从t时刻开始，智能体完成一局游戏，观测到全部奖励$r_t,\cdot\cdot\cdot,r_n$可以计算出$u_t=\sum_{k=t}^n\gamma^{k-t}\cdot{r_k}$，我们可以用$u_t$代替$Q_\pi(s,a)$，所以上式近似为：

![image-20230704205257654](D:\TyporaPicture\23.7.6\image-20230704205257654.png)

通过梯度上升更新策略网络参数$\theta$:

![image-20230704205331090](D:\TyporaPicture\23.7.6\image-20230704205331090.png)

**训练流程：**

<img src="D:\TyporaPicture\23.7.6\image-20230704214037403.png" alt="image-20230704214037403" style="zoom: 67%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230704214045436.png" alt="image-20230704214045436" style="zoom:77%;" />

### 2.1.4 Actor-Critic算法

按照分类：Hybrid、On-policy、单步更新

可用于：离散动作空间、连续动作空间

为什么Actor-Critic算法会出现在书中的Policy-based章节呢，是因为Actor-Critic算法本质上是基于策略的算法，因为这系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。所以如果粗略的分类，RL算法可分为Value-based与Policy-based。

上面的Reinforce用实际观测的回报近似$Q_\pi$，而Actor-Critic方法用神经网络近似$Q_\pi$

我们此处增加一个价值网络，记为$q(s,a;w)$，后续用TD误差更新价值网络，利用价值网络近似后的策略梯度更新策略网络：

![image-20230704213751363](D:\TyporaPicture\23.7.6\image-20230704213751363.png)

**训练流程**，由于使用TD误差更新动作价值网络，所以增加一个目标网络用于计算目标值：

<img src="D:\TyporaPicture\23.7.6\image-20230704215333762.png" alt="image-20230704215333762" style="zoom:67%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230704215342842.png" alt="image-20230704215342842" style="zoom:77%;" />



## 2.2带基线的策略梯度方法

在上面两种策略梯度方法中，引入基线（baseline）之后，Reinforce算法变为Reinforce with Baseline，Actor-Critic算法变为Advantage Actor-Critic（A2C）

### 2.2.1策略梯度中的基线

对策略梯度公式做一个微小改动就能够大幅提升Reinforce与Actor-Critic表现：

<img src="D:\TyporaPicture\23.7.6\image-20230704223110351.png" alt="image-20230704223110351" style="zoom:80%;" />

其中b可以是任意的函数，只要不依赖于动作A即可，例如可以是状态价值函数$V_\pi(S)$

这样对计算得到的梯度不会有影响是因为：

<img src="D:\TyporaPicture\23.7.6\image-20230704223345235.png" alt="image-20230704223345235" style="zoom:80%;" />

证明：

由于基线b不依赖于动作A，可以把b提取到期望外面：

<img src="D:\TyporaPicture\23.7.6\image-20230704223806567.png" alt="image-20230704223806567" style="zoom:80%;" />

由于上式的求和是关于a，而偏导是关于$\theta$，因此可以将求和放入偏导内部：

<img src="D:\TyporaPicture\23.7.6\image-20230704223921507.png" alt="image-20230704223921507" style="zoom:80%;" />

所以

<img src="D:\TyporaPicture\23.7.6\image-20230704223933567.png" alt="image-20230704223933567" style="zoom:80%;" />

一般用$b=V_\pi(s)$作为基线。

为什么要引入基线：如果一个RL任务中，所有的动作奖励值都是大于0的，只是大小有区别，那么就会发生，采集到的动作的概率值经过训练后一定上升，而没采集到的动作的概率值下降，这对于别的动作来说不公平，所以通过引入基线，使得就算某些动作的Q值是正的，如果小于该基线值，也会让其概率下降。



### 2.2.2带基线的Reinforce算法

按照分类：Policy-based、On-policy、回合更新

可用于：离散动作空间、连续动作空间(较少)

引入基线的策略梯度为：

![image-20230704230045522](D:\TyporaPicture\23.7.6\image-20230704230045522.png)

使用实际观测的回报u来近似$Q_\pi(s,a)$，此外用一个网络$v(s;w)$近似$V_\pi(s)$，从而

![image-20230704230247458](D:\TyporaPicture\23.7.6\image-20230704230247458.png)

带基线的Reinforce需要两个网络：策略网络$\pi(a|s;\theta)$与价值网络$v(s;w)$，两个网络的输入都是状态s，在实际应用时可以共享对于状态s的嵌入模块。

虽然该算法有一个策略网络和一个价值函数，但该方法不是Actor-Critic，因为价值网络没有起到指导策略网络的作用，仅仅是作为一个基线值，真正帮助策略网络更新的是实际观测到的回报u

得到带基线的策略梯度来更新策略网络：

![image-20230705111050659](D:\TyporaPicture\23.7.6\image-20230705111050659.png)

训练价值网络目的是让$v(s_t;w)$拟合$V_\pi(s_t)$，即拟合$u_t$的期望，利用蒙特卡洛抽样的方式近似，根据一条轨迹得到的$Q(s_t,a_t)$就是$V(s_t)$，不需要对所有动作的q值求期望，所以此时有了ground truth，直接用$u_t$训练价值网络即可：

![image-20230705111553991](D:\TyporaPicture\23.7.6\image-20230705111553991.png)

**训练流程**，由于没有利用TD误差更新网络，所以不需要目标网络：

<img src="D:\TyporaPicture\23.7.6\image-20230705111617387.png" alt="image-20230705111617387" style="zoom:67%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230705111656449.png" alt="image-20230705111656449" style="zoom:77%;" />

### 2.2.3 Advantage Actor-Critic算法 (A2C)

按照分类：Hybrid、On-policy、单步更新

可用于：离散动作空间、连续动作空间

训练价值网络：由于不像Reinforce中有$V_\pi(s_t)$的真实值，所以要用TD误差更新价值网络，$v(s_{t+1};w)$的目标值为：

![image-20230705133817824](D:\TyporaPicture\23.7.6\image-20230705133817824.png)

价值函数的损失函数：

<img src="D:\TyporaPicture\23.7.6\image-20230705134708306.png" alt="image-20230705134708306" style="zoom:80%;" />

训练策略网络：

将策略梯度中的$Q_\pi(s_t,a_t)$用期望代替

<img src="D:\TyporaPicture\23.7.6\image-20230705134731943.png" alt="image-20230705134731943" style="zoom:80%;" />

对期望做蒙特卡洛近似：

![image-20230705134853497](D:\TyporaPicture\23.7.6\image-20230705134853497.png)

即策略梯度为：

![image-20230705134928739](D:\TyporaPicture\23.7.6\image-20230705134928739.png)

**训练流程**，同样利用目标网络计算状态价值的目标值：

<img src="D:\TyporaPicture\23.7.6\image-20230705135002066.png" alt="image-20230705135002066" style="zoom:80%;" />



## 2.3策略学习高级技巧

### 2.3.1 TRPO算法

按照分类：根据Q的近似方式分成Policy-based或Hybrid、On-policy、回合更新

可用于：离散动作空间、连续动作空间

置信域方法：

<img src="D:\TyporaPicture\23.7.6\image-20230705163037935.png" alt="image-20230705163037935" style="zoom:67%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230705163045583.png" alt="image-20230705163045583" style="zoom:67%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230705163102873.png" alt="image-20230705163102873" style="zoom:67%;" />

将策略梯度的目标函数$J(\theta)$变换一种等价形式：

<img src="D:\TyporaPicture\23.7.6\image-20230705163438076.png" alt="image-20230705163438076" style="zoom:80%;" />



目标函数的等价形式：

<img src="D:\TyporaPicture\23.7.6\image-20230705164223514.png" alt="image-20230705164223514" style="zoom:80%;" />

将其中的$Q_\pi(S,A)$近似成观测到的折扣回报后得到近似的函数为：

<img src="D:\TyporaPicture\23.7.6\image-20230705164853307.png" alt="image-20230705164853307" style="zoom:80%;" />

**训练流程：**

TRPO需要重复做近似和最大化这两个步骤：

<img src="D:\TyporaPicture\23.7.6\image-20230705163744376.png" alt="image-20230705163744376" style="zoom:80%;" />



### 2.3.2 PPO算法

按照分类：根据Q的近似方式分成Policy-based或Hybrid、On-policy、回合更新

可用于：离散动作空间、连续动作空间

版本1：

将TRPO中的约束项直接放到目标函数中：
$$
\theta_{new}=\mathop{argmax}\limits_{\theta}({\sum_{t=1}^n\frac{\pi(a_t|s_t;\theta)}{\pi(a_t|s_t;\theta_{now})}\cdot{u_t}}-\beta{D_{KL}[\pi(\cdot|s_t;\theta_{now})||\pi(\cdot|s_t;\theta)]})
$$
其中设定了可自动调节的KL因子$\beta$，$\beta$的更新规则如下：

设$d_k={D_{KL}[\pi(\cdot|s_t;\theta_{now})||\pi(\cdot|s_t;\theta)]}$

如果$d_k<\sigma/1.5$则$\beta_{k+1}=\beta_k/2$，如果$d_k>1.5*\sigma$则$\beta_{k+1}=2*\beta_k$，否则$\beta_{k+1}=\beta_k$



版本2：min(ba,ca)!=min(b,c)*a

PPO的另一种形式PPO-截断更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大：
$$
\theta_{new}=\mathop{argmax}_{\theta}[min(\frac{\pi(a_t|s_t;\theta)}{\pi(a_t|s_t;\theta_{now})}\cdot{u_t},clip(\frac{\pi(a_t|s_t;\theta)}{\pi(a_t|s_t;\theta_{now})},1-\epsilon,1+\epsilon)\cdot{u_t})]
$$
如果$u_t>0$，那么最大化上式要增大$\frac{\pi(a_t|s_t;\theta)}{\pi(a_t|s_t;\theta_{now})}$，但由于截断与取min操作的存在，不会让该比值超过$1+\epsilon$。

如果$u_t<0$，那么最大化上式要减小$\frac{\pi(a_t|s_t;\theta)}{\pi(a_t|s_t;\theta_{now})}$，但由于截断与取min操作的存在，不会让该比值小于$1-\epsilon$，因为$u_t$是负数，所以该比值过小则min操作会取到后面那项。



### 2.3.3 SAC算法

按照分类：Hybrid、Off-policy、单步更新

可用于：离散动作空间、连续动作空间

策略学习的目的是学出一个策略网络$\pi(a|s;\theta)$用于控制智能体，但如果策略函数的概率分布比较集中，例如三个动作分别的概率为[0.03,0.96,0.01]，这个随机策略接近确定性的策略。确定性大的好处在于不容易选中很差的动作，比较安全。但缺点是智能体在训练过程中容易安于现状，不去尝试没做过的动作，不去探索更多的状态，便无法找到更好的策略。

<img src="D:\TyporaPicture\23.7.6\image-20230705214836725.png" alt="image-20230705214836725" style="zoom:80%;" />

所以我们希望策略网络的输出不要集中在一个动作上，我们把熵作为正则项，放到目标函数中，使用熵正则的策略学习可以写作这样的最大化问题：

![image-20230705215035363](D:\TyporaPicture\23.7.6\image-20230705215035363.png)

在SAC算法中，状态价值函数为：

![image-20230705220304320](D:\TyporaPicture\23.7.6\image-20230705220304320.png)

即熵越大状态价值越大，体现策略越好



在SAC算法中，有两个动作价值函数Q和一个策略函数$\pi$，每次计算Q值时会挑选一个Q值小的网络，从而缓解Q值过高估计的问题。训练动作价值网络利用TD误差：

![image-20230705220149650](D:\TyporaPicture\23.7.6\image-20230705220149650.png)

训练策略网络，需要最大化V：

![image-20230705221149541](D:\TyporaPicture\23.7.6\image-20230705221149541.png)

更新熵正则项系数$\alpha$：

![image-20230705221442975](D:\TyporaPicture\23.7.6\image-20230705221442975.png)

当策略的熵低于目标值$H_0$时，训练目标$L(\alpha)$会使$\alpha$增大，进而在上述最小化损失函数$L_\pi(\theta)$的过程中增加了策略熵对应项的重要性；而当策略的熵高于目标值$H_0$时，训练目标$L(\alpha)$会使$\alpha$减小，进而使得策略训练时更专注于价值提升。

训练流程：

<img src="D:\TyporaPicture\23.7.6\image-20230705215423030.png" alt="image-20230705215423030" style="zoom:80%;" />



## 2.4确定性策略学习

### 2.4.1确定性策略

上面讲的所有算法都属于随机策略，其中的策略函数对于离散动作空间会输出所有动作的概率值，对于连续动作空间会输出动作的均值和方差后通过重参数化技巧在高斯分布中采样得到动作，策略函数都没有直接输出动作，而下面的两个算法DDPG与TD3的策略函数都是直接输出动作，即策略是确定性策略。

### 2.4.2 DDPG算法

按照分类：Hybrid、Off-policy、单步更新

可用于：连续动作空间

<img src="D:\TyporaPicture\23.7.6\image-20230705222511432.png" alt="image-20230705222511432" style="zoom:80%;" />

**训练流程：**

<img src="D:\TyporaPicture\23.7.6\image-20230705222554367.png" alt="image-20230705222554367" style="zoom:80%;" />



### 2.4.3 TD3算法

按照分类：Hybrid、Off-policy、单步更新

可用于：连续动作空间

<img src="D:\TyporaPicture\23.7.6\image-20230705223017411.png" alt="image-20230705223017411" style="zoom:80%;" />

TD3算法是DDPG的优化版本，主要有三个优化：

1）将当前Q网络与目标Q网络都变为两个网络，$Q1(A')$和$Q2(A')$取min代替DDPG中的Q计算target，$target=r+\gamma*min(Q1,Q2)$

target是两个Q网络的target，虽然更新目标一样，两个网络都会逐渐与实际Q值相同，但是参数初始值不同，导致计算得到的Q值不同，还是有一定空间选择较小的值去估算Q值，避免Q值被高估。

2）actor的延迟更新：让critic更新多次后再对actor更新。因为actor的更新是沿着Q增大的方向，当actor好不容易去到最高点；q值更新了，这并不是最高点，这时候actor只能转头再继续寻找新的最高点

所以我们可以把critic的更新频率，调的比actor要高一点。让critic更加确定，actor再行动。

3）在DDPG中，计算目标Q值时，是不会在目标策略网络的输出上加噪声的，而在TD3中，在计算target时，输入s'到actor输出a后，给a增加噪声，相当于用一小块范围内去估算target，使得估计出值更准确。



在DDPG中，计算target的时候，我们输入s和a，获得q，也就是这块布上的一点A。通过估算target估算另外一点s，a，也就是布上的另外一点B的Q值。

<img src="D:\TyporaPicture\23.7.6\image-20230705223847480.png" alt="image-20230705223847480" style="zoom:67%;" />

当更新多次的时候，就相当于用A点附近的一小部分范围的去估算B，这样可以让B点的估计更准确，更健壮

<img src="D:\TyporaPicture\23.7.6\image-20230705223904574.png" alt="image-20230705223904574" style="zoom:67%;" />



## 2.5模仿学习

模仿学习（Imitation Learning）不是强化学习，而是强化学习的一种替代品，模仿学习与强化学习有相同的目的：两者的目的都是学习策略网络，从而控制智能体。模仿学习与强化学习有不同的原理：模仿学习向人类专家学习，目标是让策略网络做出的决策与人类专家相同；而强化学习利用环境反馈的奖励改进策略，目标是让累计奖励（即回报）最大化。

### 2.5.1行为克隆

行为克隆不需要智能体与环境有任何交互，行为克隆的目的是模仿人的动作，学习一个随机策略$\pi(a_t|s_t)$或者确定性策略$\pi(s_t)$，虽然行为克隆的目的与强化学习中的策略学习类似，但是行为克隆的本质是监督学习，对于连续动作空间，就是一个回归问题，对于离散动作空间，就是一个分类问题。

训练前需要预先准备一个数据集，由（状态，动作）这样的二元组构成：

<img src="D:\TyporaPicture\23.7.6\image-20230705231539177.png" alt="image-20230705231539177" style="zoom:80%;" />

**连续动作空间：**

损失函数为MAE：

![image-20230705231632022](D:\TyporaPicture\23.7.6\image-20230705231632022.png)

训练流程：

<img src="D:\TyporaPicture\23.7.6\image-20230705231712899.png" alt="image-20230705231712899" style="zoom:80%;" />

**离散动作空间：**

损失函数为交叉熵损失函数：

<img src="D:\TyporaPicture\23.7.6\image-20230705231853609.png" alt="image-20230705231853609" style="zoom:80%;" />

训练流程：

<img src="D:\TyporaPicture\23.7.6\image-20230705231902342.png" alt="image-20230705231902342" style="zoom:80%;" />

为什么行为克隆效果不好：因为给定好的数据集相当于是专家给定的ground truth数据，人类不会探索奇怪的状态和动作，因此数据集中的状态和动作缺乏多样性，智能体在真实的环境中可能遇到没见过的状态，此时的决策可能会很糟糕，导致下一时刻的状态比较罕见，于是后续的动作又会很差，产生恶性循环。

行为克隆的优势在于离线学习，避免与环境的交互，尽管效果不如强化学习，但是成本很低。可以先用行为克隆初始化策略网络，而不是随机初始化，然后再做强化学习，这样可以加快训练过程并且减小对物理环境的影响。

### 2.5.2生成判别模仿学习(GAIL)

生成判别模仿学习需要让智能体与环境交互，但是无法从环境获得奖励。GAIL还需要收集专家的决策记录（即很多条轨迹）。GAIL的目标是学习一个策略函数，使得判别器无法区分一个轨迹是策略网络的决策还是人类专家的决策。

GAIL的生成器与判别器：

**训练数据：**GAIL的训练数据是专家操作智能体得到的轨迹，记为：

![image-20230706003529105](D:\TyporaPicture\23.7.6\image-20230706003529105.png)

**生成器：**

生成器是策略网络$\pi(a|s;\theta)$，输入是状态，输出是一个向量，其中值表示执行动作的概率值：

![image-20230706003905027](D:\TyporaPicture\23.7.6\image-20230706003905027.png)

**判别器：**

判别器的输入是状态s，输出是一个向量：

![image-20230706004015206](D:\TyporaPicture\23.7.6\image-20230706004015206.png)

$p_a$接近1说明$(s,a)$为真，即动作a是专家做的，接近0说明$(s,a)$为假，即a是策略网络生成的

训练生成器：

根据策略网络采样这样一条轨迹：

![image-20230706004225411](D:\TyporaPicture\23.7.6\image-20230706004225411.png)

其中$u_t$是判别器的输出：

<img src="D:\TyporaPicture\23.7.6\image-20230706004300302.png" alt="image-20230706004300302" style="zoom:80%;" />

根据TRPO更新策略网络，目标函数为：

<img src="D:\TyporaPicture\23.7.6\image-20230706004347581.png" alt="image-20230706004347581" style="zoom:80%;" />

训练判别器：

损失函数为：

![image-20230706004517031](D:\TyporaPicture\23.7.6\image-20230706004517031.png)

**训练流程：**

<img src="D:\TyporaPicture\23.7.6\image-20230706003635475.png" alt="image-20230706003635475" style="zoom:80%;" />

<img src="D:\TyporaPicture\23.7.6\image-20230706003646107.png" alt="image-20230706003646107" style="zoom:80%;" />



