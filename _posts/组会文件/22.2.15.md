##  PRIORITIZED EXPERIENCE REPLAY   ICLR 2016（优先经验回放） 



### 1.Prioritized Replay DQN之前算法的问题

​        在引入Prioritized Replay DQN之前，已经有许多DQN模型以及他们的改进算法，比如Nature DQN，DDQN等，他们都是通过经验回放来采样，进而计算目标Q值。采取经验回放是因为深度神经网络作为有监督学习模型，要求样本数据满足独立同分布，但是DQN算法得到的样本是前后有关系的，经验回放机制可以通过存储-采样的方法打破样本之间的关联性。



​        之前的模型在采样的时候，在经验回放池里面的所有的样本都有相同的被采样到的概率。但是在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，对反向梯度的计算影响不大。

​       这样如果TD误差的绝对值|δ(t)|较大的样本更容易被采样，则我们的算法会比较容易收敛。



### 2.Prioritized Replay DQN算法的建模

​       Prioritized Replay DQN根据每个样本的TD误差绝对值|δ(t)|，给定该样本的优先级正比于|δ(t)|，将这个优先级的值存入经验回放池。之前的DQN算法，仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。

​       使用SumTree这样的二叉树结构来存储带有优先级的经验回放的样本数据：



![img](D:\TyporaPicture\Untitled\1644890948608-01b46c9c-4ee8-49c2-8a36-302bb312bb34.png)

​       所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。

​       这样可以通过查看根节点中保存的优先级值判断从什么区间中均匀采样，比如上图根节点为42，就在[0,42]中均匀采样，采样到哪个区间，就是哪个样本，显然优先级大的叶子节点更容易被采集到。



​       除了经验回放值，现在Q网络的算法损失函数也有优化，考虑了样本优先级的损失函数为：

![img](D:\TyporaPicture\Untitled\1644891594107-081117d1-0fa6-4657-9f51-eb6dc22ff7fa.png)

​       其中wj为第j个样本的优先级权重，由TD误差|δ(t)|归一化得到。



​       此外，在每次对Q网络的参数进行更新后，需要重新计算每个样本的TD误差，并将TD误差更新到SunTree上面。



### 3. Prioritized Replay DQN算法流程

![img](D:\TyporaPicture\Untitled\1644891983934-6761665d-32dc-4def-af9a-0aa2152b8629.png)

​       Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，同时其可以直接集成DDQN算法。



![img](D:\TyporaPicture\Untitled\1647908346797-16c271ae-5748-4d12-867e-12e980cf9df6.png)