# 1. Continuous Input Embedding Size Search For Recommender Systems   SIGIR 2023

## 1.1 出发点

隐因子模型将每个用户与物品表示为一个embedding向量，用于两两计算其相似度，其中所有的embedding向量被限制成一个固定的维度，并且该维度通常很大（例如256维）。随着当代电子商务的发展，用户与物品个数越来越多，这种设计embedding的方式会非常消耗内存空间。为了促进轻量化的推荐模型发展，强化学习近期被用来对不同的用户与物品赋予不同的embedding维度。但是现有的利用强化学习来搜索不同维度的算法都将agent的动作空间限制在一个高度离散的，预先定义好的候选集中，例如1~256维的搜索空间，只选取其中的10个值作为每个用户/物品维度的选择范围。

## 1.2 本文工作

1）本文指出，对于利用强化学习解决可变的embedding维度搜索问题，将高度离散的动作空间扩展到连续动作空间（当然这里的连续是指整数的连续，因为嵌入维度必须为整数）更有利于搜索对于每个用户/物品更优的嵌入维度。

2）提出了模型CIESS，一个基于RL的嵌入维度搜索算法，将agent的动作空间扩展到连续动作空间，为每一个用户/物品搜索最优嵌入维度。并且在actor网络的输出中应用随机游走策略(random walk)增大探索程度，来得到更优的策略网络。

3）基于多种base recommenders开展实验，证实了CIESS模型的有效性

## 1.3 模型总览

<img src="D:\TyporaPicture\23.5.18\image-20230516204809593.png" alt="image-20230516204809593" style="zoom: 80%;" />

CIESS模型总体分为两部分，两部分在训练时交替工作：一个推荐模型$F_\Theta(\cdot)$以及一个基于强化学习的嵌入维度搜索模型$G_\Phi(\cdot)$。

在训练的每一次迭代中，推荐模型$F_\Theta(\cdot)$会根据搜索模型$G_\Phi(\cdot)$输出的维度改变其嵌入向量的维度大小，之后训练推荐模型$F_\Theta(\cdot)$，利用Recall以及NDCG指标衡量该推荐模型效果此后会根据推荐模型效果更新搜索模型$G_\Phi(\cdot)$，之后进行下一轮迭代。

## 1.4 Base Recommender with Masked Embeddings

用户以及物品的embedding table **E**的维度为：

<img src="D:\TyporaPicture\23.5.18\image-20230516210127469.png" alt="image-20230516210127469" style="zoom:80%;" />

其中$d_{max}$是初始的所有用户/物品的嵌入维度，也是agent动作空间可以输出的最大值。

为了使得嵌入维度可变，引入一个mask矩阵M：

<img src="D:\TyporaPicture\23.5.18\image-20230516210528802.png" alt="image-20230516210528802" style="zoom:80%;" />

其中的每一行为：

<img src="D:\TyporaPicture\23.5.18\image-20230516210735370.png" alt="image-20230516210735370" style="zoom:80%;" />

此时某个用户/物品的embedding为：

<img src="D:\TyporaPicture\23.5.18\image-20230516210639907.png" alt="image-20230516210639907" style="zoom:80%;" />

有了该mask矩阵M，每个用户/物品我们只保留其前$d_n$个值，后面所有值设为0，使得embedding table变为一个稀疏矩阵，利用一些存储稀疏矩阵的技巧，对于0的存储几乎不会占用内存空间，从而实现降低内存使用的目的。

得到embedding后根据推荐模型得到用户对某物品的喜好：

<img src="D:\TyporaPicture\23.5.18\image-20230516211630526.png" alt="image-20230516211630526" style="zoom:80%;" />

其中的推荐模型$F_\Theta(\cdot)$可以选择任意推荐模型。

推荐模型训练时的loss选取BPRloss：

<img src="D:\TyporaPicture\23.5.18\image-20230516211953167.png" alt="image-20230516211953167" style="zoom:80%;" />

其中$(u,v,v')$代表用户$u$对$v$的喜欢程度超过$v'$

由于我们需要考虑内存大小的限制因素，所以需要在embedding table的稀疏度大于c时候最小化BPRloss：

<img src="D:\TyporaPicture\23.5.18\image-20230516212303168.png" alt="image-20230516212303168" style="zoom:80%;" />

## 1.5 MDP建模

### 1.5.1 环境(environment)

1.4节中的base recommender即为强化学习的环境，其根据actor的输出维度改变embedding table，训练好推荐模型后根据评测结果返回一个reward以及下一个状态。

### 1.5.2 状态(state)

<img src="D:\TyporaPicture\23.5.18\image-20230516214346463.png" alt="image-20230516214346463" style="zoom:80%;" />

其中$f_n$为用户/物品的流行程度，即数据集中出现次数

其中$q_n$代表n的维度从$d_{max}$到此时$d_n$的推荐质量影响大小：

<img src="D:\TyporaPicture\23.5.18\image-20230516214644446.png" alt="image-20230516214644446" style="zoom:80%;" />

对于推荐模型$F_\Theta(\cdot)$的衡量标准使用$Recall@k$以及$NDCG@k$

对于某用户，推荐效果为：

<img src="D:\TyporaPicture\23.5.18\image-20230516214917684.png" alt="image-20230516214917684" style="zoom:80%;" />

而对于物品来说，无法直接计算其Recall以及NDCG，所以通过所以与其交互过的user的指标得到：

<img src="D:\TyporaPicture\23.5.18\image-20230516215106404.png" alt="image-20230516215106404" style="zoom:80%;" />

### 1.5.3 奖励(reward)

奖励函数的设计中需要体现推荐模型的效果以及内存的使用情况：

<img src="D:\TyporaPicture\23.5.18\image-20230516215332312.png" alt="image-20230516215332312" style="zoom:80%;" />

其中的平方可以加速训练的初始阶段，例如${d_n}/{d_{max}}$从1到0.9，并且可以使得训练后程变得平稳，例如$d_n/d_{max}$从0.2到0.1

### 1.5.4 动作(action)

在每一次迭代i中，策略网络(即actor网络)根据某用户/物品状态$s_n^i$输出一个嵌入维度$d_n^i\in{\mathbb{N}}$，随后推荐模型根据该嵌入维度更新mask矩阵M，得到奖励以及下一个状态$s_n^{t+1}$，将四元组$(s_n^i,d_n^i,r^i_n,s_n^{i+1})$存储到经验回放池B中用于训练强化学习模型。

## 1.6 Actor and Critic

强化学习模型使用TD3模型（DDPG的优化版本）

使用两个actor网络分别用于用户以及物品的维度输出：

<img src="D:\TyporaPicture\23.5.18\image-20230516220805501.png" alt="image-20230516220805501" style="zoom:80%;" />

对应的，分别为用户以及物品建立一个critic网络用于计算q值，分别为$Q_u(s_u^i,d_u^i)$以及$Q_v(s_v^i,d_v^i)$

为了增加actor的探索力度，首先在actor网络的输出中加入高斯噪声，随后根据输出，在一张只包含整数的图中进行随机游走得到最终输出的嵌入维度。

该图的建立过程为：

<img src="D:\TyporaPicture\23.5.18\image-20230516222054947.png" alt="image-20230516222054947" style="zoom:80%;" />

从图中的d走到d'的概率为：

<img src="D:\TyporaPicture\23.5.18\image-20230516222106157.png" alt="image-20230516222106157" style="zoom:80%;" />

一共可以走5步，得到一个整数序列： $\mathcal{Z}_{\hat{d_n^i}}$

对于该序列的每个整数通过critic网络计算q值，选择最大的q值作为最终输出的维度：

<img src="D:\TyporaPicture\23.5.18\image-20230516222629437.png" alt="image-20230516222629437" style="zoom:80%;" />

## 1.7 训练过程总览

<img src="D:\TyporaPicture\23.5.18\image-20230516222705805.png" alt="image-20230516222705805" style="zoom:80%;" />

## 1.8 嵌入维度挑选

挑选出平均的$q_n$最高的一个$l$个mask矩阵M，并且这些矩阵都需要满足稀疏度大于c：

<img src="D:\TyporaPicture\23.5.18\image-20230516223054831.png" alt="image-20230516223054831" style="zoom: 80%;" />

对这$l$个M再次训练推荐模型直至收敛，得到平均$q_n$最大的M作为最终的嵌入维度：

<img src="D:\TyporaPicture\23.5.18\image-20230516223548941.png" alt="image-20230516223548941" style="zoom:80%;" />

这里需要再次训练的原因是在之前的训练阶段考虑到效率问题，没有在推荐模型完全收敛后才去评测。

## 1.9 实验

### 1.9.1 实验结果

![image-20230516223712130](D:\TyporaPicture\23.5.18\image-20230516223712130.png)

其中推荐模型选取了三种：LightGCN，NGCF，NCF

对比的嵌入维度搜索算法主要有：

PEP：通过学习一个soft pruning thresholds来得到稀疏的embeddings

ESAPN：通过RL搜索，但是动作空间离散

OptEmb：通过进化算法搜索最优嵌入维度

SU：所有用户/物品嵌入维度相同

SR：所有用户/物品嵌入维度随机

### 1.9.2 消融实验

<img src="D:\TyporaPicture\23.5.18\image-20230516224841255.png" alt="image-20230516224841255" style="zoom:67%;" />

**对本文的质疑：**嵌入维度搜索问题真的适合用强化学习解决吗，因为在本文状态以及动作的设定中，在不同状态s下执行相同的动作的话，s'是相同的，也就不是一个序列问题。老师讲说这是之前那种的特殊情况相当于，把前面的全挤出去了，相当于维护了一个长度为1的list作为状态。



# 2.Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs  SIGIR 2020

## 2.1 出发点

知识图谱如今被广泛用于推荐系统当中，一方面知识图谱中用户与物品的连接可以使得模型更好地学习到用户-物品的关系来提高推荐的准确性，另一方面某用户与某物品在知识图谱中的多跳连接关系使得推荐的可解释性提高。

传统的利用知识图谱的推荐方法使用穷举的方法来寻找可行路径，会导致收敛性差、可解释性不佳等问题，本文提出通过引入监督信息的方式监督路径寻找的过程来解决该问题。

## 2.2 本文工作

1）本文提出了一种基于元启发式的提取方法，从知识图谱中提取出示例路径（可理解为专家信息），这些示例路径将引导后续智能体的学习

2）提出了模型ADversarial Actor-Critic（ADAC），其能够利用提取出的不完美的示例数据指引路径搜索过程，来得到准确性更高、可解释性更强的推荐结果

3）在真实数据集中开展的实验证实，提出的模型在推荐的准确性上提高了6.8%，在可解释性上提高9.3%

## 2.3 问题定义

本文的基于知识图谱的可解释推荐问题可以定义如下：

**输入：**

用户集合$U$，物品集合$V$，交互数据$V_u$:表示与用户$u$交互过的物品集合，以及知识图谱$\mathcal{G}=\{(e,r,e')|e,e'\in\mathcal{E},r\in\mathcal{R}\}$

比如两个实体为Nike与旗下某运动鞋，关系为生产关系。此外该知识图谱中还包含了用户与物品的交互记录。

**输出：**

对于某个用户$u$，本文的模型输出两部分：

推荐的物品集合$\hat{V_u}\subseteq{V}$

以及对于每个推荐的物品的原因路径：$\tau_{u,\hat{v}_u}=[u\stackrel{r_1}\rightarrow{e_1}\stackrel{r_2}\rightarrow...\stackrel{r_{k-1}}\longrightarrow{e_{k-1}}\stackrel{r_k}\rightarrow{\hat{v}_u}]$



## 2.4 模型总览

![image-20230517231516290](D:\TyporaPicture\23.5.18\image-20230517231516290.png)



## 2.5 示例路径提取

示例路径为：

<img src="D:\TyporaPicture\23.5.18\image-20230517231720070.png" alt="image-20230517231720070" style="zoom:80%;" />

其中$\tau_{u,v_u}^E=[u\stackrel{r_1^E}\rightarrow{e_1^E}\stackrel{r_2^E}\rightarrow...\stackrel{r_{k-1}^E}\longrightarrow{e_{k-1}^E}\stackrel{r_k^E}\rightarrow{v_u}]$

示例路径由以下这三种方式提取：

1）最短路径：研究表明，精炼的解释降低了用户的认知负担，同时也被认为是更具有解释性的，因此用户-物品之间更短的路径比随机采样得到的路径更具有解释性。在此我们考虑某用户以及与其有过交互的物品，首先在知识图谱中去除这两者之间的连接边，随后运用Dijkstra得到该用户到该物品的最短路，将该路径视为示例路径。对每个用户以及与该用户产生过交互的物品重复该过程，来得到一组示例路径。

2）元路径（meta-path）：

<img src="D:\TyporaPicture\23.5.18\image-20230517232901358.png" alt="image-20230517232901358" style="zoom:80%;" />

本文的模型通过极少量的（1~3条）人工定义的元路径来提高模型性能。我们在知识图谱中进行有限制的随机游走，将每个用户$u$设为随机游走的起点，然后仅采样那些路径的元路径属于预定义集合的路径，在所有采样得到的路径中，只保留最终物品为该用户交互过的物品的路径，将这些路径视为示例路径。

3）兴趣路径：一个更具解释性的路径应当在实体级别符合用户的兴趣，比如路径中尽可能多的包含用户感兴趣的物品。本文通过随机游走的方式得到一组路径，起点设置为用户$u$，随后查看每条路径中的实体是否多数为该用户该兴趣的物品，如果是则保留。最后去除掉最终物品不是该用户感兴趣的物品的路径，剩下的为示例路径。

## 2.6 MDP建模

### 2.6.1 environment

该知识图谱为actor-critic网络训练时的环境，其根据actor采取的搜索动作返回一个奖励值以及下时刻的状态

### 2.6.2 state

t时刻的状态为从某用户$u$开始到$e_t$实体的一个路径：

![image-20230517234439676](D:\TyporaPicture\23.5.18\image-20230517234439676.png)

### 2.6.3 action

动作是actor网络的输出：

![image-20230517234528100](D:\TyporaPicture\23.5.18\image-20230517234528100.png)

动作空间有所限制，因为只能转移到知识图谱中的邻居节点，并且该节点不能出现在$s_t$中：

<img src="D:\TyporaPicture\23.5.18\image-20230517234829424.png" alt="image-20230517234829424" style="zoom:80%;" />



### 2.6.4 transition（即P矩阵）

下一个状态可以根据上一个状态以及动作完全确定：

$s_{t+1}=\delta(s_t,a_t)=(u,e_{t-K+1},...,r_t,e_t,r_{t+1},e_{t+1})$

### 2.6.5 reward

奖励值包含三部分：$R_{p,t}$、$R_{m,t}$、$R_{e,t}$

其中前两部分为后面的对抗模仿学习部分的判别器D得到，$R_{e,t}=\rho(s_T,a_T)=\mathbb{I}_{V_u}(e_T)$，当最终T时刻路径最后的物品在用户u的交互记录中则为1，否则为0，该奖励非常稀疏。

## 2.7 Actor-Critic

### 2.7.1 Actor网络

该网络根据知识图谱（环境）中的状态以及可以选取的动作得到当前采取每个动作的概率值：

<img src="D:\TyporaPicture\23.5.18\image-20230518090357163.png" alt="image-20230518090357163" style="zoom:80%;" />



### 2.7.2 Critic网络

因为奖励函数中的两部分$R_{e,t}$与$R_{m,t}$都非常稀疏，所以需要利用critic网络来评价动作的价值从而更好地指引actor的训练。

<img src="D:\TyporaPicture\23.5.18\image-20230518091020194.png" alt="image-20230518091020194" style="zoom:80%;" />

训练该网络使用TD误差：

$R_t$为：

<img src="D:\TyporaPicture\23.5.18\image-20230518091120328.png" alt="image-20230518091120328" style="zoom:80%;" />

目标函数为：

<img src="D:\TyporaPicture\23.5.18\image-20230518091138592.png" alt="image-20230518091138592" style="zoom:80%;" />



## 2.8 对抗模仿学习

**path判别器：**

<img src="D:\TyporaPicture\23.5.18\image-20230518091756587.png" alt="image-20230518091756587" style="zoom:80%;" />

通过该loss函数更新该判别器：

<img src="D:\TyporaPicture\23.5.18\image-20230518091834339.png" alt="image-20230518091834339" style="zoom:80%;" />

得到的奖励为：

<img src="D:\TyporaPicture\23.5.18\image-20230518091849930.png" alt="image-20230518091849930" style="zoom:80%;" />

**meta-path判别器：**

<img src="D:\TyporaPicture\23.5.18\image-20230518091925782.png" alt="image-20230518091925782" style="zoom:80%;" />

其中M为该路径中关系的embedding：

![image-20230518092001121](D:\TyporaPicture\23.5.18\image-20230518092001121.png)

通过该loss函数更新：

<img src="D:\TyporaPicture\23.5.18\image-20230518092040212.png" alt="image-20230518092040212" style="zoom:80%;" />

得到的奖励为：

<img src="D:\TyporaPicture\23.5.18\image-20230518092052159.png" alt="image-20230518092052159" style="zoom:80%;" />



## 2.9 实验

![image-20230518091218920](D:\TyporaPicture\23.5.18\image-20230518091218920.png)

ADAC-C:移除了critic网络，利用策略梯度方式更新网络

ADAC-P:移除了path判别器

ADAC-M:移除了meta-path判别器



将得到的路径$\tau_{u,\hat{v}_u}$中出现的实体按照出现次数排序，之后与reviews中的ground truth words匹配得到可解释性：

<img src="D:\TyporaPicture\23.5.18\image-20230518091515652.png" alt="image-20230518091515652" style="zoom:80%;" />













