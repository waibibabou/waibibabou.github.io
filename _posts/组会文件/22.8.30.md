# 1 KERL: A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation   SIGIR 2020

## 1.1 背景/出发点

知识图谱由于其在诸多领域的适用性，并且能够指导基于强化学习的训练方法，其在序列推荐任务中已经有所使用，然而先前的研究中，通常只是利用知识图谱来‘exploitation——利用’，很少有人考虑知识图谱在‘exploration——探索’中的应用，这导致他们无法很好地捕捉用户的兴趣偏好在未来的变化。

## 1.2 本文主要工作

1）将序列化推荐任务进行马尔科夫决策过程建模，并且融合知识图谱提高推荐性能。

###### 2）在框架中进行了三个主要的技术扩展，第一，提出用KG（知识图谱）信息增强状态表示。第二，设计了一个能够同时计算序列级和知识级的奖励值的复合奖励函数。第三，提出一个截断的策略梯度来训练模型，针对感应网络训练的不稳定性，引入了带有模拟子序列的学习机制。将模型命名为Knowledge-guided Reinforcement Learning Model（KERL）

3）在四个真实数据集中进行了实验，在不同的评测标准中推荐效果均有所提升

## 1.3 任务定义与MDP建模

**状态s~t~**设定为用户浏览序列的前t条记录以及知识图谱G：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830013657865.png" alt="image-20220830013657865"  />

其中i~1:t~代表浏览序列的前t个物品。

**动作a~t~**设定为推荐的某一物品。

**奖励值r**为推荐的物品序列与数据集中的真实序列的相似度，以及根据知识图谱得到的序列embedding向量的相似度相加得到奖励值。

该推荐任务定义为：根据用户的历史交互记录以及知识图谱G，预测该用户下一个将要产生交互的物品。



## 1.4 模型搭建

### 1.4.1 模型整体框架

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830020322092.png" alt="image-20220830020322092" style="zoom: 200%;" />

### 1.4.2 策略函数定义以及状态转化

策略函数定义：

![image-20220830014819964](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830014819964.png)

其中q~ij~代表第j个物品的特征向量，W~1~代表该策略函数的参数，v~st~代表当前状态的特征向量。

该函数的作用类似DDPG算法中的Actor网络，根据当前的状态以及所有物品的特征向量，进行Softmax，得到所有物品的被推荐的概率，后续在模型训练完成后，将使用该策略函数进行推荐。

状态转化：

![image-20220830015618347](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830015618347.png)

### 1.4.3 状态特征向量的获取

状态特征向量的表示方法将包括两部分：序列级的表示以及知识级的表示

1）序列级的表示

该部分体现的是用户浏览序列的特征，使用GRU从前向后进行序列的特征提取：

![image-20220830020637886](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830020637886.png)

Φ~gru~代表GRU网络参数

2）知识级的表示

在以往的大多数方法中，主要考虑了使用知识图谱来得到用户短期的行为兴趣向量（利用），却很少考虑利用知识图谱来最大化长远的目标（探索），为了平衡探索以及利用，在知识级的特征向量表示中包括两部分：当前的偏好以及未来的偏好

**当前偏好（current preference）：**

一个物品在知识图谱中为一个实体，使用知识图谱的embedding method TransE将图中的一个物品i  embedding得到其特征向量v~i~

使用平均池化并求和得到当前偏好的特征向量：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830022444333.png" alt="image-20220830022444333" style="zoom:80%;" />

**未来偏好（future preference）:**

利用得到的当前偏好通过一个全连接网络直接预测未来偏好：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830022939315.png" alt="image-20220830022939315" style="zoom:80%;" />

其中f~t:t+k~代表未来的k步之内的偏好，Φ~mlp~代表该MLP的网络参数

3）得到状态特征向量

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830023700820.png" alt="image-20220830023700820" style="zoom:80%;" />

将序列的特征向量以及当前偏好向量以及未来偏好向量连接得到状态特征向量。



### 1.4.4 奖励值的获取

奖励值同样由序列级的奖励以及知识级的奖励组成：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830024120099.png" alt="image-20220830024120099" style="zoom:80%;" />

其中i~t:t+k~代表数据集中的真实序列， i_hat~t:t+k~代表该模型预测的序列

1）序列级的奖励

该奖励通过对比预测出的长度为k的序列与真实序列的相似性计算得到，计算方式为BLEU：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830024645685.png" alt="image-20220830024645685" style="zoom:80%;" />

其中的prec~m~为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830024655677.png" alt="image-20220830024655677" style="zoom: 80%;" />

M为超参，代表在计算相似度时只考虑最长为M的序列。



2）知识级的奖励

使用相同的公式计算t:t+k时间段的真实序列以及预测序列的偏好特征向量：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830022444333.png" alt="image-20220830022444333" style="zoom: 80%;" />

使用余弦相似度计算知识级的奖励：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830025358092.png" alt="image-20220830025358092" style="zoom: 80%;" />

### 1.4.5 训练过程

**模型的整体训练流程：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830025642463.png" alt="image-20220830025642463" style="zoom: 67%;" />

此处的截断表示在对Φ更新的公式中只对t到t+k时间的序列求和，而不是t时间后的所有序列。

## 1.5 实验结果

**数据集：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830030653699.png" alt="image-20220830030653699" style="zoom:67%;" />

前三个为亚马逊的化妆品、CD、书籍的数据集，最后一个为一个用户听音乐记录的数据集

**实验结果：**

去除历史记录过少的用户与物品，对于每一个用户，按照时间顺序对历史行为记录排序。

在最后模型的对比评测时，模型将根据策略函数得到概率最高的k个物品，成为top-K推荐，评测指标选为Hit-Ratio@k以及NDCG@k。

![image-20220830031324721](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830031324721.png)

**消融实验：**

对于状态设定的消融实验：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830031535337.png" alt="image-20220830031535337" style="zoom: 80%;" />

对于奖励值设定的消融实验：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830031620159.png" alt="image-20220830031620159" style="zoom:80%;" />





# 2 Deep Reinforcement Learning for Page-wise Recommendations   RecSys   2019

## 2.1 背景/出发点

在商品推荐的app中，例如京东、淘宝，商品都是一个个矩形密铺在页面上，而不像传统的信息检索问题中搜索结果是一条条从上到下顺序排列（例如百度）。现有的对物品的ranking算法来源于信息检索领域，只适用于顺序排列的页面，对于电商网站这样的密铺页面不是很好的建模方式。

二维页面：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830102944168.png" alt="image-20220830102944168" style="zoom: 33%;" />

一维页面：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830103029897.png" alt="image-20220830103029897" style="zoom: 50%;" />

## 2.2 本文主要工作

1）提出一个方法，其能够推荐生成一组多样性较高的物品并同时将这些物品展示为一个二维的推荐页面

2）提出一个page-wise的推荐框架DeepPage，其能够根据用户的反馈数据最优化一整个页面的推荐结果

3）在真实数据集中验证了模型的有效性

## 2.3 MDP建模

状态S：设定为某用户的当前偏好，通过浏览历史记录计算得到

动作A：设定为推荐的一个页面的物品信息

![image-20220830104628288](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830104628288.png)

奖励值R：在agent做出推荐动作后，用户对一个页面的推荐结果做出反馈，可以选择略过、点击、购买，奖励值分别为0、1、5

## 2.4 Actor网络结构

Actor网络输入为当前状态，输出为推荐的物品信息。

### 2.4.1 得到初始状态

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830105246665.png" alt="image-20220830105246665" style="zoom: 67%;" />

输入为用户在此之前的点击或者购买的物品的信息，e~i~表示一个物品的特征向量。通过一个GRU提取出序列的信息，将GRU最后的状态h~N~作为初始状态s^ini^

### 2.4.2 得到当前状态

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830110205284.png" alt="image-20220830110205284" style="zoom:67%;" />

对于每一个page的输入为：<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830110352822.png" alt="image-20220830110352822" style="zoom: 67%;" />

M为一个推荐页面所包含的物品数量，x~i~为：<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830110606662.png" alt="image-20220830110606662" style="zoom:67%;" />

其中e~i~为物品i的特征向量，c~i~为物品i的类别，f~i~为用户对于该物品的反馈结果

x~i~经过embedding后得到X~i~，由三部分的embedding向量连接得到：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830111233898.png" alt="image-20220830111233898" style="zoom:67%;" />

后续使用CNN二维卷积提取该页面的特征信息：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830111626500.png" alt="image-20220830111626500" style="zoom:67%;" />

与得到初始状态类似，使用GRU提取所有页面组成的序列的特征，s^ini^作为GRU的初始状态。

引入Attention机制，来进一步获取用户的当前偏好信息：

![image-20220830111943895](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830111943895.png)

其中α~t~为：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830112451141.png" alt="image-20220830112451141" style="zoom:67%;" />

### 2.4.3 得到输出的动作

在得到当前状态后，使用反卷积网络得到动作矩阵a^cur^：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830112906185.png" alt="image-20220830112906185" style="zoom:67%;" />

a^cur^为一个h*w|E|大小的矩阵，h为页面的行数，w为页面的列数，|E|为特征向量的长度

直观理解反卷积如下，此为GAN生成图像的过程：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830113513972.png" alt="image-20220830113513972" style="zoom:67%;" />

## 2.5 Critic网络结构

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830113715024.png" alt="image-20220830113715024" style="zoom:67%;" />

a^cur^~pro~代表原始的动作矩阵，其中每个e~i~为一个item的特征向量，然而某些e~i~并不在数据集中，所以需要寻找每个e~i~最相似的在数据集中的item：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830114218526.png" alt="image-20220830114218526" style="zoom:67%;" />

从而将a^cur^~pro~映射为a^cur^~val~矩阵

后续使用卷积将矩阵a^cur^转为一个低维度的向量，作为Critic网络的输入：

![image-20220830114435257](C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830114435257.png)

## 2.6 训练过程

在线训练过程：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830114740613.png" alt="image-20220830114740613" style="zoom:75%;" />

**Critic网络更新方式：**

loss函数（TD误差）：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830115618968.png" alt="image-20220830115618968" style="zoom:80%;" />

从而得到梯度：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830115516837.png" alt="image-20220830115516837" style="zoom: 80%;" />

**Actor网络更新方式：**

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830115638960.png" alt="image-20220830115638960" style="zoom:80%;" />

沿着Critic网络输出的Q值增大的方向更新参数。

在线训练过程中不存在推荐的物品不在数据集中从而无法得到奖励值的情况。



对于离线训练过程则存在该问题，本文的解决方法是，采取的动作使用数据集中的推荐物品，而在上图13步前加入一步，基于如下公式最小化a^cur^~pro~与a^cur^~val~的差距：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830115406552.png" alt="image-20220830115406552" style="zoom:80%;" />

（并不是很能理解该做法的正确性，较认可根据数据集建立在线模拟器的方式）



## 2.7 测试过程

在线测试过程：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830120043646.png" alt="image-20220830120043646" style="zoom:67%;" />

离线测试过程：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830120153244.png" alt="image-20220830120153244" style="zoom:67%;" />

该过程对于该session中的所有待推荐物品进行重新排序，只对某一session的物品排序而不对所有物品排序还是因为推荐的物品交互记录不在数据集中的问题。



## 2.8 实验结果

### 2.8.1 实验设定

获取初始状态使用10条最近的记录，一个页面的布局为5行2列，略过、点击、购买的奖励值分别为0、1、5

### 2.8.2 离线测试结果

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830121000135.png" alt="image-20220830121000135" style="zoom:67%;" />

### 2.8.3 在线测试结果

将累计的奖励值作为评测指标：

<img src="C:\Users\27399\AppData\Roaming\Typora\typora-user-images\image-20220830121712427.png" alt="image-20220830121712427" style="zoom:67%;" />



