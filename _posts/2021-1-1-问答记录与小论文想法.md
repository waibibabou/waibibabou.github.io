---
layout: post
title: 问答记录与小论文想法
subtitle: 呜呜呜
tags: [想法]
comments: true
author: zyk
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/duck.png
share-img: /assets/img/path.jpg
---


DRCGR那篇里面相当于真样本其实并不是在数据集中随机抽取的，而是与假样本前半个序列相同的真样本



能不能把《Leveraging Long and Short-Term Information in Content-Aware Movie Recommendation via Adversarial Training》这篇文章做成历史浏览序列的，想的是G的动作相当于强化学习中的a，会影响G输入中的历史向量。那真样本如何定义呢，因为数据集中无法得到与假样本前半个序列相同的真样本。

还有个想法是，把这篇看做DDPG模型，G的输出为a，建立一个在线模拟器得到r，存储五元组到经验回访池中，但问题是，G与D的更新感觉都用不到经验回访池中的数据？又变成单独的GAN了



**可信用户引导强化学习**
**差小的时候需要反传梯度使其变大，差大时候就不反传梯度使其不变**

10.26大组会：使用transformer作为gan网络的生成器

数据怎么用，分布怎么学

大组会时候说为什么在D的输入中有两部分其中有s_hat，感觉是因为真样本就是正负反馈对？

老师给的那篇论文中，真样本对也可以想成m-,m+   假样本对是mgt,m+  D是衡量该样本对的真假

对于m+的得分，越高越好，m-的得分，越低越好，但是D是不相信G的结果的，所以在hinge loss中取反

确实见到过动作a产生下一个状态，然后把这个五元组存起来供后续训练用，其没有影响下一时刻的输入，输入是从数据集得到的固定的。
不过我觉得这种不靠谱，感觉违背了强化学习本身的马尔可夫性？



DEERS以及List-wise两篇文章中都是根据reward值再决定a是否改变s，在更多情况下应该是无论reward大小，a都应该改变s。



**问题一：对于GAN中生成器的训练方式，是使用GAN的公式还是用PG的方式**

**问题二：强化学习是没有监督信号的，那在结合知识图谱的这篇中KERL A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation SIGIR 2020**

**其根据预测序列与真实序列的相似性得到reward，这不是一种监督学习吗**

**为什么要跟真实序列作比较**





问：List-wise那篇文章中构建一个online模拟器的原因：如果没有这种模拟器机制，那为了知道在状态s下a的reward，就必须只推荐这个session中出现过的物品，将actor输出映射到这个session中最相近的物品上，而且这也是看做在一个session中s不变的，不然怎么可能知道在这个精确的s下a的反馈（如果有那不就成监督学习了吗，或者说是行为克隆了）

答：要将这个session中的所有物品排序，计算NDCG



问：KERL那篇论文中，将策略函数输出的t+1到t+k的序列与真实序列相比较得到r，那都有真实序列了直接做成监督模型，算loss不就行了吗？

答：我觉得算loss与作为r还是有很大不同的，作为监督模型算loss意思就是在这个s就应该输出这个a，但是算r，在这个s下并不是要输出r最大的a，强化学习模型需要长远考虑，考虑后面的q值

我觉得获取r的过程是可以类似监督型获得的，但是从s获得a的过程不能有监督信号，不是会执行r最高的动作而是q最高的，这是强化学习的优势，会考虑以后的回报
ddpg的两篇和知识图谱那个本质都是与数据集中的一些已有记录比较得到r，这并不是监督学习，没有说利用s对应的标签输出进行训练

一个动作的r高也不是一定输出该动作



policy-based的强化学习适用于离散动作与连续动作，离散动作则策略函数的输出为每个动作的概率值，根据概率值抽样后执行动作，例如reinforce算法，actor-critic算法。连续动作的策略函数的输出为确定性的动作，如ddpg，td3.

value-based的强化学习只适用于离散动作。



问：使用simulator的好处？





List-wise那篇，是比对数据集中最相似的得到r与st+1，不是通过学习环境得到了奖励函数以及P，所以这是model-free的方法。





把3.9号讲的两篇结合起来？都是根据奖励值的差异训练，一个是训练embedding一个是训练simulator

可以用taobao数据集，开源



在策略梯度中，把梯度写成下面的更一般的形式：
$$
g=E[\sum_{t=0}^n\psi_t\nabla_\theta\log\pi_\theta(a_t|s_t)]
$$

其中$\psi_t$可以有以下多种形式：

1.$\sum_{t'=0}^T\gamma^tr_{t'}$：轨迹的总回报

2.$\sum_{t'=t}^T\gamma^{t'-t}r_t'$：动作$a_t$之后的回报

3.$\sum_{t'=t}^T\gamma^{t'-t}r_{t'}-b(s_t)$：基准线版本的改进

4.$Q^{\pi_\theta}(s_t,a_t)$：动作价值函数

5.$A^{\pi_\theta}(s_t,a_t)$：优势函数

6.$r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)$：时序差分残差

actor_critic算法：hands-on RL用的6，强化学习电子书用的4



##### 10.9

能否用LSTM训一个user behavior model，曾静的方法？



能否套在reward shaping下面，因为现在的奖励就是很稀疏



老师说的：user behavior model如果是pretrain后fix不太好  把positive以及negetive的s与simulator框在一个大框下，叫user interest

